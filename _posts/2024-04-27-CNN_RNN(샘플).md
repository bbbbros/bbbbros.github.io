# [실습-퀴즈] Python을 활용한 AI 모델링 - 딥러닝 파트
+ 이번시간에는 Python을 활용한 AI 모델링에서 딥러닝에 대해 실습해 보겠습니다.
+ 여기서는 딥러닝 모델 DNN, CNN, RNN 에 대해 코딩하여 모델 구축해 보겠습니다.
+ 한가지 당부 드리고 싶은 말은 "백문이불여일타" 입니다. 
+ 이론보다 실습이 더 많은 시간과 노력이 투자 되어야 합니다.

## 학습목차
1. 실습준비
2. 딥러닝 모델(DNN, CNN, RNN) 프로세스
 - 데이터 가져오기
 - 데이터 전처리
 - Train, Test 데이터셋 분할
 - 데이터 정규화
 - 딥러닝 모델 : DNN, CNN, RNN

# 
# 1. 실습준비


```python
# 코드실행시 경고 메시지 무시

import warnings
warnings.filterwarnings(action='ignore') 
```

# 
# 2. 딥러닝 심층신경망(DNN) 모델 프로세스
① 라이브러리 임포트(import)  
② 데이터 가져오기(Loading the data)  
③ 탐색적 데이터 분석(Exploratory Data Analysis)  
④ 데이터 전처리(Data PreProcessing) : 데이터타입 변환, Null 데이터 처리, 누락데이터 처리, 
더미특성 생성, 특성 추출 (feature engineering) 등  
⑤ Train, Test  데이터셋 분할  
⑥ 데이터 정규화(Normalizing the Data)  
⑦ 모델 개발(Creating the Model)  
⑧ 모델 성능 평가

## ① 라이브러리 임포트

##### 필요 라이브러리 임포트


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

## ② 데이터 로드

#### cust_data.csv 파일 컬럼명
+ 고객등급(class), 성별(sex), 나이(age), 사용서비스수(service), 서비스중지여부 (stop), 미납여부(npay)
+ 3개월 평균 요금(avg_bill), A서비스 3개월 평균요금(A_bill), B서비스 3개월 평균요금(B_bill), 해지여부(termination)

##### <font color=blue> **[문제] 같은 폴더내에 있는 cust_data.csv 파일을 Pandas read_csv 함수를 이용하여 읽어 df 변수에 저장하세요.** </font>


```python
# 읽어 들일 파일명 : cust_data.csv
# Pandas read_csv 함수 활용
# 결과 : df 저장

df = pd.read_csv('cust_data.csv')

```

## ③ 데이터 분석


```python
# 10컬럼, 9930 라인
df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 7814 entries, 0 to 7813
    Data columns (total 12 columns):
     #   Column       Non-Null Count  Dtype  
    ---  ------       --------------  -----  
     0   class        7814 non-null   object 
     1   sex          7814 non-null   object 
     2   age          7814 non-null   int64  
     3   service      7814 non-null   int64  
     4   stop         7814 non-null   object 
     5   npay         7814 non-null   object 
     6   avg_bill     7814 non-null   float64
     7   A_bill       7814 non-null   float64
     8   B_bill       7814 non-null   float64
     9   termination  7814 non-null   object 
     10  by_age       7814 non-null   int64  
     11  bill_rating  7814 non-null   object 
    dtypes: float64(3), int64(3), object(6)
    memory usage: 732.7+ KB



```python
df.tail()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>class</th>
      <th>sex</th>
      <th>age</th>
      <th>service</th>
      <th>stop</th>
      <th>npay</th>
      <th>avg_bill</th>
      <th>A_bill</th>
      <th>B_bill</th>
      <th>termination</th>
      <th>by_age</th>
      <th>bill_rating</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>7809</th>
      <td>C</td>
      <td>M</td>
      <td>76</td>
      <td>1</td>
      <td>N</td>
      <td>N</td>
      <td>1860.0000</td>
      <td>1716.000000</td>
      <td>0.0000</td>
      <td>N</td>
      <td>75</td>
      <td>low</td>
    </tr>
    <tr>
      <th>7810</th>
      <td>C</td>
      <td>F</td>
      <td>15</td>
      <td>1</td>
      <td>N</td>
      <td>Y</td>
      <td>1296.0999</td>
      <td>194.414985</td>
      <td>643.1001</td>
      <td>N</td>
      <td>15</td>
      <td>low</td>
    </tr>
    <tr>
      <th>7811</th>
      <td>G</td>
      <td>M</td>
      <td>12</td>
      <td>1</td>
      <td>N</td>
      <td>N</td>
      <td>13799.6666</td>
      <td>2069.949990</td>
      <td>10605.9266</td>
      <td>N</td>
      <td>10</td>
      <td>midhigh</td>
    </tr>
    <tr>
      <th>7812</th>
      <td>C</td>
      <td>F</td>
      <td>40</td>
      <td>0</td>
      <td>N</td>
      <td>N</td>
      <td>3140.0000</td>
      <td>942.000000</td>
      <td>1884.0000</td>
      <td>Y</td>
      <td>40</td>
      <td>low</td>
    </tr>
    <tr>
      <th>7813</th>
      <td>C</td>
      <td>F</td>
      <td>59</td>
      <td>1</td>
      <td>N</td>
      <td>N</td>
      <td>2436.9000</td>
      <td>365.535000</td>
      <td>1839.9000</td>
      <td>N</td>
      <td>55</td>
      <td>low</td>
    </tr>
  </tbody>
</table>
</div>




```python
# termination 레이블 불균형 
df['termination'].value_counts().plot(kind='bar')
```




    <AxesSubplot:>




    
![png](output_15_1.png)
    


## ④ 데이터 전처리

+ Object 컬럼에 대해 Pandas get_dummies 함수 활용하여 One-Hot-Encoding


```python
# Object 컬럼 리스트 정의
cal_cols = ['class', 'sex', 'stop', 'npay', 'termination', 'bill_rating']
```

##### <font color=blue> **[문제] ['class', 'sex', 'stop', 'npay', 'termination', 'bill_rating'] 컬럼에 대해 One-Hot-Encoding 수행하고 그 결과를 df1 변수에 저장하세요.** </font>


```python
# ['class', 'sex', 'stop', 'npay', 'termination', 'bill_rating'] : cal_cols 변수에 대해 One-Hot-Endcoding 수행
# One-Hot-Endcoding 수행 : pandas get_dummies() 함수 이용
# get_dummies() 함수 옵션 : data=df, columns=cal_cols, drop_first=True
# 결과 : df1 저장

df1 = pd.get_dummies(df, columns = cal_cols, drop_first=True)


```


```python
# 19컬럼, 7814 라인
df1.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 7814 entries, 0 to 7813
    Data columns (total 19 columns):
     #   Column               Non-Null Count  Dtype  
    ---  ------               --------------  -----  
     0   age                  7814 non-null   int64  
     1   service              7814 non-null   int64  
     2   avg_bill             7814 non-null   float64
     3   A_bill               7814 non-null   float64
     4   B_bill               7814 non-null   float64
     5   by_age               7814 non-null   int64  
     6   class_D              7814 non-null   uint8  
     7   class_E              7814 non-null   uint8  
     8   class_F              7814 non-null   uint8  
     9   class_G              7814 non-null   uint8  
     10  class_H              7814 non-null   uint8  
     11  sex_M                7814 non-null   uint8  
     12  stop_Y               7814 non-null   uint8  
     13  npay_Y               7814 non-null   uint8  
     14  termination_Y        7814 non-null   uint8  
     15  bill_rating_low      7814 non-null   uint8  
     16  bill_rating_lowmid   7814 non-null   uint8  
     17  bill_rating_mid      7814 non-null   uint8  
     18  bill_rating_midhigh  7814 non-null   uint8  
    dtypes: float64(3), int64(3), uint8(13)
    memory usage: 465.6 KB


## ⑤ Train, Test  데이터셋 분할


```python
from sklearn.model_selection import train_test_split
```


```python
X = df1.drop('termination_Y', axis=1).values
y = df1['termination_Y'].values
```


```python
X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                    test_size=0.3, 
                                                    stratify=y,
                                                    random_state=42)
```


```python
X_train.shape
```




    (5469, 18)




```python
y_train.shape
```




    (5469,)



## ⑥ 데이터 정규화/스케일링(Normalizing/Scaling)


```python
# 숫자 분포 이루어진 컬럼 확인
df1.tail()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>service</th>
      <th>avg_bill</th>
      <th>A_bill</th>
      <th>B_bill</th>
      <th>by_age</th>
      <th>class_D</th>
      <th>class_E</th>
      <th>class_F</th>
      <th>class_G</th>
      <th>class_H</th>
      <th>sex_M</th>
      <th>stop_Y</th>
      <th>npay_Y</th>
      <th>termination_Y</th>
      <th>bill_rating_low</th>
      <th>bill_rating_lowmid</th>
      <th>bill_rating_mid</th>
      <th>bill_rating_midhigh</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>7809</th>
      <td>76</td>
      <td>1</td>
      <td>1860.0000</td>
      <td>1716.000000</td>
      <td>0.0000</td>
      <td>75</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7810</th>
      <td>15</td>
      <td>1</td>
      <td>1296.0999</td>
      <td>194.414985</td>
      <td>643.1001</td>
      <td>15</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7811</th>
      <td>12</td>
      <td>1</td>
      <td>13799.6666</td>
      <td>2069.949990</td>
      <td>10605.9266</td>
      <td>10</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7812</th>
      <td>40</td>
      <td>0</td>
      <td>3140.0000</td>
      <td>942.000000</td>
      <td>1884.0000</td>
      <td>40</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7813</th>
      <td>59</td>
      <td>1</td>
      <td>2436.9000</td>
      <td>365.535000</td>
      <td>1839.9000</td>
      <td>55</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




```python
from sklearn.preprocessing import MinMaxScaler
```


```python
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
```


```python
X_train[:2]
```




    array([[0.38      , 0.33333333, 0.4295439 , 0.06384702, 0.41944434,
            0.4       , 0.        , 0.        , 0.        , 0.        ,
            1.        , 1.        , 0.        , 1.        , 0.        ,
            0.        , 0.        , 1.        ],
           [0.58      , 0.11111111, 0.20111297, 0.38498933, 0.        ,
            0.6       , 1.        , 0.        , 0.        , 0.        ,
            0.        , 1.        , 0.        , 0.        , 0.        ,
            1.        , 0.        , 0.        ]])



+ 모델 입력갯수, 출력갯수 확인

#### 
## ⑦ 딥러닝 심층신경망(DNN) 모델 구현

#### 
### 라이브러리 임포트


```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout
```

#### 
### 하이퍼파라미터 설정 : batch_size, epochs


```python
batch_size = 16
epochs = 20
```

#### 
### 모델 입력(features) 갯수 확인


```python
X_train.shape
```




    (5469, 18)



#### 
### 모델 출력(label) 갯수 확인


```python
y_train.shape
```




    (5469,)



#### 
### A. 이진분류 DNN모델 구성 

![hidden Layer](https://github.com/gzone2000/TEMP_TEST/raw/master/hidden_layer1.PNG)
+ [출처] https://subscription.packtpub.com/book/data/9781788995207/1/ch01lvl1sec03/deep-learning-intuition

##### <font color=blue> **[문제] 요구사항대로 Sequential 모델을 만들어 보세요.** </font>


```python
# Sequential() 모델 정의 하고 model로 저장
# input layer는 input_shape=() 옵션을 사용한다.
# 18개 input layer
# unit 4개 hidden layer
# unit 3개 hidden layer 
# 1개 output layser : 이진분류





```


```python
model = Sequential()

# 첫번째 히든레이어
model.add(Dense(4,activation='relu',input_shape=(18,)))
model.add(Dropout(0.3))
model.add(Dense(3,activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(1,activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, epochs=epochs)
```

    Epoch 1/20
    342/342 [==============================] - 1s 2ms/step - loss: 0.5087 - acc: 0.8661 - val_loss: 0.3463 - val_acc: 0.8994
    Epoch 2/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.3983 - acc: 0.8986 - val_loss: 0.3016 - val_acc: 0.8994
    Epoch 3/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.3468 - acc: 0.9024 - val_loss: 0.2863 - val_acc: 0.8994
    Epoch 4/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.3192 - acc: 0.9030 - val_loss: 0.2747 - val_acc: 0.8994
    Epoch 5/20
    342/342 [==============================] - 1s 2ms/step - loss: 0.2974 - acc: 0.9034 - val_loss: 0.2643 - val_acc: 0.8994
    Epoch 6/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2920 - acc: 0.9010 - val_loss: 0.2529 - val_acc: 0.8994
    Epoch 7/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2973 - acc: 0.8943 - val_loss: 0.2416 - val_acc: 0.8994
    Epoch 8/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2726 - acc: 0.9012 - val_loss: 0.2305 - val_acc: 0.8994
    Epoch 9/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2585 - acc: 0.8997 - val_loss: 0.2218 - val_acc: 0.8994
    Epoch 10/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2554 - acc: 0.8987 - val_loss: 0.2119 - val_acc: 0.8994
    Epoch 11/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2460 - acc: 0.8970 - val_loss: 0.2039 - val_acc: 0.8994
    Epoch 12/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2272 - acc: 0.8999 - val_loss: 0.1999 - val_acc: 0.8994
    Epoch 13/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2432 - acc: 0.8921 - val_loss: 0.1981 - val_acc: 0.8994
    Epoch 14/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2279 - acc: 0.9013 - val_loss: 0.1937 - val_acc: 0.8994
    Epoch 15/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2276 - acc: 0.8977 - val_loss: 0.1911 - val_acc: 0.8994
    Epoch 16/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2125 - acc: 0.9070 - val_loss: 0.1880 - val_acc: 0.8994
    Epoch 17/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2254 - acc: 0.8945 - val_loss: 0.1868 - val_acc: 0.8994
    Epoch 18/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2130 - acc: 0.9027 - val_loss: 0.1816 - val_acc: 0.8994
    Epoch 19/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2172 - acc: 0.9042 - val_loss: 0.1830 - val_acc: 0.8994
    Epoch 20/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2154 - acc: 0.8948 - val_loss: 0.1792 - val_acc: 0.8994


#### 
### 모델 확인


```python
model.summary()
```

    Model: "sequential"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    dense (Dense)                (None, 4)                 76        
    _________________________________________________________________
    dropout (Dropout)            (None, 4)                 0         
    _________________________________________________________________
    dense_1 (Dense)              (None, 3)                 15        
    _________________________________________________________________
    dropout_1 (Dropout)          (None, 3)                 0         
    _________________________________________________________________
    dense_2 (Dense)              (None, 1)                 4         
    =================================================================
    Total params: 95
    Trainable params: 95
    Non-trainable params: 0
    _________________________________________________________________


#### 
### 모델 구성 -  과적합 방지

![dropout](https://github.com/gzone2000/TEMP_TEST/raw/master/dropout.PNG)
+ [출처] https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5


```python
model = Sequential()
model.add(Dense(4, activation='relu', input_shape=(18,)))
model.add(Dropout(0.3))
model.add(Dense(3, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(1, activation='sigmoid'))
```

#### 
### 과적합 방지 모델 확인


```python
model.summary()
```

    Model: "sequential_1"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    dense_3 (Dense)              (None, 4)                 76        
    _________________________________________________________________
    dropout_2 (Dropout)          (None, 4)                 0         
    _________________________________________________________________
    dense_4 (Dense)              (None, 3)                 15        
    _________________________________________________________________
    dropout_3 (Dropout)          (None, 3)                 0         
    _________________________________________________________________
    dense_5 (Dense)              (None, 1)                 4         
    =================================================================
    Total params: 95
    Trainable params: 95
    Non-trainable params: 0
    _________________________________________________________________


#### 
### 모델 컴파일 – 이진 분류 모델


```python
model.compile(optimizer='adam', 
              loss='binary_crossentropy', 
              metrics=['accuracy']) 
```

- 모델 컴파일 – 다중 분류 모델 (Y값을 One-Hot-Encoding 한경우) <br>
model.compile(optimizer='adam', 
              loss='categorical_crossentropy', 
              metrics=['accuracy']) 

- 모델 컴파일 – 다중 분류 모델  (Y값을 One-Hot-Encoding 하지 않은 경우) <br>
model.compile(optimizer='adam', 
              loss='sparse_categorical_crossentropy', 
              metrics=['accuracy'])

- 모델 컴파일 – 예측 모델
model.compile(optimizer='adam', loss='mse')

#### 
### 모델 학습

##### <font color=blue> **[문제] 요구사항대로 DNN 모델을 학습시키세요.** </font>
+ 모델 이름 : model
+ epoch : 10번
+ batch_size : 10번


```python
# 앞쪽에서 정의된 모델 이름 : model
# Sequential 모델의 fit() 함수 사용
# @인자
### X, y : X_train, y_train
### validation_data=(X_test, y_test)
### epochs 10번
### batch_size 10번




```

#### 
### B. 다중 분류 DNN 구성
+ 18개 input layer
+ unit 5개 hidden layer
+ dropout
+ unit 4개 hidden layer 
+ dropout
+ 2개 output layser : 이진분류

![다중분류](https://github.com/gzone2000/TEMP_TEST/raw/master/hidden_layer2.PNG)
+ [출처] https://www.educba.com/dnn-neural-network/


```python
# 18개 input layer
# unit 5개 hidden layer
# dropout
# unit 4개 hidden layer 
# dropout
# 2개 output layser : 다중분류

model = Sequential()
model.add(Dense(5, activation='relu', input_shape=(18,)))
model.add(Dropout(0.3))
model.add(Dense(4, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(2, activation='softmax'))
```

#### 
### 모델 확인


```python
model.summary()
```

    Model: "sequential_2"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    dense_6 (Dense)              (None, 5)                 95        
    _________________________________________________________________
    dropout_4 (Dropout)          (None, 5)                 0         
    _________________________________________________________________
    dense_7 (Dense)              (None, 4)                 24        
    _________________________________________________________________
    dropout_5 (Dropout)          (None, 4)                 0         
    _________________________________________________________________
    dense_8 (Dense)              (None, 2)                 10        
    =================================================================
    Total params: 129
    Trainable params: 129
    Non-trainable params: 0
    _________________________________________________________________


#### 
### 모델 컴파일 – 다중 분류 모델


```python
model.compile(optimizer='adam', 
              loss='sparse_categorical_crossentropy', 
              metrics=['accuracy']) 
```

#### 
### 모델 학습


```python
history = model.fit(X_train, y_train, 
          validation_data=(X_test, y_test),
          epochs=20, 
          batch_size=16)
```

    Epoch 1/20
    342/342 [==============================] - 1s 2ms/step - loss: 0.5140 - accuracy: 0.8968 - val_loss: 0.3098 - val_accuracy: 0.8994
    Epoch 2/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.3426 - accuracy: 0.8966 - val_loss: 0.2901 - val_accuracy: 0.8994
    Epoch 3/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.3161 - accuracy: 0.9019 - val_loss: 0.2805 - val_accuracy: 0.8994
    Epoch 4/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.3077 - accuracy: 0.8996 - val_loss: 0.2702 - val_accuracy: 0.8994
    Epoch 5/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2932 - accuracy: 0.9020 - val_loss: 0.2629 - val_accuracy: 0.8994
    Epoch 6/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2716 - accuracy: 0.9070 - val_loss: 0.2531 - val_accuracy: 0.8994
    Epoch 7/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2614 - accuracy: 0.9045 - val_loss: 0.2424 - val_accuracy: 0.8994
    Epoch 8/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2718 - accuracy: 0.8936 - val_loss: 0.2288 - val_accuracy: 0.8994
    Epoch 9/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2357 - accuracy: 0.9045 - val_loss: 0.2132 - val_accuracy: 0.8994
    Epoch 10/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2312 - accuracy: 0.8977 - val_loss: 0.1959 - val_accuracy: 0.8994
    Epoch 11/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2156 - accuracy: 0.9007 - val_loss: 0.1843 - val_accuracy: 0.8994
    Epoch 12/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.1941 - accuracy: 0.9054 - val_loss: 0.1733 - val_accuracy: 0.8994
    Epoch 13/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.1995 - accuracy: 0.9006 - val_loss: 0.1655 - val_accuracy: 0.8994
    Epoch 14/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2038 - accuracy: 0.8932 - val_loss: 0.1577 - val_accuracy: 0.8994
    Epoch 15/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.1868 - accuracy: 0.9028 - val_loss: 0.1537 - val_accuracy: 0.8994
    Epoch 16/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.9004 - val_loss: 0.1463 - val_accuracy: 0.8994
    Epoch 17/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.1763 - accuracy: 0.8959 - val_loss: 0.1419 - val_accuracy: 0.8994
    Epoch 18/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.1785 - accuracy: 0.9010 - val_loss: 0.1390 - val_accuracy: 0.8994
    Epoch 19/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.1704 - accuracy: 0.9010 - val_loss: 0.1354 - val_accuracy: 0.8994
    Epoch 20/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.1727 - accuracy: 0.9112 - val_loss: 0.1340 - val_accuracy: 0.9667


#### 
### Callback : 조기종료, 모델 저장


```python
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
```


```python
early_stop = EarlyStopping(monitor='val_loss', mode='min', 
                           verbose=1, patience=5)
```


```python
check_point = ModelCheckpoint('best_model.h5', verbose=1,
                              monitor='val_loss', mode='min', 
                              save_best_only=True)
```

#### 
### 모델 학습


```python
history = model.fit(x=X_train, y=y_train, 
          epochs=50 , batch_size=20,
          validation_data=(X_test, y_test), 
          verbose=1,
          callbacks=[early_stop, check_point])
```

    Epoch 1/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1738 - accuracy: 0.9303 - val_loss: 0.1329 - val_accuracy: 0.9642
    
    Epoch 00001: val_loss improved from inf to 0.13293, saving model to best_model.h5
    Epoch 2/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1791 - accuracy: 0.9287 - val_loss: 0.1336 - val_accuracy: 0.9689
    
    Epoch 00002: val_loss did not improve from 0.13293
    Epoch 3/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1692 - accuracy: 0.9278 - val_loss: 0.1328 - val_accuracy: 0.9710
    
    Epoch 00003: val_loss improved from 0.13293 to 0.13283, saving model to best_model.h5
    Epoch 4/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1705 - accuracy: 0.9327 - val_loss: 0.1307 - val_accuracy: 0.9672
    
    Epoch 00004: val_loss improved from 0.13283 to 0.13074, saving model to best_model.h5
    Epoch 5/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1730 - accuracy: 0.9323 - val_loss: 0.1343 - val_accuracy: 0.9731
    
    Epoch 00005: val_loss did not improve from 0.13074
    Epoch 6/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1676 - accuracy: 0.9347 - val_loss: 0.1315 - val_accuracy: 0.9723
    
    Epoch 00006: val_loss did not improve from 0.13074
    Epoch 7/50
    274/274 [==============================] - 0s 2ms/step - loss: 0.1720 - accuracy: 0.9327 - val_loss: 0.1319 - val_accuracy: 0.9714
    
    Epoch 00007: val_loss did not improve from 0.13074
    Epoch 8/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1759 - accuracy: 0.9325 - val_loss: 0.1283 - val_accuracy: 0.9684
    
    Epoch 00008: val_loss improved from 0.13074 to 0.12832, saving model to best_model.h5
    Epoch 9/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1692 - accuracy: 0.9382 - val_loss: 0.1282 - val_accuracy: 0.9748
    
    Epoch 00009: val_loss improved from 0.12832 to 0.12819, saving model to best_model.h5
    Epoch 10/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1604 - accuracy: 0.9387 - val_loss: 0.1274 - val_accuracy: 0.9710
    
    Epoch 00010: val_loss improved from 0.12819 to 0.12742, saving model to best_model.h5
    Epoch 11/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1612 - accuracy: 0.9378 - val_loss: 0.1246 - val_accuracy: 0.9727
    
    Epoch 00011: val_loss improved from 0.12742 to 0.12461, saving model to best_model.h5
    Epoch 12/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1740 - accuracy: 0.9376 - val_loss: 0.1246 - val_accuracy: 0.9731
    
    Epoch 00012: val_loss improved from 0.12461 to 0.12459, saving model to best_model.h5
    Epoch 13/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1610 - accuracy: 0.9373 - val_loss: 0.1247 - val_accuracy: 0.9744
    
    Epoch 00013: val_loss did not improve from 0.12459
    Epoch 14/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1675 - accuracy: 0.9373 - val_loss: 0.1260 - val_accuracy: 0.9731
    
    Epoch 00014: val_loss did not improve from 0.12459
    Epoch 15/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1667 - accuracy: 0.9382 - val_loss: 0.1260 - val_accuracy: 0.9744
    
    Epoch 00015: val_loss did not improve from 0.12459
    Epoch 16/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1704 - accuracy: 0.9353 - val_loss: 0.1266 - val_accuracy: 0.9727
    
    Epoch 00016: val_loss did not improve from 0.12459
    Epoch 17/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1664 - accuracy: 0.9395 - val_loss: 0.1237 - val_accuracy: 0.9736
    
    Epoch 00017: val_loss improved from 0.12459 to 0.12368, saving model to best_model.h5
    Epoch 18/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1687 - accuracy: 0.9369 - val_loss: 0.1253 - val_accuracy: 0.9706
    
    Epoch 00018: val_loss did not improve from 0.12368
    Epoch 19/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1720 - accuracy: 0.9371 - val_loss: 0.1248 - val_accuracy: 0.9740
    
    Epoch 00019: val_loss did not improve from 0.12368
    Epoch 20/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1633 - accuracy: 0.9393 - val_loss: 0.1253 - val_accuracy: 0.9731
    
    Epoch 00020: val_loss did not improve from 0.12368
    Epoch 21/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1666 - accuracy: 0.9376 - val_loss: 0.1270 - val_accuracy: 0.9748
    
    Epoch 00021: val_loss did not improve from 0.12368
    Epoch 22/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1720 - accuracy: 0.9358 - val_loss: 0.1254 - val_accuracy: 0.9719
    
    Epoch 00022: val_loss did not improve from 0.12368
    Epoch 00022: early stopping


#### 
### 모델 성능 평가


```python
losses = pd.DataFrame(model.history.history)
```


```python
losses.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss</th>
      <th>accuracy</th>
      <th>val_loss</th>
      <th>val_accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.173845</td>
      <td>0.930335</td>
      <td>0.132929</td>
      <td>0.964179</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.179104</td>
      <td>0.928689</td>
      <td>0.133556</td>
      <td>0.968870</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.169194</td>
      <td>0.927775</td>
      <td>0.132832</td>
      <td>0.971002</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.170495</td>
      <td>0.932712</td>
      <td>0.130738</td>
      <td>0.967164</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.173001</td>
      <td>0.932346</td>
      <td>0.134301</td>
      <td>0.973134</td>
    </tr>
  </tbody>
</table>
</div>



+ 성능 시각화


```python
losses[['loss','val_loss']].plot()
```




    <AxesSubplot:>




    
![png](output_82_1.png)
    



```python
losses[['loss','val_loss', 'accuracy','val_accuracy']].plot()
```




    <AxesSubplot:>




    
![png](output_83_1.png)
    



```python
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Acc')
plt.legend(['acc', 'val_acc'])
plt.show()
```


    
![png](output_84_0.png)
    



```python

```


```python

```

###  
## 2) CNN

![CNN](https://miro.medium.com/max/2000/1*vkQ0hXDaQv57sALXAJquxA.jpeg)
+ [출처] https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53


+ CNN은 이미지 처리에 높은 성능을 보여주고 있어 DNN에서 다뤄던 Tabular 형태의 데이터를 가지고 테스트 진행하기 어려워
+ 따로 이미지 준비하여 CNN 실습을 진행하겠습니다.

#### 업로드된 이미지 파일 가져오기


```python
import os
from glob import glob
import tensorflow as tf
```


```python
# dataset-clean,dirty.zip 파일 확인
FILENAME = 'dataset-clean_dirty.zip'
glob(FILENAME)
```




    ['dataset-clean_dirty.zip']




```python
# dataset-clean_dirty.zip 파일을 IMAGE 디렉토리로 복사 및 압축 풀기
if not os.path.exists('IMAGE') :
    !mkdir IMAGE
    !cp dataset-clean_dirty.zip ./IMAGE
    !cd IMAGE ; unzip dataset-clean_dirty.zip
```


```python
# ./IMAGE/clean 폴더 안의 이미지 갯수
!ls -l ./IMAGE/clean | grep jpg | wc -l
```

    435



```python
# ./IMAGE/clean 폴더 안의 이미지 갯수
!ls -l ./IMAGE/dirty | grep jpg | wc -l
```

    435


#### 이미지 파일 하나 읽어 이미지 보기


```python
clean_img_path = './IMAGE/clean/plastic1.jpg'
```


```python
gfile = tf.io.read_file(clean_img_path)
image = tf.io.decode_image(gfile, dtype=tf.float32)
```


```python
image.shape
```




    TensorShape([384, 512, 3])




```python
plt.imshow(image)
plt.show()
```


    
![png](output_100_0.png)
    



```python
dirty_img_path = './IMAGE/dirty/dirty_plastic1.jpg'
```


```python
gfile = tf.io.read_file(dirty_img_path)
image = tf.io.decode_image(gfile, dtype=tf.float32)
```


```python
image.shape
```




    TensorShape([384, 512, 3])




```python
plt.imshow(image)
plt.show()
```


    
![png](output_104_0.png)
    


#### Data Preprocess
+ tensorflow ImageDataGenerator 함수 활용하여 이미지 데이터 스케일 및 트레인 데이터/ 테스트 데이트 나누기
+ flow_from_directory 함수 활용하여 나누어진 트레인 데이터와 테스트 데이터에 대해 배치 사이즈 나누고 , 셔플하고 labeling 수행


```python
# Hyperparameter Tunning

num_epochs = 50 
batch_size = 4
learning_rate = 0.001

input_shape = (384, 512, 3)  # 사이즈 확인
num_classes = 2    # clean, dirty
```


```python
from tensorflow.keras.preprocessing.image import ImageDataGenerator
```


```python
# ImageDataGenerator 이용하여 이미지 전처리하기

training_datagen = ImageDataGenerator(
      rescale=1. / 255,
      validation_split=0.2     # train set : 435 * (1 - 0.2) = 348
    )

test_datagen = ImageDataGenerator(
      rescale=1./255,
      validation_split=0.2     # test set : 435 * 0.2 = 87
    )
```


```python
# 이미지 데이터 읽고 배치 , 셔플하고 labeling 수행

# IMAGE 포더 밑에 .ipynb_checkpoints 폴더 있을경우 폴데 삭제
!rm -rf ./IMAGE/.ipynb_checkpoints

training_generator = training_datagen.flow_from_directory(
    './IMAGE/',
    batch_size=batch_size, 
    target_size=(384, 512),       # 사이즈 확인
    class_mode = 'categorical',   # binary , categorical
    shuffle = True,
    subset = 'training'           # training, validation. validation_split 사용하므로 subset 지정
    )

test_generator = test_datagen.flow_from_directory(
    './IMAGE/',
    batch_size=batch_size, 
    target_size=(384, 512),       # 사이즈 확인
    class_mode = 'categorical',   # binary , categorical
    shuffle = True,
    subset = 'validation'         # training, validation. validation_split 사용하므로 subset 지정
    )
```

    Found 696 images belonging to 2 classes.
    Found 174 images belonging to 2 classes.



```python
# class 이름 및 번호 매핑 확인
print(training_generator.class_indices)
```

    {'clean': 0, 'dirty': 1}



```python
batch_samples = next(iter(training_generator))

print('True Value : ',batch_samples[1][0])
plt.imshow(batch_samples[0][0])   
plt.show()
```

    True Value :  [0. 1.]



    
![png](output_111_1.png)
    


#### CNN 모델링


```python
#CNN 라이브러리 임포트

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.layers import Conv2D, MaxPooling2D
```


```python
# Feature extraction
model = Sequential()
model.add(Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape))
model.add(MaxPooling2D(pool_size=2))
model.add(Conv2D(filters=16, kernel_size=3, activation='relu'))
model.add(MaxPooling2D(pool_size=2))

# Classification
model.add(Flatten())
model.add(Dense(50, activation='relu'))
model.add(Dense(2, activation='softmax'))
```


```python
model.summary()
```

    Model: "sequential_3"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv2d (Conv2D)              (None, 382, 510, 32)      896       
    _________________________________________________________________
    max_pooling2d (MaxPooling2D) (None, 191, 255, 32)      0         
    _________________________________________________________________
    conv2d_1 (Conv2D)            (None, 189, 253, 16)      4624      
    _________________________________________________________________
    max_pooling2d_1 (MaxPooling2 (None, 94, 126, 16)       0         
    _________________________________________________________________
    flatten (Flatten)            (None, 189504)            0         
    _________________________________________________________________
    dense_9 (Dense)              (None, 50)                9475250   
    _________________________________________________________________
    dense_10 (Dense)             (None, 2)                 102       
    =================================================================
    Total params: 9,480,872
    Trainable params: 9,480,872
    Non-trainable params: 0
    _________________________________________________________________


#### 모델 컴파일 – 이진 분류 모델


```python
#model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), 
model.compile(optimizer='adam', 
              loss='categorical_crossentropy', 
              metrics=['accuracy']) 
```

#### 모델 훈련(학습) 하기


```python
history = model.fit(training_generator, 
          epochs=3 ,
          steps_per_epoch = len(training_generator) / batch_size,
          validation_steps = len(test_generator) / batch_size,
          validation_data=test_generator, 
          verbose=1
)
```

    Epoch 1/3
    43/43 [==============================] - 55s 1s/step - loss: 4.3408 - accuracy: 0.4576 - val_loss: 0.6290 - val_accuracy: 0.7500
    Epoch 2/3
    43/43 [==============================] - 54s 1s/step - loss: 0.5992 - accuracy: 0.8250 - val_loss: 1.0628 - val_accuracy: 0.7045
    Epoch 3/3
    43/43 [==============================] - 53s 1s/step - loss: 0.6465 - accuracy: 0.8287 - val_loss: 0.5723 - val_accuracy: 0.7727


#### **성능 시각화 - 성능평가**


```python
losses = pd.DataFrame(model.history.history)
```


```python
losses.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss</th>
      <th>accuracy</th>
      <th>val_loss</th>
      <th>val_accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.943923</td>
      <td>0.500000</td>
      <td>0.628953</td>
      <td>0.750000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.553870</td>
      <td>0.846591</td>
      <td>1.062774</td>
      <td>0.704545</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.649390</td>
      <td>0.840909</td>
      <td>0.572314</td>
      <td>0.772727</td>
    </tr>
  </tbody>
</table>
</div>




```python
losses[['loss','val_loss']].plot()
```




    <AxesSubplot:>




    
![png](output_123_1.png)
    



```python
losses[['loss','val_loss', 'accuracy','val_accuracy']].plot()
```




    <AxesSubplot:>




    
![png](output_124_1.png)
    



```python
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Acc')
plt.legend(['acc', 'val_acc'])
plt.show()
```


    
![png](output_125_0.png)
    


#### **예측하기**


```python
# test_generator 샘플 데이터 가져오기
# 배치 사이즈 32 확인

batch_img, batch_label = next(iter(test_generator))
print(batch_img.shape)
print(batch_label.shape)
```

    (4, 384, 512, 3)
    (4, 2)



```python
# 4개 Test 샘플 이미지 그려보고 예측해 보기

i = 1 
plt.figure(figsize=(16, 30))
for img, label in list(zip(batch_img, batch_label)):
    pred = model.predict(img.reshape(-1,384, 512,3))
    pred_t = np.argmax(pred)
    plt.subplot(8, 4, i)
    plt.title(f'True Value:{np.argmax(label)}, Pred Value: {pred_t}')
    plt.imshow(img)   
    i = i + 1
```


    
![png](output_128_0.png)
    



```python

```


```python

```

###  
## 3) RNN 

![RNN](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Recurrent_neural_network_unfold.svg/1200px-Recurrent_neural_network_unfold.svg.png)
+ [출처] https://en.wikipedia.org/wiki/File:Recurrent_neural_network_unfold.svg


+ RNN은 주로 시계열 처리나 자연어 처리에 사용됩니다.
+ 우리 실습에 시계열 데이터나 자연어 관련 데이터가 없어 DNN에서 사용한 Tabular 데이터를 가지고 RNN 실습하도록 하겠습니다.

#### RNN 모델링


```python
#RNN 라이브러리 임포트

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.layers import LSTM
```


```python
X_train.shape, X_test.shape
```




    ((5469, 18), (2345, 18))




```python
X_train = X_train.reshape(-1,18,1)
X_test = X_test.reshape(-1,18,1)
```


```python
X_train.shape, X_test.shape
```




    ((5469, 18, 1), (2345, 18, 1))




```python
# define model
model = Sequential()
model.add(LSTM(32, activation='relu', return_sequences=True, input_shape=(18, 1)))
model.add(LSTM(16, activation='relu', return_sequences=True))
model.add(Flatten())
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
```


```python
model.summary()
```

    Model: "sequential_4"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    lstm (LSTM)                  (None, 18, 32)            4352      
    _________________________________________________________________
    lstm_1 (LSTM)                (None, 18, 16)            3136      
    _________________________________________________________________
    flatten_1 (Flatten)          (None, 288)               0         
    _________________________________________________________________
    dense_11 (Dense)             (None, 8)                 2312      
    _________________________________________________________________
    dense_12 (Dense)             (None, 1)                 9         
    =================================================================
    Total params: 9,809
    Trainable params: 9,809
    Non-trainable params: 0
    _________________________________________________________________


#### **모델 컴파일 – 이진 분류 모델**


```python
model.compile(optimizer='adam', 
              loss='binary_crossentropy', 
              metrics=['accuracy']) 
```

#### **모델 학습**


```python
history = model.fit(x=X_train, y=y_train, 
          epochs=10 , batch_size=128,
          validation_data=(X_test, y_test), 
          verbose=1
)
```

    Epoch 1/10
    43/43 [==============================] - 4s 69ms/step - loss: 0.6894 - accuracy: 0.7618 - val_loss: 0.6762 - val_accuracy: 0.8994
    Epoch 2/10
    43/43 [==============================] - 2s 55ms/step - loss: 0.6722 - accuracy: 0.8966 - val_loss: 0.6599 - val_accuracy: 0.8994
    Epoch 3/10
    43/43 [==============================] - 2s 55ms/step - loss: 0.6559 - accuracy: 0.9006 - val_loss: 0.6444 - val_accuracy: 0.8994
    Epoch 4/10
    43/43 [==============================] - 2s 55ms/step - loss: 0.6411 - accuracy: 0.8958 - val_loss: 0.6295 - val_accuracy: 0.8994
    Epoch 5/10
    43/43 [==============================] - 2s 55ms/step - loss: 0.6263 - accuracy: 0.8968 - val_loss: 0.6152 - val_accuracy: 0.8994
    Epoch 6/10
    43/43 [==============================] - 2s 54ms/step - loss: 0.6124 - accuracy: 0.8962 - val_loss: 0.6015 - val_accuracy: 0.8994
    Epoch 7/10
    43/43 [==============================] - 2s 53ms/step - loss: 0.5969 - accuracy: 0.9044 - val_loss: 0.5883 - val_accuracy: 0.8994
    Epoch 8/10
    43/43 [==============================] - 2s 54ms/step - loss: 0.5838 - accuracy: 0.9039 - val_loss: 0.5758 - val_accuracy: 0.8994
    Epoch 9/10
    43/43 [==============================] - 2s 54ms/step - loss: 0.5714 - accuracy: 0.9035 - val_loss: 0.5638 - val_accuracy: 0.8994
    Epoch 10/10
    43/43 [==============================] - 2s 54ms/step - loss: 0.5589 - accuracy: 0.9049 - val_loss: 0.5523 - val_accuracy: 0.8994


#### 성능 시각화 - 성능평가


```python
losses = pd.DataFrame(model.history.history)
```


```python
losses.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss</th>
      <th>accuracy</th>
      <th>val_loss</th>
      <th>val_accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.684950</td>
      <td>0.863595</td>
      <td>0.676207</td>
      <td>0.89936</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.668217</td>
      <td>0.899616</td>
      <td>0.659939</td>
      <td>0.89936</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.652319</td>
      <td>0.899616</td>
      <td>0.644379</td>
      <td>0.89936</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.637058</td>
      <td>0.899616</td>
      <td>0.629486</td>
      <td>0.89936</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.622426</td>
      <td>0.899616</td>
      <td>0.615195</td>
      <td>0.89936</td>
    </tr>
  </tbody>
</table>
</div>




```python
losses[['loss','val_loss']].plot()
```




    <AxesSubplot:>




    
![png](output_148_1.png)
    



```python
losses[['loss','val_loss', 'accuracy','val_accuracy']].plot()
```




    <AxesSubplot:>




    
![png](output_149_1.png)
    



```python
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Acc')
plt.legend(['acc', 'val_acc'])
plt.show()
```


    
![png](output_150_0.png)
    



```python

```


```python

```

## 배운 내용 정리
1. 딥러닝 모델 프로세스
① 라이브러리 임포트(import)  
② 데이터 가져오기(Loading the data)  
③ 탐색적 데이터 분석(Exploratory Data Analysis)  
④ 데이터 전처리(Data PreProcessing) : 데이터타입 변환, Null 데이터 처리, 누락데이터 처리, 
더미특성 생성, 특성 추출 (feature engineering) 등  
⑤ Train, Test  데이터셋 분할  
⑥ 데이터 정규화(Normalizing the Data)  
⑦ 모델 개발(Creating the Model)  
⑧ 모델 성능 평가
2. 딥러닝 모델 DNN, CNN, RNN 모델 구축
