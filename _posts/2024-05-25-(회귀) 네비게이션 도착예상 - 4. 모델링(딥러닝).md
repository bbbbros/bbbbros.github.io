---
published : false
---



# 데이터 불러오기부터

### 가. 라이브러리 Import


```python
import pandas as pd
import numpy as np
```

### 나. 데이터 불러오기


```python
df_feature = pd.read_csv("onenavi_train_feature.csv",sep="|")
df_target = pd.read_csv("onenavi_train_target.csv",sep="|")
```

### 나. Train/Test Data Split


```python
from sklearn.model_selection import train_test_split

# train_test_split
train_x, test_x, train_y, test_y = train_test_split(df_feature, df_target, test_size=0.20, random_state=42)
```

# 1. 회귀문제에 딥러닝 모델 적용하기
* 딥러닝은 머신 러닝의 특정한 한 분야로서 연속된 층layer에서 점진적으로 의미 있는 표현을 배우는 데 강점이 있으며, 데이터로부터 표현을 학습하는 새로운 방식입니다.
* 즉 계층적 표현 학습이라고도 할 수 있습니다.

[공식 Document 및 참고자료]
* 텐서 플로우 '자동차 연비 예측하기: 회귀'(https://www.tensorflow.org/tutorials/keras/regression?hl=ko)
* 텐서 플로우 블로그(https://tensorflow.blog/)

### 가. 모델링

[모델링에서 눈여겨 볼 것들!]
* activation : output이 다음 레이어로 전해지는 과정 중 역할을 수행하는 수학적인 게이트(gate)
![image-2.png](attachment:image-2.png)

* optimizer : 손실함수를 최소화하도록 도와주는 알고리즘
![image.png](attachment:image.png)

### 실습1. <u>각자 모델을 만들어보고 실습을 해보세요.</u>


```python
%pip install seaborn
```

    Collecting seaborn
      Downloading seaborn-0.11.2-py3-none-any.whl (292 kB)
         |████████████████████████████████| 292 kB 26.8 MB/s            
    [?25hRequirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.5.4)
    Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.19.5)
    Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.1.5)
    Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.6/dist-packages (from seaborn) (3.3.3)
    Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (8.1.0)
    Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (0.10.0)
    Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (2.8.1)
    Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (2.4.7)
    Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (1.3.1)
    Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23->seaborn) (2022.1)
    Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.15.0)
    Installing collected packages: seaborn
    Successfully installed seaborn-0.11.2
    [33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m
    Note: you may need to restart the kernel to use updated packages.



```python
# 모델 만들기 : 아주 간단한 모델
# 라이브러 import
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout
from keras.callbacks import ModelCheckpoint, EarlyStopping
```


```python
def build_model():
    model = Sequential()
    model.add(Dense(64, activation='relu', input_shape=(len(train_x.keys()),)))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(1))

    model.compile(loss='mse', 
                  optimizer='adam', 
                  metrics=['mae', 'mse'])
    return model
```


```python
model = build_model()
model.summary()
```

    Model: "sequential_7"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    dense_21 (Dense)             (None, 64)                6976      
    _________________________________________________________________
    dense_22 (Dense)             (None, 64)                4160      
    _________________________________________________________________
    dense_23 (Dense)             (None, 1)                 65        
    =================================================================
    Total params: 11,201
    Trainable params: 11,201
    Non-trainable params: 0
    _________________________________________________________________



```python
%pip install pydot
```

    Collecting pydot
      Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)
    Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot) (2.4.7)
    Installing collected packages: pydot
    Successfully installed pydot-1.4.2
    [33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m
    Note: you may need to restart the kernel to use updated packages.



```python
tf.keras.utils.plot_model(model, show_shapes=True)
```

    ('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')



```python
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
early_stop = EarlyStopping(monitor='val_loss', patience=10)
check_point = ModelCheckpoint('best_model.h5',
                              save_weights_only=True, 
                              monitor='val_loss', 
                              verbose=1,
                              save_best_only=True) # 체크포인트 저장
    
history = model.fit(train_x, train_y, epochs=30,  
                   validation_data = (test_x,test_y),
                    callbacks=[early_stop, check_point]
                    )
```

    Epoch 1/30
    1386/1386 [==============================] - 7s 5ms/step - loss: 593329.1393 - mae: 573.2043 - mse: 593329.1393 - val_loss: 121717.2734 - val_mae: 244.3500 - val_mse: 121717.2734
    
    Epoch 00001: val_loss improved from inf to 121717.27344, saving model to best_model.h5
    Epoch 2/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 103058.6646 - mae: 225.3200 - mse: 103058.6646 - val_loss: 98195.9297 - val_mae: 211.2344 - val_mse: 98195.9297
    
    Epoch 00002: val_loss improved from 121717.27344 to 98195.92969, saving model to best_model.h5
    Epoch 3/30
    1386/1386 [==============================] - 6s 4ms/step - loss: 88304.3689 - mae: 205.2097 - mse: 88304.3689 - val_loss: 90349.0000 - val_mae: 196.2987 - val_mse: 90349.0000
    
    Epoch 00003: val_loss improved from 98195.92969 to 90349.00000, saving model to best_model.h5
    Epoch 4/30
    1386/1386 [==============================] - 6s 4ms/step - loss: 80877.0562 - mae: 193.2907 - mse: 80877.0562 - val_loss: 84777.4453 - val_mae: 191.0167 - val_mse: 84777.4453
    
    Epoch 00004: val_loss improved from 90349.00000 to 84777.44531, saving model to best_model.h5
    Epoch 5/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 78533.2144 - mae: 185.7811 - mse: 78533.2144 - val_loss: 80881.2031 - val_mae: 183.8583 - val_mse: 80881.2031
    
    Epoch 00005: val_loss improved from 84777.44531 to 80881.20312, saving model to best_model.h5
    Epoch 6/30
    1386/1386 [==============================] - 7s 5ms/step - loss: 77998.1120 - mae: 181.6531 - mse: 77998.1120 - val_loss: 79450.5625 - val_mae: 176.9391 - val_mse: 79450.5625- mae: 181.7783 - mse: 78268
    
    Epoch 00006: val_loss improved from 80881.20312 to 79450.56250, saving model to best_model.h5
    Epoch 7/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 70560.8512 - mae: 176.6843 - mse: 70560.8512 - val_loss: 78093.2734 - val_mae: 176.3120 - val_mse: 78093.2734
    
    Epoch 00007: val_loss improved from 79450.56250 to 78093.27344, saving model to best_model.h5
    Epoch 8/30
    1386/1386 [==============================] - 6s 4ms/step - loss: 80288.7966 - mae: 178.3875 - mse: 80288.7966 - val_loss: 77053.9375 - val_mae: 175.5093 - val_mse: 77053.9375
    
    Epoch 00008: val_loss improved from 78093.27344 to 77053.93750, saving model to best_model.h5
    Epoch 9/30
    1386/1386 [==============================] - 6s 4ms/step - loss: 69323.7960 - mae: 175.3413 - mse: 69323.7960 - val_loss: 77332.7266 - val_mae: 179.4075 - val_mse: 77332.7266
    
    Epoch 00009: val_loss did not improve from 77053.93750
    Epoch 10/30
    1386/1386 [==============================] - 7s 5ms/step - loss: 70924.8052 - mae: 174.4118 - mse: 70924.8052 - val_loss: 77279.0234 - val_mae: 175.4523 - val_mse: 77279.0234
    
    Epoch 00010: val_loss did not improve from 77053.93750
    Epoch 11/30
    1386/1386 [==============================] - 7s 5ms/step - loss: 69712.4073 - mae: 174.3478 - mse: 69712.4073 - val_loss: 78725.5078 - val_mae: 186.0028 - val_mse: 78725.5078
    
    Epoch 00011: val_loss did not improve from 77053.93750
    Epoch 12/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 71413.5600 - mae: 175.5041 - mse: 71413.5600 - val_loss: 76772.2578 - val_mae: 175.9894 - val_mse: 76772.2578
    
    Epoch 00012: val_loss improved from 77053.93750 to 76772.25781, saving model to best_model.h5
    Epoch 13/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 68080.7048 - mae: 173.6786 - mse: 68080.7048 - val_loss: 76382.7812 - val_mae: 175.7033 - val_mse: 76382.7812
    
    Epoch 00013: val_loss improved from 76772.25781 to 76382.78125, saving model to best_model.h5
    Epoch 14/30
    1386/1386 [==============================] - 6s 4ms/step - loss: 71684.0425 - mae: 175.5767 - mse: 71684.0425 - val_loss: 76551.9062 - val_mae: 174.4230 - val_mse: 76551.9062
    
    Epoch 00014: val_loss did not improve from 76382.78125
    Epoch 15/30
    1386/1386 [==============================] - 6s 4ms/step - loss: 68012.0417 - mae: 173.2575 - mse: 68012.0417 - val_loss: 76326.3203 - val_mae: 174.4740 - val_mse: 76326.3203
    
    Epoch 00015: val_loss improved from 76382.78125 to 76326.32031, saving model to best_model.h5
    Epoch 16/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 69204.8748 - mae: 172.9420 - mse: 69204.8748 - val_loss: 76322.3906 - val_mae: 177.5919 - val_mse: 76322.3906
    
    Epoch 00016: val_loss improved from 76326.32031 to 76322.39062, saving model to best_model.h5
    Epoch 17/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 72090.7364 - mae: 174.9827 - mse: 72090.7364 - val_loss: 76033.9375 - val_mae: 176.5630 - val_mse: 76033.9375
    
    Epoch 00017: val_loss improved from 76322.39062 to 76033.93750, saving model to best_model.h5
    Epoch 18/30
    1386/1386 [==============================] - 6s 4ms/step - loss: 68294.8020 - mae: 171.7366 - mse: 68294.8020 - val_loss: 76337.6250 - val_mae: 179.1116 - val_mse: 76337.6250
    
    Epoch 00018: val_loss did not improve from 76033.93750
    Epoch 19/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 71656.7191 - mae: 173.6613 - mse: 71656.7191 - val_loss: 75996.2891 - val_mae: 174.8504 - val_mse: 75996.2891
    
    Epoch 00019: val_loss improved from 76033.93750 to 75996.28906, saving model to best_model.h5
    Epoch 20/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 71076.0464 - mae: 173.6597 - mse: 71076.0464 - val_loss: 75704.8203 - val_mae: 176.7341 - val_mse: 75704.8203
    
    Epoch 00020: val_loss improved from 75996.28906 to 75704.82031, saving model to best_model.h5
    Epoch 21/30
    1386/1386 [==============================] - 6s 4ms/step - loss: 71031.0362 - mae: 173.4920 - mse: 71031.0362 - val_loss: 75914.6250 - val_mae: 178.3625 - val_mse: 75914.6250
    
    Epoch 00021: val_loss did not improve from 75704.82031
    Epoch 22/30
    1386/1386 [==============================] - 6s 4ms/step - loss: 69643.1824 - mae: 173.8104 - mse: 69643.1824 - val_loss: 75695.0781 - val_mae: 173.2245 - val_mse: 75695.0781
    
    Epoch 00022: val_loss improved from 75704.82031 to 75695.07812, saving model to best_model.h5
    Epoch 23/30
    1386/1386 [==============================] - 6s 4ms/step - loss: 72244.6161 - mae: 173.9917 - mse: 72244.6161 - val_loss: 75574.6484 - val_mae: 174.1367 - val_mse: 75574.6484
    
    Epoch 00023: val_loss improved from 75695.07812 to 75574.64844, saving model to best_model.h5
    Epoch 24/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 66800.8024 - mae: 173.3260 - mse: 66800.8024 - val_loss: 75405.0078 - val_mae: 174.5164 - val_mse: 75405.0078
    
    Epoch 00024: val_loss improved from 75574.64844 to 75405.00781, saving model to best_model.h5
    Epoch 25/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 65774.3501 - mae: 171.4668 - mse: 65774.3501 - val_loss: 75618.3594 - val_mae: 177.9086 - val_mse: 75618.3594
    
    Epoch 00025: val_loss did not improve from 75405.00781
    Epoch 26/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 64830.9131 - mae: 170.5170 - mse: 64830.9131 - val_loss: 75538.0469 - val_mae: 176.2345 - val_mse: 75538.0469
    
    Epoch 00026: val_loss did not improve from 75405.00781
    Epoch 27/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 65951.8903 - mae: 170.7175 - mse: 65951.8903 - val_loss: 75575.7422 - val_mae: 176.9959 - val_mse: 75575.7422
    
    Epoch 00027: val_loss did not improve from 75405.00781
    Epoch 28/30
    1386/1386 [==============================] - 6s 4ms/step - loss: 67944.6472 - mae: 171.3108 - mse: 67944.6472 - val_loss: 75140.6953 - val_mae: 174.1938 - val_mse: 75140.6953
    
    Epoch 00028: val_loss improved from 75405.00781 to 75140.69531, saving model to best_model.h5
    Epoch 29/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 68487.2708 - mae: 172.4478 - mse: 68487.2708 - val_loss: 75331.8516 - val_mae: 174.9387 - val_mse: 75331.8516
    
    Epoch 00029: val_loss did not improve from 75140.69531
    Epoch 30/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 69988.0527 - mae: 173.1501 - mse: 69988.0527 - val_loss: 75463.1406 - val_mae: 177.6526 - val_mse: 75463.1406
    
    Epoch 00030: val_loss did not improve from 75140.69531



```python
history.history.keys()
```




    dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse'])




```python
plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0, 30), history.history["loss"], label="train_loss")
plt.plot(np.arange(0, 30), history.history["val_loss"], label="val_loss")
plt.title("Training Loss")
plt.xlabel("Epoch #")
plt.ylabel("Loss")
plt.legend()
plt.show()
```


![output_20_0](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/3b36a2e1-2f02-475a-b1d3-cd8c36da4e7e)




```python
plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0, 30), history.history["mae"], label="mae")
plt.plot(np.arange(0, 30), history.history["val_mae"], label="val_mae")
plt.title("Training Loss")
plt.xlabel("Epoch #")
plt.ylabel("Loss")
plt.legend()
plt.show()
```

    
![output_21_0](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/f6fb0d2c-d606-4af9-afe2-7576736b5679)


### 실습2. <u>만들어진 모델를 기반으로 모델파일로 저장해주세요.</u>


```python
# 아래에 실습코드를 작성하세요. 모델을 저장할 때는 다른 모델과 이름이 겹치지 않도록 다르게 저장해주세요.
# (Hint : model.save("Model.h5")
model.save("DeeplearningModel.h5")





```


```python

```
