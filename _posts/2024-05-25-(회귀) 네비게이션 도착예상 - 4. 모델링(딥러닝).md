---
published : false
---



# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°ë¶€í„°

### ê°€. ë¼ì´ë¸ŒëŸ¬ë¦¬ Import


```python
import pandas as pd
import numpy as np
```

### ë‚˜. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°


```python
df_feature = pd.read_csv("onenavi_train_feature.csv",sep="|")
df_target = pd.read_csv("onenavi_train_target.csv",sep="|")
```

### ë‚˜. Train/Test Data Split


```python
from sklearn.model_selection import train_test_split

# train_test_split
train_x, test_x, train_y, test_y = train_test_split(df_feature, df_target, test_size=0.20, random_state=42)
```

# 1. íšŒê·€ë¬¸ì œì— ë”¥ëŸ¬ë‹ ëª¨ë¸ ì ìš©í•˜ê¸°
* ë”¥ëŸ¬ë‹ì€ ë¨¸ì‹  ëŸ¬ë‹ì˜ íŠ¹ì •í•œ í•œ ë¶„ì•¼ë¡œì„œ ì—°ì†ëœ ì¸µlayerì—ì„œ ì ì§„ì ìœ¼ë¡œ ì˜ë¯¸ ìˆëŠ” í‘œí˜„ì„ ë°°ìš°ëŠ” ë° ê°•ì ì´ ìˆìœ¼ë©°, ë°ì´í„°ë¡œë¶€í„° í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ìƒˆë¡œìš´ ë°©ì‹ì…ë‹ˆë‹¤.
* ì¦‰ ê³„ì¸µì  í‘œí˜„ í•™ìŠµì´ë¼ê³ ë„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

[ê³µì‹ Document ë° ì°¸ê³ ìë£Œ]
* í…ì„œ í”Œë¡œìš° 'ìë™ì°¨ ì—°ë¹„ ì˜ˆì¸¡í•˜ê¸°: íšŒê·€'(https://www.tensorflow.org/tutorials/keras/regression?hl=ko)
* í…ì„œ í”Œë¡œìš° ë¸”ë¡œê·¸(https://tensorflow.blog/)

### ê°€. ëª¨ë¸ë§

[ëª¨ë¸ë§ì—ì„œ ëˆˆì—¬ê²¨ ë³¼ ê²ƒë“¤!]
* activation : outputì´ ë‹¤ìŒ ë ˆì´ì–´ë¡œ ì „í•´ì§€ëŠ” ê³¼ì • ì¤‘ ì—­í• ì„ ìˆ˜í–‰í•˜ëŠ” ìˆ˜í•™ì ì¸ ê²Œì´íŠ¸(gate)
![image-2.png](attachment:image-2.png)

* optimizer : ì†ì‹¤í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ë„ë¡ ë„ì™€ì£¼ëŠ” ì•Œê³ ë¦¬ì¦˜
![image.png](attachment:image.png)

### ì‹¤ìŠµ1. <u>ê°ì ëª¨ë¸ì„ ë§Œë“¤ì–´ë³´ê³  ì‹¤ìŠµì„ í•´ë³´ì„¸ìš”.</u>


```python
%pip install seaborn
```

    Collecting seaborn
      Downloading seaborn-0.11.2-py3-none-any.whl (292 kB)
         |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 292 kB 26.8 MB/s            
    [?25hRequirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.5.4)
    Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.19.5)
    Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.1.5)
    Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.6/dist-packages (from seaborn) (3.3.3)
    Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (8.1.0)
    Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (0.10.0)
    Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (2.8.1)
    Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (2.4.7)
    Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (1.3.1)
    Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23->seaborn) (2022.1)
    Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.15.0)
    Installing collected packages: seaborn
    Successfully installed seaborn-0.11.2
    [33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m
    Note: you may need to restart the kernel to use updated packages.



```python
# ëª¨ë¸ ë§Œë“¤ê¸° : ì•„ì£¼ ê°„ë‹¨í•œ ëª¨ë¸
# ë¼ì´ë¸ŒëŸ¬ import
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout
from keras.callbacks import ModelCheckpoint, EarlyStopping
```


```python
def build_model():
    model = Sequential()
    model.add(Dense(64, activation='relu', input_shape=(len(train_x.keys()),)))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(1))

    model.compile(loss='mse', 
                  optimizer='adam', 
                  metrics=['mae', 'mse'])
    return model
```


```python
model = build_model()
model.summary()
```

    Model: "sequential_7"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    dense_21 (Dense)             (None, 64)                6976      
    _________________________________________________________________
    dense_22 (Dense)             (None, 64)                4160      
    _________________________________________________________________
    dense_23 (Dense)             (None, 1)                 65        
    =================================================================
    Total params: 11,201
    Trainable params: 11,201
    Non-trainable params: 0
    _________________________________________________________________



```python
%pip install pydot
```

    Collecting pydot
      Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)
    Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot) (2.4.7)
    Installing collected packages: pydot
    Successfully installed pydot-1.4.2
    [33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m
    Note: you may need to restart the kernel to use updated packages.



```python
tf.keras.utils.plot_model(model, show_shapes=True)
```

    ('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')



```python
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
early_stop = EarlyStopping(monitor='val_loss', patience=10)
check_point = ModelCheckpoint('best_model.h5',
                              save_weights_only=True, 
                              monitor='val_loss', 
                              verbose=1,
                              save_best_only=True) # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    
history = model.fit(train_x, train_y, epochs=30,  
                   validation_data = (test_x,test_y),
                    callbacks=[early_stop, check_point]
                    )
```

    Epoch 1/30
    1386/1386 [==============================] - 7s 5ms/step - loss: 593329.1393 - mae: 573.2043 - mse: 593329.1393 - val_loss: 121717.2734 - val_mae: 244.3500 - val_mse: 121717.2734
    
    Epoch 00001: val_loss improved from inf to 121717.27344, saving model to best_model.h5
    Epoch 2/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 103058.6646 - mae: 225.3200 - mse: 103058.6646 - val_loss: 98195.9297 - val_mae: 211.2344 - val_mse: 98195.9297
    
    Epoch 00002: val_loss improved from 121717.27344 to 98195.92969, saving model to best_model.h5
    Epoch 3/30
    1386/1386 [==============================] - 6s 4ms/step - loss: 88304.3689 - mae: 205.2097 - mse: 88304.3689 - val_loss: 90349.0000 - val_mae: 196.2987 - val_mse: 90349.0000
    
    Epoch 00003: val_loss improved from 98195.92969 to 90349.00000, saving model to best_model.h5
    Epoch 4/30
    1386/1386 [==============================] - 6s 4ms/step - loss: 80877.0562 - mae: 193.2907 - mse: 80877.0562 - val_loss: 84777.4453 - val_mae: 191.0167 - val_mse: 84777.4453
    
    Epoch 00004: val_loss improved from 90349.00000 to 84777.44531, saving model to best_model.h5
    Epoch 5/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 78533.2144 - mae: 185.7811 - mse: 78533.2144 - val_loss: 80881.2031 - val_mae: 183.8583 - val_mse: 80881.2031
    
    Epoch 00005: val_loss improved from 84777.44531 to 80881.20312, saving model to best_model.h5
    Epoch 6/30
    1386/1386 [==============================] - 7s 5ms/step - loss: 77998.1120 - mae: 181.6531 - mse: 77998.1120 - val_loss: 79450.5625 - val_mae: 176.9391 - val_mse: 79450.5625- mae: 181.7783 - mse: 78268
    
    Epoch 00006: val_loss improved from 80881.20312 to 79450.56250, saving model to best_model.h5
    Epoch 7/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 70560.8512 - mae: 176.6843 - mse: 70560.8512 - val_loss: 78093.2734 - val_mae: 176.3120 - val_mse: 78093.2734
    
    Epoch 00007: val_loss improved from 79450.56250 to 78093.27344, saving model to best_model.h5
    Epoch 8/30
    1386/1386 [==============================] - 6s 4ms/step - loss: 80288.7966 - mae: 178.3875 - mse: 80288.7966 - val_loss: 77053.9375 - val_mae: 175.5093 - val_mse: 77053.9375
    
    Epoch 00008: val_loss improved from 78093.27344 to 77053.93750, saving model to best_model.h5
    Epoch 9/30
    1386/1386 [==============================] - 6s 4ms/step - loss: 69323.7960 - mae: 175.3413 - mse: 69323.7960 - val_loss: 77332.7266 - val_mae: 179.4075 - val_mse: 77332.7266
    
    Epoch 00009: val_loss did not improve from 77053.93750
    Epoch 10/30
    1386/1386 [==============================] - 7s 5ms/step - loss: 70924.8052 - mae: 174.4118 - mse: 70924.8052 - val_loss: 77279.0234 - val_mae: 175.4523 - val_mse: 77279.0234
    
    Epoch 00010: val_loss did not improve from 77053.93750
    Epoch 11/30
    1386/1386 [==============================] - 7s 5ms/step - loss: 69712.4073 - mae: 174.3478 - mse: 69712.4073 - val_loss: 78725.5078 - val_mae: 186.0028 - val_mse: 78725.5078
    
    Epoch 00011: val_loss did not improve from 77053.93750
    Epoch 12/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 71413.5600 - mae: 175.5041 - mse: 71413.5600 - val_loss: 76772.2578 - val_mae: 175.9894 - val_mse: 76772.2578
    
    Epoch 00012: val_loss improved from 77053.93750 to 76772.25781, saving model to best_model.h5
    Epoch 13/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 68080.7048 - mae: 173.6786 - mse: 68080.7048 - val_loss: 76382.7812 - val_mae: 175.7033 - val_mse: 76382.7812
    
    Epoch 00013: val_loss improved from 76772.25781 to 76382.78125, saving model to best_model.h5
    Epoch 14/30
    1386/1386 [==============================] - 6s 4ms/step - loss: 71684.0425 - mae: 175.5767 - mse: 71684.0425 - val_loss: 76551.9062 - val_mae: 174.4230 - val_mse: 76551.9062
    
    Epoch 00014: val_loss did not improve from 76382.78125
    Epoch 15/30
    1386/1386 [==============================] - 6s 4ms/step - loss: 68012.0417 - mae: 173.2575 - mse: 68012.0417 - val_loss: 76326.3203 - val_mae: 174.4740 - val_mse: 76326.3203
    
    Epoch 00015: val_loss improved from 76382.78125 to 76326.32031, saving model to best_model.h5
    Epoch 16/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 69204.8748 - mae: 172.9420 - mse: 69204.8748 - val_loss: 76322.3906 - val_mae: 177.5919 - val_mse: 76322.3906
    
    Epoch 00016: val_loss improved from 76326.32031 to 76322.39062, saving model to best_model.h5
    Epoch 17/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 72090.7364 - mae: 174.9827 - mse: 72090.7364 - val_loss: 76033.9375 - val_mae: 176.5630 - val_mse: 76033.9375
    
    Epoch 00017: val_loss improved from 76322.39062 to 76033.93750, saving model to best_model.h5
    Epoch 18/30
    1386/1386 [==============================] - 6s 4ms/step - loss: 68294.8020 - mae: 171.7366 - mse: 68294.8020 - val_loss: 76337.6250 - val_mae: 179.1116 - val_mse: 76337.6250
    
    Epoch 00018: val_loss did not improve from 76033.93750
    Epoch 19/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 71656.7191 - mae: 173.6613 - mse: 71656.7191 - val_loss: 75996.2891 - val_mae: 174.8504 - val_mse: 75996.2891
    
    Epoch 00019: val_loss improved from 76033.93750 to 75996.28906, saving model to best_model.h5
    Epoch 20/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 71076.0464 - mae: 173.6597 - mse: 71076.0464 - val_loss: 75704.8203 - val_mae: 176.7341 - val_mse: 75704.8203
    
    Epoch 00020: val_loss improved from 75996.28906 to 75704.82031, saving model to best_model.h5
    Epoch 21/30
    1386/1386 [==============================] - 6s 4ms/step - loss: 71031.0362 - mae: 173.4920 - mse: 71031.0362 - val_loss: 75914.6250 - val_mae: 178.3625 - val_mse: 75914.6250
    
    Epoch 00021: val_loss did not improve from 75704.82031
    Epoch 22/30
    1386/1386 [==============================] - 6s 4ms/step - loss: 69643.1824 - mae: 173.8104 - mse: 69643.1824 - val_loss: 75695.0781 - val_mae: 173.2245 - val_mse: 75695.0781
    
    Epoch 00022: val_loss improved from 75704.82031 to 75695.07812, saving model to best_model.h5
    Epoch 23/30
    1386/1386 [==============================] - 6s 4ms/step - loss: 72244.6161 - mae: 173.9917 - mse: 72244.6161 - val_loss: 75574.6484 - val_mae: 174.1367 - val_mse: 75574.6484
    
    Epoch 00023: val_loss improved from 75695.07812 to 75574.64844, saving model to best_model.h5
    Epoch 24/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 66800.8024 - mae: 173.3260 - mse: 66800.8024 - val_loss: 75405.0078 - val_mae: 174.5164 - val_mse: 75405.0078
    
    Epoch 00024: val_loss improved from 75574.64844 to 75405.00781, saving model to best_model.h5
    Epoch 25/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 65774.3501 - mae: 171.4668 - mse: 65774.3501 - val_loss: 75618.3594 - val_mae: 177.9086 - val_mse: 75618.3594
    
    Epoch 00025: val_loss did not improve from 75405.00781
    Epoch 26/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 64830.9131 - mae: 170.5170 - mse: 64830.9131 - val_loss: 75538.0469 - val_mae: 176.2345 - val_mse: 75538.0469
    
    Epoch 00026: val_loss did not improve from 75405.00781
    Epoch 27/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 65951.8903 - mae: 170.7175 - mse: 65951.8903 - val_loss: 75575.7422 - val_mae: 176.9959 - val_mse: 75575.7422
    
    Epoch 00027: val_loss did not improve from 75405.00781
    Epoch 28/30
    1386/1386 [==============================] - 6s 4ms/step - loss: 67944.6472 - mae: 171.3108 - mse: 67944.6472 - val_loss: 75140.6953 - val_mae: 174.1938 - val_mse: 75140.6953
    
    Epoch 00028: val_loss improved from 75405.00781 to 75140.69531, saving model to best_model.h5
    Epoch 29/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 68487.2708 - mae: 172.4478 - mse: 68487.2708 - val_loss: 75331.8516 - val_mae: 174.9387 - val_mse: 75331.8516
    
    Epoch 00029: val_loss did not improve from 75140.69531
    Epoch 30/30
    1386/1386 [==============================] - 6s 5ms/step - loss: 69988.0527 - mae: 173.1501 - mse: 69988.0527 - val_loss: 75463.1406 - val_mae: 177.6526 - val_mse: 75463.1406
    
    Epoch 00030: val_loss did not improve from 75140.69531



```python
history.history.keys()
```




    dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse'])




```python
plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0, 30), history.history["loss"], label="train_loss")
plt.plot(np.arange(0, 30), history.history["val_loss"], label="val_loss")
plt.title("Training Loss")
plt.xlabel("Epoch #")
plt.ylabel("Loss")
plt.legend()
plt.show()
```


![output_20_0](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/3b36a2e1-2f02-475a-b1d3-cd8c36da4e7e)




```python
plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0, 30), history.history["mae"], label="mae")
plt.plot(np.arange(0, 30), history.history["val_mae"], label="val_mae")
plt.title("Training Loss")
plt.xlabel("Epoch #")
plt.ylabel("Loss")
plt.legend()
plt.show()
```

    
![output_21_0](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/f6fb0d2c-d606-4af9-afe2-7576736b5679)


### ì‹¤ìŠµ2. <u>ë§Œë“¤ì–´ì§„ ëª¨ë¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸íŒŒì¼ë¡œ ì €ì¥í•´ì£¼ì„¸ìš”.</u>


```python
# ì•„ë˜ì— ì‹¤ìŠµì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”. ëª¨ë¸ì„ ì €ì¥í•  ë•ŒëŠ” ë‹¤ë¥¸ ëª¨ë¸ê³¼ ì´ë¦„ì´ ê²¹ì¹˜ì§€ ì•Šë„ë¡ ë‹¤ë¥´ê²Œ ì €ì¥í•´ì£¼ì„¸ìš”.
# (Hint : model.save("Model.h5")
model.save("DeeplearningModel.h5")





```


```python

```
