# [실습-퀴즈] Python을 활용한 AI 모델링 - 딥러닝 파트
+ 이번시간에는 Python을 활용한 AI 모델링에서 딥러닝에 대해 실습해 보겠습니다.
+ 여기서는 딥러닝 모델 DNN에 대해 코딩하여 모델 구축해 보겠습니다.
+ 한가지 당부 드리고 싶은 말은 "백문이불여일타" 입니다. 
+ 이론보다 실습이 더 많은 시간과 노력이 투자 되어야 합니다.

## 학습목차
1. 실습준비
2. 딥러닝 심층신경망(DNN) 모델 프로세스
 - 데이터 가져오기
 - 데이터 전처리
 - Train, Test 데이터셋 분할
 - 데이터 정규화
 - DNN 딥러닝 모델

# 
# 1. 실습준비


```python
# 코드실행시 경고 메시지 무시

import warnings
warnings.filterwarnings(action='ignore') 
```

# 
# 2. 딥러닝 심층신경망(DNN) 모델 프로세스
① 라이브러리 임포트(import)  
② 데이터 가져오기(Loading the data)  
③ 탐색적 데이터 분석(Exploratory Data Analysis)  
④ 데이터 전처리(Data PreProcessing) : 데이터타입 변환, Null 데이터 처리, 누락데이터 처리, 
더미특성 생성, 특성 추출 (feature engineering) 등  
⑤ Train, Test  데이터셋 분할  
⑥ 데이터 정규화(Normalizing the Data)  
⑦ 모델 개발(Creating the Model)  
⑧ 모델 성능 평가

## ① 라이브러리 임포트

##### 필요 라이브러리 임포트


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

## ② 데이터 로드

#### cust_data.csv 파일 컬럼명
+ 고객등급(class), 성별(sex), 나이(age), 사용서비스수(service), 서비스중지여부 (stop), 미납여부(npay)
+ 3개월 평균 요금(avg_bill), A서비스 3개월 평균요금(A_bill), B서비스 3개월 평균요금(B_bill), 해지여부(termination)

##### <font color=blue> **[문제] 같은 폴더내에 있는 cust_data.csv 파일을 Pandas read_csv 함수를 이용하여 읽어 df 변수에 저장하세요.** </font>


```python
# 읽어 들일 파일명 : cust_data.csv
# Pandas read_csv 함수 활용
# 결과 : df 저장
df = pd.read_csv('cust_data.csv')
df.info()


```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 7814 entries, 0 to 7813
    Data columns (total 12 columns):
     #   Column       Non-Null Count  Dtype  
    ---  ------       --------------  -----  
     0   class        7814 non-null   object 
     1   sex          7814 non-null   object 
     2   age          7814 non-null   int64  
     3   service      7814 non-null   int64  
     4   stop         7814 non-null   object 
     5   npay         7814 non-null   object 
     6   avg_bill     7814 non-null   float64
     7   A_bill       7814 non-null   float64
     8   B_bill       7814 non-null   float64
     9   termination  7814 non-null   object 
     10  by_age       7814 non-null   int64  
     11  bill_rating  7814 non-null   object 
    dtypes: float64(3), int64(3), object(6)
    memory usage: 732.7+ KB


## ③ 데이터 분석


```python
# 10컬럼, 9930 라인
df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 7814 entries, 0 to 7813
    Data columns (total 12 columns):
     #   Column       Non-Null Count  Dtype  
    ---  ------       --------------  -----  
     0   class        7814 non-null   object 
     1   sex          7814 non-null   object 
     2   age          7814 non-null   int64  
     3   service      7814 non-null   int64  
     4   stop         7814 non-null   object 
     5   npay         7814 non-null   object 
     6   avg_bill     7814 non-null   float64
     7   A_bill       7814 non-null   float64
     8   B_bill       7814 non-null   float64
     9   termination  7814 non-null   object 
     10  by_age       7814 non-null   int64  
     11  bill_rating  7814 non-null   object 
    dtypes: float64(3), int64(3), object(6)
    memory usage: 732.7+ KB



```python
df.tail()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>class</th>
      <th>sex</th>
      <th>age</th>
      <th>service</th>
      <th>stop</th>
      <th>npay</th>
      <th>avg_bill</th>
      <th>A_bill</th>
      <th>B_bill</th>
      <th>termination</th>
      <th>by_age</th>
      <th>bill_rating</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>7809</th>
      <td>C</td>
      <td>M</td>
      <td>76</td>
      <td>1</td>
      <td>N</td>
      <td>N</td>
      <td>1860.0000</td>
      <td>1716.000000</td>
      <td>0.0000</td>
      <td>N</td>
      <td>75</td>
      <td>low</td>
    </tr>
    <tr>
      <th>7810</th>
      <td>C</td>
      <td>F</td>
      <td>15</td>
      <td>1</td>
      <td>N</td>
      <td>Y</td>
      <td>1296.0999</td>
      <td>194.414985</td>
      <td>643.1001</td>
      <td>N</td>
      <td>15</td>
      <td>low</td>
    </tr>
    <tr>
      <th>7811</th>
      <td>G</td>
      <td>M</td>
      <td>12</td>
      <td>1</td>
      <td>N</td>
      <td>N</td>
      <td>13799.6666</td>
      <td>2069.949990</td>
      <td>10605.9266</td>
      <td>N</td>
      <td>10</td>
      <td>midhigh</td>
    </tr>
    <tr>
      <th>7812</th>
      <td>C</td>
      <td>F</td>
      <td>40</td>
      <td>0</td>
      <td>N</td>
      <td>N</td>
      <td>3140.0000</td>
      <td>942.000000</td>
      <td>1884.0000</td>
      <td>Y</td>
      <td>40</td>
      <td>low</td>
    </tr>
    <tr>
      <th>7813</th>
      <td>C</td>
      <td>F</td>
      <td>59</td>
      <td>1</td>
      <td>N</td>
      <td>N</td>
      <td>2436.9000</td>
      <td>365.535000</td>
      <td>1839.9000</td>
      <td>N</td>
      <td>55</td>
      <td>low</td>
    </tr>
  </tbody>
</table>
</div>




```python
# termination 레이블 불균형  타겟변수
df['termination'].value_counts().plot(kind='bar')
```




    <AxesSubplot:>


![output_15_1](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/fe207cfd-481d-4e5d-bf35-6126fa896b51)



## ④ 데이터 전처리

+ Object 컬럼에 대해 Pandas get_dummies 함수 활용하여 One-Hot-Encoding


```python
# Object 컬럼 리스트 정의
cal_cols = ['class', 'sex', 'stop', 'npay', 'termination', 'bill_rating'] 

```

##### <font color=blue> **[문제] ['class', 'sex', 'stop', 'npay', 'termination', 'bill_rating'] 컬럼에 대해 One-Hot-Encoding 수행하고 그 결과를 df1 변수에 저장하세요.** </font>


```python
# ['class', 'sex', 'stop', 'npay', 'termination', 'bill_rating'] : cal_cols 변수에 대해 One-Hot-Endcoding 수행
# One-Hot-Endcoding 수행 : pandas get_dummies() 함수 이용
# get_dummies() 함수 옵션 : data=df, columns=cal_cols, drop_first=True
# 결과 : df1 저장

# df1 = pd.get_dummies(df, columns=['class', 'sex', 'stop', 'npay', 'termination', 'bill_rating'] ,drop_first=True)

df1 = pd.get_dummies(df, columns = cal_cols, drop_first=True)

```


```python
# 19컬럼, 7814 라인
df1.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 7814 entries, 0 to 7813
    Data columns (total 19 columns):
     #   Column               Non-Null Count  Dtype  
    ---  ------               --------------  -----  
     0   age                  7814 non-null   int64  
     1   service              7814 non-null   int64  
     2   avg_bill             7814 non-null   float64
     3   A_bill               7814 non-null   float64
     4   B_bill               7814 non-null   float64
     5   by_age               7814 non-null   int64  
     6   class_D              7814 non-null   uint8  
     7   class_E              7814 non-null   uint8  
     8   class_F              7814 non-null   uint8  
     9   class_G              7814 non-null   uint8  
     10  class_H              7814 non-null   uint8  
     11  sex_M                7814 non-null   uint8  
     12  stop_Y               7814 non-null   uint8  
     13  npay_Y               7814 non-null   uint8  
     14  termination_Y        7814 non-null   uint8  
     15  bill_rating_low      7814 non-null   uint8  
     16  bill_rating_lowmid   7814 non-null   uint8  
     17  bill_rating_mid      7814 non-null   uint8  
     18  bill_rating_midhigh  7814 non-null   uint8  
    dtypes: float64(3), int64(3), uint8(13)
    memory usage: 465.6 KB


## ⑤ Train, Test  데이터셋 분할


```python
from sklearn.model_selection import train_test_split
```


```python
# 입력값과 결과값을 구분하여 변수에 넣는다.
from sklearn.model_selection import train_test_split
X = df1.drop(columns = 'termination_Y', axis=1).values
y = df1['termination_Y'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                    test_size=0.3,
                                                    stratify=y,
                                                    random_state=42)
```

## ⑥ 데이터 정규화/스케일링(Normalizing/Scaling)


```python
from sklearn.preprocessing import MinMaxScaler 
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
```


```python
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
```


```python
X_train
```




    array([[3.80000000e-01, 3.33333333e-01, 4.29543900e-01, ...,
            0.00000000e+00, 0.00000000e+00, 1.00000000e+00],
           [5.80000000e-01, 1.11111111e-01, 2.01112973e-01, ...,
            1.00000000e+00, 0.00000000e+00, 0.00000000e+00],
           [5.10000000e-01, 2.22222222e-01, 5.02527096e-01, ...,
            0.00000000e+00, 0.00000000e+00, 1.00000000e+00],
           ...,
           [2.70000000e-01, 1.11111111e-01, 3.54623358e-06, ...,
            0.00000000e+00, 0.00000000e+00, 0.00000000e+00],
           [7.60000000e-01, 1.11111111e-01, 5.49328940e-02, ...,
            0.00000000e+00, 0.00000000e+00, 0.00000000e+00],
           [3.70000000e-01, 2.22222222e-01, 6.62643974e-01, ...,
            0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])




```python
X_test
```




    array([[0.19      , 0.11111111, 0.36233703, ..., 0.        , 1.        ,
            0.        ],
           [0.74      , 0.11111111, 0.05363722, ..., 0.        , 0.        ,
            0.        ],
           [0.72      , 0.22222222, 0.311307  , ..., 0.        , 1.        ,
            0.        ],
           ...,
           [0.58      , 0.22222222, 0.26642899, ..., 0.        , 1.        ,
            0.        ],
           [0.23      , 0.11111111, 0.24478714, ..., 0.        , 1.        ,
            0.        ],
           [0.28      , 0.11111111, 0.52512655, ..., 0.        , 0.        ,
            0.        ]])




```python
df1.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 7814 entries, 0 to 7813
    Data columns (total 19 columns):
     #   Column               Non-Null Count  Dtype  
    ---  ------               --------------  -----  
     0   age                  7814 non-null   int64  
     1   service              7814 non-null   int64  
     2   avg_bill             7814 non-null   float64
     3   A_bill               7814 non-null   float64
     4   B_bill               7814 non-null   float64
     5   by_age               7814 non-null   int64  
     6   class_D              7814 non-null   uint8  
     7   class_E              7814 non-null   uint8  
     8   class_F              7814 non-null   uint8  
     9   class_G              7814 non-null   uint8  
     10  class_H              7814 non-null   uint8  
     11  sex_M                7814 non-null   uint8  
     12  stop_Y               7814 non-null   uint8  
     13  npay_Y               7814 non-null   uint8  
     14  termination_Y        7814 non-null   uint8  
     15  bill_rating_low      7814 non-null   uint8  
     16  bill_rating_lowmid   7814 non-null   uint8  
     17  bill_rating_mid      7814 non-null   uint8  
     18  bill_rating_midhigh  7814 non-null   uint8  
    dtypes: float64(3), int64(3), uint8(13)
    memory usage: 465.6 KB



```python
X_train[:2]
```




    array([[0.38      , 0.33333333, 0.4295439 , 0.06384702, 0.41944434,
            0.4       , 0.        , 0.        , 0.        , 0.        ,
            1.        , 1.        , 0.        , 1.        , 0.        ,
            0.        , 0.        , 1.        ],
           [0.58      , 0.11111111, 0.20111297, 0.38498933, 0.        ,
            0.6       , 1.        , 0.        , 0.        , 0.        ,
            0.        , 1.        , 0.        , 0.        , 0.        ,
            1.        , 0.        , 0.        ]])



#### 
## ⑦ 딥러닝 심층신경망(DNN) 모델 구현

#### 
### 라이브러리 임포트


```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout
```

#### 
### 하이퍼파라미터 설정 : batch_size, epochs


```python
batch_size = 16
epochs = 20
```

#### 
### 모델 입력(features) 갯수 확인


```python
X_train.shape
```




    (5469, 18)



#### 
### 모델 출력(label) 갯수 확인


```python
y_train.shape
```




    (5469,)



#### 
### A. 이진분류 DNN모델 구성 

![hidden Layer](https://github.com/gzone2000/TEMP_TEST/raw/master/hidden_layer1.PNG)
+ [출처] https://subscription.packtpub.com/book/data/9781788995207/1/ch01lvl1sec03/deep-learning-intuition

##### <font color=blue> **[문제] 요구사항대로 Sequential 모델을 만들어 보세요.** </font>


```python
# Sequential() 모델 정의 하고 model로 저장
# input layer는 input_shape=() 옵션을 사용한다.
# 18개 input layer
# unit 4개 hidden layer
# unit 3개 hidden layer 
# 1개 output layser : 이진분류
```


```python
model = Sequential()

# 첫번째 히든레이어
model.add(Dense(4,activation='relu',input_shape=(18,)))
model.add(Dropout(0.3))
model.add(Dense(3,activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(1,activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, epochs=epochs)
```

    Epoch 1/20
    342/342 [==============================] - 1s 2ms/step - loss: 0.6332 - acc: 0.6584 - val_loss: 0.3514 - val_acc: 0.8994
    Epoch 2/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.4160 - acc: 0.8641 - val_loss: 0.3097 - val_acc: 0.8994
    Epoch 3/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.3730 - acc: 0.8999 - val_loss: 0.2988 - val_acc: 0.8994
    Epoch 4/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.3477 - acc: 0.9030 - val_loss: 0.2925 - val_acc: 0.8994
    Epoch 5/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.3406 - acc: 0.8996 - val_loss: 0.2810 - val_acc: 0.8994
    Epoch 6/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.3202 - acc: 0.9016 - val_loss: 0.2715 - val_acc: 0.8994
    Epoch 7/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.3016 - acc: 0.9021 - val_loss: 0.2599 - val_acc: 0.8994
    Epoch 8/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2971 - acc: 0.8998 - val_loss: 0.2355 - val_acc: 0.8994
    Epoch 9/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2814 - acc: 0.9021 - val_loss: 0.2077 - val_acc: 0.8994
    Epoch 10/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2622 - acc: 0.9169 - val_loss: 0.1944 - val_acc: 0.8998
    Epoch 11/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2329 - acc: 0.9305 - val_loss: 0.1724 - val_acc: 0.9203
    Epoch 12/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2374 - acc: 0.9219 - val_loss: 0.1676 - val_acc: 0.9173
    Epoch 13/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2125 - acc: 0.9335 - val_loss: 0.1613 - val_acc: 0.9394
    Epoch 14/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2097 - acc: 0.9350 - val_loss: 0.1549 - val_acc: 0.9450
    Epoch 15/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2099 - acc: 0.9350 - val_loss: 0.1521 - val_acc: 0.9458
    Epoch 16/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2058 - acc: 0.9351 - val_loss: 0.1484 - val_acc: 0.9531
    Epoch 17/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2202 - acc: 0.9299 - val_loss: 0.1493 - val_acc: 0.9527
    Epoch 18/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2038 - acc: 0.9344 - val_loss: 0.1495 - val_acc: 0.9467
    Epoch 19/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2238 - acc: 0.9232 - val_loss: 0.1485 - val_acc: 0.9441
    Epoch 20/20
    342/342 [==============================] - 0s 1ms/step - loss: 0.2048 - acc: 0.9342 - val_loss: 0.1454 - val_acc: 0.9531


#### 
### 모델 확인


```python
model.summary()
```

    Model: "sequential"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    dense (Dense)                (None, 4)                 76        
    _________________________________________________________________
    dropout (Dropout)            (None, 4)                 0         
    _________________________________________________________________
    dense_1 (Dense)              (None, 3)                 15        
    _________________________________________________________________
    dropout_1 (Dropout)          (None, 3)                 0         
    _________________________________________________________________
    dense_2 (Dense)              (None, 1)                 4         
    =================================================================
    Total params: 95
    Trainable params: 95
    Non-trainable params: 0
    _________________________________________________________________


#### 
### 모델 구성 -  과적합 방지

![dropout](https://github.com/gzone2000/TEMP_TEST/raw/master/dropout.PNG)
+ [출처] https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5


```python
model = Sequential()
model.add(Dense(4, activation='relu', input_shape=(18,)))
model.add(Dropout(0.3))
model.add(Dense(3, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(1, activation='sigmoid'))
```

#### 
### 과적합 방지 모델 확인


```python
model.summary()
```

    Model: "sequential_1"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    dense_3 (Dense)              (None, 4)                 76        
    _________________________________________________________________
    dropout_2 (Dropout)          (None, 4)                 0         
    _________________________________________________________________
    dense_4 (Dense)              (None, 3)                 15        
    _________________________________________________________________
    dropout_3 (Dropout)          (None, 3)                 0         
    _________________________________________________________________
    dense_5 (Dense)              (None, 1)                 4         
    =================================================================
    Total params: 95
    Trainable params: 95
    Non-trainable params: 0
    _________________________________________________________________


#### 
### 모델 컴파일 – 이진 분류 모델


```python
model.compile(optimizer='adam', 
              loss='binary_crossentropy', 
              metrics=['accuracy']) 
```

- 모델 컴파일 – 다중 분류 모델 (Y값을 One-Hot-Encoding 한경우) <br>
model.compile(optimizer='adam', 
              loss='categorical_crossentropy', 
              metrics=['accuracy']) 

- 모델 컴파일 – 다중 분류 모델  (Y값을 One-Hot-Encoding 하지 않은 경우) <br>
model.compile(optimizer='adam', 
              loss='sparse_categorical_crossentropy', 
              metrics=['accuracy'])

- 모델 컴파일 – 예측 모델
model.compile(optimizer='adam', loss='mse')

#### 
### 모델 학습

##### <font color=blue> **[문제] 요구사항대로 DNN 모델을 학습시키세요.** </font>
+ 모델 이름 : model
+ epoch : 10번
+ batch_size : 10번


```python
# 앞쪽에서 정의된 모델 이름 : model
# Sequential 모델의 fit() 함수 사용
# @인자
### X, y : X_train, y_train
### validation_data=(X_test, y_test)
### epochs 10번
### batch_size 10번


import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout

epochs = 20
batch_size = 20
```

#### 
### B. 다중 분류 DNN 구성
+ 18개 input layer
+ unit 5개 hidden layer
+ dropout
+ unit 4개 hidden layer 
+ dropout
+ 2개 output layser : 이진분류

![다중분류](https://github.com/gzone2000/TEMP_TEST/raw/master/hidden_layer2.PNG)
+ [출처] https://www.educba.com/dnn-neural-network/


```python
# 18개 input layer
# unit 5개 hidden layer
# dropout
# unit 4개 hidden layer
# dropout
# 2개 output layser : 이진분류

X_train.shape

model = Sequential()
model.add(Dense(5,activation='relu',input_shape=(18,)))
model.add(Dropout(0.2))
model.add(Dense(4, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(2, activation='softmax'))

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, epochs=epochs)
```

    Epoch 1/20
    274/274 [==============================] - 1s 2ms/step - loss: 0.5892 - acc: 0.7907 - val_loss: 0.3110 - val_acc: 0.8994
    Epoch 2/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.3417 - acc: 0.9002 - val_loss: 0.2927 - val_acc: 0.8994
    Epoch 3/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.3124 - acc: 0.9004 - val_loss: 0.2808 - val_acc: 0.8994
    Epoch 4/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.3041 - acc: 0.9003 - val_loss: 0.2706 - val_acc: 0.8994
    Epoch 5/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.2967 - acc: 0.8928 - val_loss: 0.2603 - val_acc: 0.8994
    Epoch 6/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.2875 - acc: 0.8954 - val_loss: 0.2499 - val_acc: 0.8994
    Epoch 7/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.2640 - acc: 0.8990 - val_loss: 0.2382 - val_acc: 0.8994
    Epoch 8/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.2480 - acc: 0.9010 - val_loss: 0.2259 - val_acc: 0.8994
    Epoch 9/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.2435 - acc: 0.9013 - val_loss: 0.2131 - val_acc: 0.8994
    Epoch 10/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.2299 - acc: 0.8996 - val_loss: 0.1791 - val_acc: 0.9092
    Epoch 11/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1956 - acc: 0.9187 - val_loss: 0.1502 - val_acc: 0.9552
    Epoch 12/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1746 - acc: 0.9411 - val_loss: 0.1376 - val_acc: 0.9582
    Epoch 13/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1811 - acc: 0.9401 - val_loss: 0.1304 - val_acc: 0.9599
    Epoch 14/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1586 - acc: 0.9440 - val_loss: 0.1219 - val_acc: 0.9595
    Epoch 15/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1538 - acc: 0.9491 - val_loss: 0.1177 - val_acc: 0.9599
    Epoch 16/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1379 - acc: 0.9548 - val_loss: 0.1182 - val_acc: 0.9603
    Epoch 17/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1475 - acc: 0.9489 - val_loss: 0.1158 - val_acc: 0.9595
    Epoch 18/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1327 - acc: 0.9539 - val_loss: 0.1146 - val_acc: 0.9595
    Epoch 19/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1268 - acc: 0.9606 - val_loss: 0.1135 - val_acc: 0.9599
    Epoch 20/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1337 - acc: 0.9555 - val_loss: 0.1120 - val_acc: 0.9599


#### 
### 모델 확인


```python
model.summary()
```

    Model: "sequential_2"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    dense_6 (Dense)              (None, 5)                 95        
    _________________________________________________________________
    dropout_4 (Dropout)          (None, 5)                 0         
    _________________________________________________________________
    dense_7 (Dense)              (None, 4)                 24        
    _________________________________________________________________
    dropout_5 (Dropout)          (None, 4)                 0         
    _________________________________________________________________
    dense_8 (Dense)              (None, 2)                 10        
    =================================================================
    Total params: 129
    Trainable params: 129
    Non-trainable params: 0
    _________________________________________________________________


#### 
### 모델 컴파일 – 다중 분류 모델


```python
model.compile(optimizer='adam', 
              loss='sparse_categorical_crossentropy', 
              metrics=['accuracy']) 
```

#### 
### 모델 학습


```python
history = model.fit(X_train, y_train, 
                    validation_data=(X_test, y_test), 
                    batch_size=batch_size, epochs=epochs)
```

    Epoch 1/20
    274/274 [==============================] - 1s 2ms/step - loss: 0.1329 - accuracy: 0.9546 - val_loss: 0.1117 - val_accuracy: 0.9620
    Epoch 2/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1369 - accuracy: 0.9550 - val_loss: 0.1114 - val_accuracy: 0.9599
    Epoch 3/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1428 - accuracy: 0.9536 - val_loss: 0.1116 - val_accuracy: 0.9591
    Epoch 4/20
    274/274 [==============================] - 1s 2ms/step - loss: 0.1336 - accuracy: 0.9564 - val_loss: 0.1085 - val_accuracy: 0.9620
    Epoch 5/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1316 - accuracy: 0.9563 - val_loss: 0.1083 - val_accuracy: 0.9608
    Epoch 6/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1305 - accuracy: 0.9555 - val_loss: 0.1072 - val_accuracy: 0.9650
    Epoch 7/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1288 - accuracy: 0.9567 - val_loss: 0.1089 - val_accuracy: 0.9667
    Epoch 8/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1291 - accuracy: 0.9559 - val_loss: 0.1067 - val_accuracy: 0.9642
    Epoch 9/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1445 - accuracy: 0.9543 - val_loss: 0.1065 - val_accuracy: 0.9620
    Epoch 10/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1218 - accuracy: 0.9583 - val_loss: 0.1067 - val_accuracy: 0.9608
    Epoch 11/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1101 - accuracy: 0.9651 - val_loss: 0.1055 - val_accuracy: 0.9646
    Epoch 12/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1129 - accuracy: 0.9660 - val_loss: 0.1044 - val_accuracy: 0.9676
    Epoch 13/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1238 - accuracy: 0.9612 - val_loss: 0.1049 - val_accuracy: 0.9633
    Epoch 14/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1238 - accuracy: 0.9599 - val_loss: 0.1045 - val_accuracy: 0.9672
    Epoch 15/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1359 - accuracy: 0.9595 - val_loss: 0.1041 - val_accuracy: 0.9663
    Epoch 16/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1244 - accuracy: 0.9634 - val_loss: 0.1036 - val_accuracy: 0.9659
    Epoch 17/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1177 - accuracy: 0.9613 - val_loss: 0.1040 - val_accuracy: 0.9663
    Epoch 18/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1190 - accuracy: 0.9602 - val_loss: 0.1031 - val_accuracy: 0.9676
    Epoch 19/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9589 - val_loss: 0.1047 - val_accuracy: 0.9646
    Epoch 20/20
    274/274 [==============================] - 0s 1ms/step - loss: 0.1228 - accuracy: 0.9580 - val_loss: 0.1048 - val_accuracy: 0.9689


#### 
### Callback : 조기종료, 모델 저장


```python
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
```


```python
early_stop = EarlyStopping(monitor='val_loss', 
                           mode='min', 
                           verbose=1, 
                           patience=5)
```


```python
check_point = ModelCheckpoint('best_model.h5',
                              verbose=1,
                              monitor='val_loss',
                              mode='min',
                              save_best_only=True)
```

#### 
### 모델 학습


```python
history = model.fit(x=X_train, y=y_train, 
          epochs=50 , batch_size=20,
          validation_data=(X_test, y_test), 
          verbose=1,
          callbacks=[early_stop, check_point])
```

    Epoch 1/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1262 - accuracy: 0.9579 - val_loss: 0.1043 - val_accuracy: 0.9684
    
    Epoch 00001: val_loss improved from inf to 0.10433, saving model to best_model.h5
    Epoch 2/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9592 - val_loss: 0.1025 - val_accuracy: 0.9680
    
    Epoch 00002: val_loss improved from 0.10433 to 0.10247, saving model to best_model.h5
    Epoch 3/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.9612 - val_loss: 0.1017 - val_accuracy: 0.9672
    
    Epoch 00003: val_loss improved from 0.10247 to 0.10167, saving model to best_model.h5
    Epoch 4/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1212 - accuracy: 0.9612 - val_loss: 0.1020 - val_accuracy: 0.9676
    
    Epoch 00004: val_loss did not improve from 0.10167
    Epoch 5/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1197 - accuracy: 0.9629 - val_loss: 0.1015 - val_accuracy: 0.9689
    
    Epoch 00005: val_loss improved from 0.10167 to 0.10153, saving model to best_model.h5
    Epoch 6/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 0.9601 - val_loss: 0.1025 - val_accuracy: 0.9689
    
    Epoch 00006: val_loss did not improve from 0.10153
    Epoch 7/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.9640 - val_loss: 0.1017 - val_accuracy: 0.9689
    
    Epoch 00007: val_loss did not improve from 0.10153
    Epoch 8/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.9618 - val_loss: 0.1017 - val_accuracy: 0.9680
    
    Epoch 00008: val_loss did not improve from 0.10153
    Epoch 9/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1250 - accuracy: 0.9587 - val_loss: 0.1018 - val_accuracy: 0.9701
    
    Epoch 00009: val_loss did not improve from 0.10153
    Epoch 10/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 0.9622 - val_loss: 0.0995 - val_accuracy: 0.9693
    
    Epoch 00010: val_loss improved from 0.10153 to 0.09946, saving model to best_model.h5
    Epoch 11/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9598 - val_loss: 0.1013 - val_accuracy: 0.9684
    
    Epoch 00011: val_loss did not improve from 0.09946
    Epoch 12/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.9600 - val_loss: 0.1011 - val_accuracy: 0.9706
    
    Epoch 00012: val_loss did not improve from 0.09946
    Epoch 13/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1189 - accuracy: 0.9601 - val_loss: 0.1000 - val_accuracy: 0.9697
    
    Epoch 00013: val_loss did not improve from 0.09946
    Epoch 14/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1153 - accuracy: 0.9601 - val_loss: 0.0992 - val_accuracy: 0.9701
    
    Epoch 00014: val_loss improved from 0.09946 to 0.09922, saving model to best_model.h5
    Epoch 15/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1262 - accuracy: 0.9576 - val_loss: 0.1023 - val_accuracy: 0.9672
    
    Epoch 00015: val_loss did not improve from 0.09922
    Epoch 16/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1174 - accuracy: 0.9605 - val_loss: 0.1010 - val_accuracy: 0.9684
    
    Epoch 00016: val_loss did not improve from 0.09922
    Epoch 17/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.9611 - val_loss: 0.0998 - val_accuracy: 0.9680
    
    Epoch 00017: val_loss did not improve from 0.09922
    Epoch 18/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1194 - accuracy: 0.9587 - val_loss: 0.1000 - val_accuracy: 0.9714
    
    Epoch 00018: val_loss did not improve from 0.09922
    Epoch 19/50
    274/274 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.9596 - val_loss: 0.1000 - val_accuracy: 0.9697
    
    Epoch 00019: val_loss did not improve from 0.09922
    Epoch 00019: early stopping


#### 
### 모델 성능 평가


```python
losses = pd.DataFrame(model.history.history)
```


```python
losses.head()
# loss, accuray --> 훈련값의 손실 및 정확도
# val_loss, val_accuracy --> 검증(test)값의 손실 및 정확도
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss</th>
      <th>accuracy</th>
      <th>val_loss</th>
      <th>val_accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.126209</td>
      <td>0.957945</td>
      <td>0.104330</td>
      <td>0.968444</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.122419</td>
      <td>0.959225</td>
      <td>0.102470</td>
      <td>0.968017</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.116726</td>
      <td>0.961236</td>
      <td>0.101672</td>
      <td>0.967164</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.121170</td>
      <td>0.961236</td>
      <td>0.101956</td>
      <td>0.967591</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.119656</td>
      <td>0.962882</td>
      <td>0.101526</td>
      <td>0.968870</td>
    </tr>
  </tbody>
</table>
</div>



+ 성능 시각화


```python
losses[['accuracy','val_accuracy']].plot()
```




    <AxesSubplot:>



![output_80_1](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/a9a44f8d-505d-40f8-a4a8-4c528a5445c9)




```python
losses[['loss','val_loss']].plot()
```




    <AxesSubplot:>



![output_81_1](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/e7a46763-657c-4ac7-98c8-af110180d7f9)

    



```python
losses[['loss','val_loss', 'accuracy','val_accuracy']].plot() 
```




    <AxesSubplot:>




![output_82_1](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/52d942c2-e8f7-41ad-a203-bb141b1a0b96)

    



```python
plt.plot(history.history['accuracy'])  # 배열의 인덱스 값이 epochs값임, epochs값이 X값으로 들어감
plt.plot(history.history['val_accuracy'])
plt.title('Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Acc')
plt.legend(['acc', 'val_acc'])
plt.show()
```


![output_83_0](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/afbd631f-045a-4594-88df-b602cead17bf)
    


## 
## 배운 내용 정리
1. 딥러닝 심층신경망(DNN) 모델 프로세스
 + 데이터 가져오기
 + 데이터 전처리
 + Train, Test 데이터셋 분할
 + 데이터 정규화
 + DNN 딥러닝 모델


```python

```
