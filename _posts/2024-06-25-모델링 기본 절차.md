AICE ASSOCIATEì˜ ì¶œì œë²”ìœ„ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ í˜ì‹  ì—­ëŸ‰ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„° ë¶„ì„ ë° ëª¨ë¸ë§ê¹Œì§€ ì…ë‹ˆë‹¤.

íƒìƒ‰ì  ë°ì´í„° ë¶„ì„
ë°ì´í„° ì „ì²˜ë¦¬
ë¨¸ì‹ ëŸ¬ë‹/ë”¥ëŸ¬ë‹ ëª¨ë¸ë§
ëª¨ë¸ ì„±ëŠ¥í‰ê°€


ğŸ’¾AICE ASSOCIATE ì´ì •ë¦¬ğŸ’¾
*ê³µë¶€í•˜ë©´ì„œ í•„ìš”í•˜ë‹¤ê³  ëŠë‚€ ë‚´ìš© ìœ„ì£¼ë¡œ ì •ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤.

ğŸš«ì£¼ì˜ì‚¬í•­ğŸš«
ğŸ”˜ ì£¼ì–´ì§„ ë‹µì•ˆ ì½”ë“œ ì‘ì„±ë€ì— ì½”ë“œë¥¼ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
"# ì—¬ê¸°ì— ë‹µì•ˆì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”"ë¼ê³  ì í˜€ìˆìœ¼ë‹ˆê¹Œ ì£¼ì˜í•´ì„œ ë´ì•¼í•©ë‹ˆë‹¤.
ğŸ”˜ ì£¼ì–´ì§„ ì½”ë“œ ì‘ì„±ë€ì„ ì œê±°í•  ê²½ìš° ì±„ì í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
ğŸ”˜ ë§Œì¼ì˜ ì‚¬íƒœì— ëŒ€ë¹„í•´ ì„ì‹œì €ì¥ ë²„íŠ¼ì„ ìˆ˜ì‹œë¡œ ëˆŒëŸ¬ ì €ì¥í•´í•˜ëŠ” ìŠµê´€ì„ ê°€ì§‘ì‹œë‹¤.
ğŸ”˜ ì œì¶œ ì‹œì—ëŠ” ìµœì¢… ì œì¶œ ë° ì¢…ë£Œ ë²„íŠ¼ì„ ëˆŒëŸ¬ì•¼ ì œì¶œë©ë‹ˆë‹¤.
ğŸ”˜ ì£¼ì–´ì§„ ë¬¸ì œì— ë§ê²Œ ì½”ë“œë¥¼ ì‘ì„±í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ë¬¸ì œì— ì œì‹œëœ ê°€ì´ë“œë¥¼ ì˜ ì½ê³  ì½”ë“œë¥¼ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
ğŸ”˜ ë³€ìˆ˜ëª…, ë°ì´í„°í”„ë ˆì„ëª…ì„ ì˜ í™•ì¸í•˜ê³  ë§ê²Œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
ğŸ”˜ ë¬¸ì œë¥¼ ìœ ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

# 1) íƒìƒ‰ì  ë°ì´í„° ë¶„ì„

### **ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ

	*scikit-learn
	import sklearn

	*pandas
	import pandas as pd

	*numpy
	import numpy as np

	*matplotlib
	import matplotlib.pyplot as plt

	*seaborn
	!pip install seaborn
	import seaborn as sns

### **ë°ì´í„° ë¡œë“œ ë° í™•ì¸

	*ë°ì´í„° ë¡œë“œ
	ì£¼ì–´ì§„ ë³€ìˆ˜ëª…ì— ë§ê²Œ pandas íŒ¨í‚¤ì§€ë¥¼ í™œìš©í•´ ë°ì´í„° í”„ë ˆì„ì„ ë¡œë“œí•©ë‹ˆë‹¤.
	pd.read_csv('{path}')
	df = pd.read_csv('íŒŒì¼ ê²½ë¡œ') # í•œê¸€ì¼ ê²½ìš° [, encoding='cp949', ,delimiter=',']

	*ë°ì´í„° í™•ì¸
	í–‰, ì—´, ë°ì´í„° í”„ë ˆì„ ì •ë³´, í†µê³„ ì •ë³´ ë“±ì„ í™•ì¸í•©ë‹ˆë‹¤.

	.head() : ì•í–‰ nê°œ í™•ì¸ (default: 5)
	df.head(n)

	.tail() : ë’·í–‰ nê°œ í™•ì¸ (default: 5)
	df.tail(n):

	.columns : ë°ì´í„° í”„ë ˆì„ ì—´ ì´ë¦„ í™•ì¸
	df.columns

	.shape : ë°ì´í„° í”„ë ˆì„ í–‰, ì—´ ê°œìˆ˜ í™•ì¸
	df.shape

	.info() : ì—´ ì •ë³´, Null ê°œìˆ˜, ì—´ íƒ€ì…, ì‚¬ì´ì¦ˆ ë“±ì˜ ë°ì´í„° í”„ë ˆì„ ì •ë³´ í™•ì¸
	df.info()

	.describe() : ê³„ì‚° ê°€ëŠ¥í•œ ê°’(ìˆ˜ì¹˜í˜• ë³€ìˆ˜)ì— ëŒ€í•œ í†µê³„ ì •ë³´ í™•ì¸
	df.describe()

	.isnull() : Nullì¸ ë°ì´í„° í™•ì¸
	ë‹¨ìˆœíˆ isnull()ë§Œ ì‚¬ìš©í•˜ê¸°ë„ í•˜ì§€ë§Œ sum()ê³¼ í•¨ê»˜ ì‚¬ìš©ë˜ì–´ í–‰ì´ë‚˜ ì—´ë³„ë¡œì˜ Null ê°œìˆ˜ë¥¼ ì„¸ê¸° ìœ„í•´ ì£¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
	df.isnull().sum()

	.value_counts() : ë²”ì£¼í˜• ë³€ìˆ˜ì— ëŒ€í•´ ê° ë²”ì£¼ë³„ ë¹ˆë„ìˆ˜ í™•ì¸
	normalize = Trueë¥¼ ì£¼ë©´ ì •ê·œí™”ëœ ê°’ìœ¼ë¡œ ë²”ì£¼ë³„ ë¹„ìœ¨ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
	df[ì—´ ì´ë¦„].value_counts(normalize = False/True)

	.select_dtypes() : ì›í•˜ëŠ” ë°ì´í„° íƒ€ì…ì— í•´ë‹¹í•˜ëŠ” ì—´ë§Œ ë°ì´í„° í”„ë ˆì„ í˜•íƒœë¡œ í™•ì¸
	type: int, float, str ë“±ì˜ ì›í•˜ëŠ” ë°ì´í„° íƒ€ì…ì˜ ì—´ë§Œ ì¶”ì¶œ

	.columnsë¥¼ í™œìš©í•´ ì—´ ì´ë¦„ë§Œ ì¶”ì¶œí•  ìˆ˜ ìˆë‹¤.
	df.select_dtypes(type)


# 2) ë°ì´í„° ì „ì²˜ë¦¬

### ** ë°ì´í„° í”„ë ˆì„ ì œê±° ë° ë³€í™˜

	* ë°ì´í„° í”„ë ˆì„ ì œê±°
	.drop() : ì„ íƒ ì—´ ì œê±°
	ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì—¬ëŸ¬ ê°œì˜ ì—´ì„ í•œ ë²ˆì— ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
	axis: í–‰(=0)ê³¼ ì—´(=1)ì„ ì£¼ì–´ ì›í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
	inplace = True: ë³€ìˆ˜ë¥¼ í• ë‹¹í•˜ì§€ ì•Šê³  ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
	df.drop(axis=0/1, inplace = False/True)

	* ë°ì´í„° í”„ë ˆì„ ë³€í™˜
	.replace() : ê°’ ë³€ê²½
	ì›í•˜ëŠ” ê°’ìœ¼ë¡œ ë³€ê²½í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ë©° ê²°ì¸¡ê°’ ëŒ€ì²´ë‚˜ ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ë¼ë²¨ë§í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

	to_replace: ì£¼ë¡œ {'ë°”ê¾¸ê³ ì í•˜ëŠ” ê°’': 'ë°”ë€ŒëŠ” ê°’'}ì˜ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì£¼ì–´ì§‘ë‹ˆë‹¤.
	inplace = True: ë³€ìˆ˜ë¥¼ í• ë‹¹í•˜ì§€ ì•Šê³  ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
	df.replace({'ë°”ê¾¸ê³ ì í•˜ëŠ” ê°’': 'ë°”ë€ŒëŠ” ê°’'}, inplace = False/True)

	.fillna() : ê²°ì¸¡ê°’ ëŒ€ì²´
	inplace = True: ë³€ìˆ˜ë¥¼ í• ë‹¹í•˜ì§€ ì•Šê³  ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
	df['ì—´ì´ë¦„'].fillna('ë°”ë€ŒëŠ” ê°’', inplace = False/True)
	
	.astype() : ë°ì´í„° í”„ë ˆì„ ì—´ íƒ€ì… ë³€í™˜
	astypeì€ ë°”ê¾¸ê³ ì í•˜ëŠ” ì—´ì— í• ë‹¹í•´ì£¼ì–´ì•¼ ë³€í™˜ ê°’ì´ ì ìš©ëœë‹¤.
	type: int, float, str ë“±ìœ¼ë¡œ ì›í•˜ëŠ” íƒ€ì…ìœ¼ë¡œ ë³€í™˜
	df['ì—´ì´ë¦„'] = df['ì—´ì´ë¦„'].astype(type)
	
	.groupby() : ê·¸ë£¹ ì§‘ê³„
	ê·¸ë£¹ë³„ ì§‘ê³„ í•¨ìˆ˜
	ì§‘ê³„ í•¨ìˆ˜ì— ë§ê²Œ ê·¸ë£¹ë³„ë¡œ ì›í•˜ëŠ” ì—´ì„ ì§‘ê³„í•  ìˆ˜ ìˆë‹¤.
	by: ê·¸ë£¹ì˜ ê¸°ì¤€ì´ ë˜ëŠ” ì—´ë¡œ ì—¬ëŸ¬ ì—´ì„ ê¸°ì¤€ìœ¼ë¡œë„ í•  ìˆ˜ ìˆë‹¤.
	as_index = bool: ê·¸ë£¹ ê¸°ì¤€ ì—´ì„ ì¸ë±ìŠ¤í™” í• ì§€ ì—¬ë¶€ë¥¼ ì„ íƒí•  ìˆ˜ ìˆë‹¤.
	ì§‘ê³„í•¨ìˆ˜: sum(í•©), mean(í‰ê· ), count(ê°œìˆ˜), median(ì¤‘ê°„ê°’) ë“±ì˜ ì§‘ê³„í•¨ìˆ˜
	df.groupby(by=['ê·¸ë£¹ê¸°ì¤€ ì—´'])['ì§‘ê³„ ëŒ€ìƒ ì—´'].ì§‘ê³„í•¨ìˆ˜() ,
  df_group = df_group.reset_index() ì¸ì—‘ìŠ¤ë¥¼ ë‹¤ì‹œ ì¬ì§€ì • ê°€ëŠ¥

### ** ì •ê·œí™”/í‘œì¤€í™” (ìŠ¤ì¼€ì¼ë§)
	ì •ê·œí™”/í‘œì¤€í™”ëŠ” ê° ë³€ìˆ˜ ê°„ì˜ ì¸¡ì •ë‹¨ìœ„ ë° ë²”ìœ„ê°€ ë‹¬ë¼ ê°’ì„ íŠ¹ì • ê°’ ì‚¬ì´ë¡œ ë³€í™”í•´ ë¹„êµê°€ ê°€ëŠ¥í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê³¼ì •ì´ë‹¤. 
	íŠ¹íˆ ê±°ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ëŠ” ëª¨ë¸ì—ì„œëŠ” ê¼­ í•„ìš”í•œ ê³¼ì •ì´ë‹¤.
	Min Max Scaleì€ sklearnì˜ MinMaxScalerë¥¼ ì‚¬ìš©í•˜ì—¬ ì ìš©í•œë‹¤.

* MinMaxScaler
	MinMaxScalerë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë¡œë“œí•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤.
	fit: ê¸°ì¤€ì´ ë˜ëŠ” ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤ì¼€ì¼ëŸ¬ ê°’ì„ ë§ì¶˜ë‹¤.
	transform: ê¸°ì¤€ ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§ì„ ì ìš©í•œë‹¤.

	# import
	from sklearn.preprocessing import MinMaxScaler

	# ê°ì²´ ìƒì„±
	mms = MinMaxScaler()

	# ìŠ¤ì¼€ì¼ë§ ì ìš©
	X_train = mms.fit_transform(X_train)
	X_test = mms.transform(X_test)

* SSëŠ”ë³„ë„ë¡œ 	

### ** ì¸ì½”ë”©
	ì¸ì½”ë”©ì€ ë¬¸ìì—´ì´ë‚˜ ë²”ì£¼í˜•ìœ¼ë¡œ ëœ ë³€ìˆ˜ë¥¼ ìˆ˜ì¹˜í™” í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ ë¬¸ìì—´ì´ ëª¨ë¸ë§ ê³¼ì •ì—ì„œ í•´ì„ë˜ì§€ ì•Šê¸° ë•Œë¬¸ì— í•„ìš”í•œ ê³¼ì •ì´ë‹¤.

*ë¼ë²¨ ì¸ì½”ë”© Label Encoding
	LabelEncoder
	LabelEncoderë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë¡œë“œí•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤.

	fit: ê¸°ì¤€ì´ ë˜ëŠ” ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¸ì½”ë” ê°’ì„ ë§ì¶˜ë‹¤.
	transform: ê¸°ì¤€ ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ì¸ì½”ë”©ì„ ì ìš©í•œë‹¤.
	# import
	from sklearn.preprocessing import LabelEncoder

	# ê°ì²´ ìƒì„±
	le = LabelEncoder()

	# ì¸ì½”ë”© ì ìš©
	df['ë²”ì£¼í˜• ì—´'] = le.fit_transform(df['ë²”ì£¼í˜• ì—´'])


* ì›-í•« ì¸ì½”ë”© One-Hot Encoding
	.get_dummies() : ì›-í•« ì¸ì½”ë”©ì€ pandasë‚´ì˜ ë©”ì†Œë“œë¡œ ì ìš©í•œë‹¤.

	columns: ì›-í•« ì¸ì½”ë”©ì„ ì ìš©í•  ì—´ ë¦¬ìŠ¤íŠ¸
	drop_first=True: ì²«ë²ˆì§¸ ë²”ì£¼ëŠ” ì œì™¸í•˜ê³  ì›-í•« ì¸ì½”ë”© ì ìš©.
	df = pd.get_dummies(df, columns=['ë²”ì£¼í˜• ì—´'], drop_first=True)


# 3) ëª¨ë¸ë§ ë° ì„±ëŠ¥ í‰ê°€

### ** ë°ì´í„° ë¶„í• 
	ì í•©í•œ ëª¨ë¸ì„ ìƒì„±í•˜ê¸° ìœ„í•´ì„œëŠ” ê³¼ì í•©(ê³¼ëŒ€ì í•©/ê³¼ì†Œì í•©)ì„ í”¼í•˜ê¸° ìœ„í•´ ê²€ì¦í•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤. 
	ì´ë¥¼ ìœ„í•´ì„œ ë°ì´í„°ë¥¼ í•™ìŠµì„ ìœ„í•œ í•™ìŠµìš© ë°ì´í„°ì™€ ê²€ì¦ì„ ìœ„í•œ ê²€ì¦ìš© ë°ì´í„°ë¡œ ë¶„í• í•œë‹¤.

	ë°ì´í„° ë¶„í• ì—ëŠ” sklearnì˜ train_test_splitì„ ì‚¬ìš©í•œë‹¤.

	train_test_split()
	train_test_splitë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë¡œë“œí•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤.

	X_train, X_test, y_train, y_test ìˆœìœ¼ë¡œ ë¶„í• í•œ ë°ì´í„° ê°’ì„ ë°˜í™˜í•œë‹¤.
	train_size/test_size: í•™ìŠµìš©/ê²€ì¦ìš© ë°ì´í„°ë¥¼ ë¶„í• í•  ê¸°ì¤€ì´ ëœë‹¤. (ì£¼ë¡œ 8:2ë‚˜ 7:3ìœ¼ë¡œ ë¶„í• í•œë‹¤.)
	stratify: íƒ€ê²Ÿ ë³€ìˆ˜ì˜ ë¶ˆê· í˜•ì´ ì‹¬í•  ë•Œ ë²”ì£¼ë¥¼ ê· í˜•ìˆê²Œ ë¶„í•  í•´ì¤€ë‹¤.
	random_state: ëœë¤ ì‹œë“œë¥¼ ì¤Œìœ¼ë¡œì¨ ë§¤ë²ˆ ê°™ì€ ê²°ê³¼ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆë„ë¡ ì„¤ì •í•œë‹¤.

	# import
	from sklearn.model_selection import train_test_split

	# ë°ì´í„° ë¶„í• 
	X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8, stratify=y, random_state=2023)
	
	# ë‹¤ë¥¸ë°©ë²•
	>>>>>>>>>>>>sampleê³¼ dropì„ ì´ìš©í•œ ë°©ë²• ì •ë¦¬ í•„ìš”

### ** ëª¨ë¸ í•™ìŠµ
	fit() : 
	ìƒì„±ëœ ëª¨ë¸ì„ í•™ìŠµìš© ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•œë‹¤.

	# ëª¨ë¸ í•™ìŠµ
	model.fit(X_train, y_train)
	
### ** ëª¨ë¸ ê²€ì¦
	score() : ìƒì„±ëœ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.
	ë¶„ë¥˜ ëª¨ë¸ì€ ì •í™•ë„ë¡œ, íšŒê·€ ëª¨ë¸ì€ ê²°ì •ê³„ìˆ˜ë¡œ ê²€ì¦í•œë‹¤.
	
	# ëª¨ë¸ ê²€ì¦
	model.score(X_test, y_test)
	
### ** ëª¨ë¸ ì˜ˆì¸¡
	predict() : ìƒì„±ëœ ëª¨ë¸ë¡œ ë°ì´í„°ì˜ ì˜ˆì¸¡ê°’ì„ ê³„ì‚°í•œë‹¤.

	# ëª¨ë¸ ì˜ˆì¸¡
	model.predict(X_test) : 
	
### ** ëª¨ë¸ ê²€ì¦
	ëª¨ë¸ ê²€ì¦ì— ì‚¬ìš©ë˜ëŠ” ì§€í‘œë„ ëª¨ë¸ì— ë”°ë¼ ë‹¬ë¼ì§„ë‹¤.

	íšŒê·€ ëª¨ë¸: MSE, RMSE, MAE, ...
	ë¶„ë¥˜ ëª¨ë¸: Accuracy, Precision, Recall, F1 Score, ...
	ëª¨ë¸ í‰ê°€ ì§€í‘œëŠ” sklearnì˜ í•¨ìˆ˜ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.
	í‰ê°€ ì§€í‘œë¥¼ ê³„ì‚°í•  ë•Œ, (Actual, Predicsion)ìœ¼ë¡œ ì…ë ¥ê°’ì„ ì£¼ëŠ” ê²ƒì´ ì¢‹ë‹¤

	# import 
	from sklearn.metrics import * # ëª¨ë“  í•¨ìˆ˜ ë¡œë“œ

	# ë¶„ë¥˜ ëª¨ë¸ í‰ê°€ ì§€í‘œ
	from sklearn.metrics import accuracy_score 		# ì •í™•ë„
	from sklearn.metrics import precision_score 	# ì •ë°€ë„
	from sklearn.metrics import recall_score 		# ì¬í˜„ìœ¨
	from sklearn.metrics import f1_score 			# f1 score
	from sklearn.metrics import confusion_matrix 	# í˜¼ë™í–‰ë ¬
	from sklearn.metrics import accuracy_score 		# ëª¨ë“  ì§€í‘œ í•œë²ˆì—

	# íšŒê·€ ëª¨ë¸ í‰ê°€ ì§€í‘œ
	from sklearn.metrics import r2_score			# R2 ê²°ì •ê³„ìˆ˜
	from sklearn.metrics import mean_squared_error	# MSE
	from sklearn.metrics import mean_absolute_error	# MAE
	+) Confusion Matrix Heatmapìœ¼ë¡œ ì‹œê°í™” í•˜ê¸°

	y_pred = model.predict(X_test) # ëª¨ë¸ ì˜ˆì¸¡ê°’ ì €ì¥
	cf_matrix = confusion_matrix(y_test, y_pred)
	print(cf_matrix) # í˜¼ë™ í–‰ë ¬

	# íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”
	sns.heatmap(cf_matrix, 	# í˜¼ë™ í–‰ë ¬
				annot=True, # ì£¼ì„
				fmt='d') 	# ì£¼ì„ í¬ë§·
	plt.show()

  # plotlibìœ¼ë¡œ ì‹œê°í™”
  fig, ax = plt.subplots(figsize=(10,3))
  plot_confusion_matrix(ax, confusion, fontsize=30)

	# ì •í™•ë„ 
	print(accuracy_score(actural, pred))

# 3-1) ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§

** 
íƒ€ê²Ÿ ë³€ìˆ˜ yê°€ ìˆ˜ì¹˜í˜•ì´ëƒ ë²”ì£¼í˜•ì´ëƒì— ë”°ë¼ ë¶„ë¥˜/íšŒê·€ ë¬¸ì œê°€ ë‹¬ë¼ì§„ë‹¤. 
í•´ë‹¹ ë¶€ë¶„ì„ ê¼­ í™•ì¸í•´ì„œ ë§ëŠ” ëª¨ë¸ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤.
**

### ** Logistic Regression (ë¡œì§€ìŠ¤í‹± íšŒê·€, ë¶„ë¥˜ëª¨ë¸ì„)
	LogisticRegression
	Logistic Regressionì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë¡œë“œí•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤.

	C: ê·œì œ ê°•ë„
	max_iter: ë°˜ë³µ íšŸìˆ˜
	# import
	from sklearn.linear_model import LogisticRegression

	# ëª¨ë¸ ìƒì„±
	lg = LogisticRegression(C=1.0, max_iter=1000)

	# ëª¨ë¸ í•™ìŠµ
	lg.fit(X_train, y_train)

	# ëª¨ë¸ í‰ê°€
	lg.score(X_test
	, y_test)

### ** Decision Tree (ì˜ì‚¬ê²°ì • ë‚˜ë¬´, ë¶„ë¥˜/íšŒê·€)
	DecisionTreeClassifier / DecisionTreeRegressor
	Decision Treeë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë¡œë“œí•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤.

	max_depth: íŠ¸ë¦¬ ê¹Šì´
	random_state: ëœë¤ ì‹œë“œ
	
	# import
	from sklearn.tree import DecisionTreeClassifier # ë¶„ë¥˜
	from sklearn.tree import DecisionTreeRegressor # íšŒê·€

	# ëª¨ë¸ ìƒì„±
	dt = DecisionTreeClassifier(max_depth=5, random_state=2023)

	# ëª¨ë¸ í•™ìŠµ
	dt.fit(X_train, y_train)

	# ëª¨ë¸ í‰ê°€
	dt.score(X_test, y_test)
	
	
### ** Random Forest (ëœë¤ í¬ë ˆìŠ¤íŠ¸, ë¶„ë¥˜/íšŒê·€)
	RandomForestClassifier / RandomForestRegressor
	Random Forestë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë¡œë“œí•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤.

	n_estimators: ì‚¬ìš©í•˜ëŠ” íŠ¸ë¦¬ ê°œìˆ˜
	random_state: ëœë¤ ì‹œë“œ
	
	# import
	from sklearn.ensemble import RandomForestClassifier # ë¶„ë¥˜
	from sklearn.ensemble import RandomForestRegressor # íšŒê·€

	# ëª¨ë¸ ìƒì„±
	rf = RandomForestClassifier(n_estimators=100, random_state=2023)

	# ëª¨ë¸ í•™ìŠµ
	rf.fit(X_train, y_train)

	# ëª¨ë¸ í‰ê°€
	rf.score(X_test, y_test)
	
	
### ** XGBoost (ë¶„ë¥˜/íšŒê·€)
	XGBClassifier / XGBRegressor
	XGBoostë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ì„¤ì¹˜ ë° ë¡œë“œí•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤.

	n_estimators: ì‚¬ìš©í•˜ëŠ” íŠ¸ë¦¬ ê°œìˆ˜
	
	# install
	!pip install xgboost

	# import
	from xgboost import XGBClassifier # ë¶„ë¥˜
	from xgboost import XGBRegressor # íšŒê·€

	# ëª¨ë¸ ìƒì„±
	xgb = XGBClassifier(n_estimators=100)

	# ëª¨ë¸ í•™ìŠµ
	xgb.fit(X_train, y_train)

	# ëª¨ë¸ í‰ê°€
	xgb.score(X_test, y_test)
	
	
### ** Light GBM (ë¶„ë¥˜/íšŒê·€)
	LGBMClassifier / LGBMRegressor
	Light GBMì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ì„¤ì¹˜ ë° ë¡œë“œí•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤.

	n_estimators: ì‚¬ìš©í•˜ëŠ” íŠ¸ë¦¬ ê°œìˆ˜
	
	# install
	!pip install lightgbm

	# import
	from lightgbm import LGBMClassifier # ë¶„ë¥˜
	from lightgbm import LGBMegressor # íšŒê·€

	# ëª¨ë¸ ìƒì„±
	lgbm = LGBMClassifier(n_estimators=100)

	# ëª¨ë¸ í•™ìŠµ
	lgbm.fit(X_train, y_train)

	# ëª¨ë¸ í‰ê°€
	lgbm.score(X_test, y_test)


# 3-2) ë”¥ëŸ¬ë‹ ëª¨ë¸ë§

** ë”¥ëŸ¬ë‹ì€ tensorflowë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ë§í•œë‹¤.
ê°€ì´ë“œì— ë§ê²Œ ëª¨ë¸ì„ ìƒì„±í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.**

	# import
	import tensorflow as tf
	from tensorflow import keras
	
### ** ëª¨ë¸ ìƒì„±
	ì£¼ë¡œ Sequentialí•œ ë°©ë²•ìœ¼ë¡œ ë ˆì´ì–´ë¥¼ ìŒ“ì•„ì•¼ í•œë‹¤.
	
	ì˜ˆì‹œ)
	# ì²«ë²ˆì§¸ ë ˆì´ì–´ - unit: 128, activation: relu, input_shape: (26,)
	# ë‘ë²ˆì§¸ ë ˆì´ì–´ - unit: 64, activation: relu
	# ì„¸ë²ˆì§¸ ë ˆì´ì–´ - unit: 32, activation: relu
	# ê° íˆë“  ë ˆì´ì–´ ì‚¬ì´ì—ëŠ” 0.3ë¹„ìœ¨ì˜ Dropout ì ìš©
	# ì•„ì›ƒí’‹ ë ˆì´ì–´ - activation: sigmoid

	# import
	from tensorflow.keras.models import Sequential
	from tensorflow.keras.layers import Dense, Dropout

	# ëª¨ë¸ ìƒì„±
	model = Sequential()

	# ëª¨ë¸ êµ¬ì¡°
	model.add(Dense(128, input_shape=(26,), activation='relu'))
	model.add(Dropout(.3))
	model.add(Dense(64, activation='relu'))
	model.add(Dropout(.3))
	model.add(Dense(32, activation='relu'))
	model.add(Dropout(.3))

	# output layerì˜ unit ìˆ˜ëŠ” íƒ€ê²Ÿ ë³€ìˆ˜ì˜ ë²”ì£¼ ê°œìˆ˜ì™€ ë™ì¼í•˜ë‹¤.
	model.add(Dense(1, activation='sigmoid')) 

	# ëª¨ë¸ ìš”ì•½
	model.summary()

### ** ëª¨ë¸ í•™ìŠµ
	* compile : ë”¥ëŸ¬ë‹ì€ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë‹¬ë¦¬ ëª¨ë¸ë§ ê³¼ì •ì—ì„œ ì»´íŒŒì¼ì´ í•„ìš”í•˜ë‹¤.
	optimizer: ëª¨ë¸ë§ì„ ìµœì í™”í•˜ëŠ” ë°©ë²•. ì£¼ë¡œ Adamì„ ì‚¬ìš©í•œë‹¤.
  
  loss: ì†ì‹¤í•¨ìˆ˜ - íƒ€ê²Ÿë³€ìˆ˜ì— ë§ëŠ” ì†ì‹¤í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì•¼ í•œë‹¤.
		íšŒê·€: mse
		ì´ì§„ë¶„ë¥˜: binary_crossentropy
		ë‹¤ì¤‘ë¶„ë¥˜: sparse_categorical_crossentropy, categorical_crossentropy
	
 metrics: ëª¨ë‹ˆí„°ë§ ì§€í‘œë¡œ, ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
		íšŒê·€: mse, rmse
		ë¶„ë¥˜: accuracy
	
	# ëª¨ë¸ ì»´íŒŒì¼
	model.compile(optimizer='adam',
				  loss='binary_crossentropy', 
				  metrics=['accuracy'])
				  
	* EarlyStopping : ëª¨ë¸ì˜ ê³¼ëŒ€ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ í•™ìŠµì´ ê°œì„ ë˜ì§€ ì•ŠëŠ”ë‹¤ë©´ í•™ìŠµì„ ì¢…ë£Œì‹œí‚¨ë‹¤.
	monitor: í•™ìŠµë˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ê¸°ì¤€	
	mode: ëª¨ë¸ ìµœì í™”ì˜ ê¸°ì¤€ - ìµœëŒ€í™”/ìµœì†Œí™”
	patience: ëª¨ë¸ ì„±ëŠ¥ì´ ê°œì„ ë˜ì§€ ì•Šì„ë•Œ ì§€ì¼œë³´ëŠ” íšŸìˆ˜
	
	
	# import
	from tensorflow.keras.callbacks import EarlyStopping

	es = EarlyStopping(monitor='val_loss',
					   mode='min')
					   
					   
	* ModelCheckpoint : ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê³¼ì •ì—ì„œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•˜ë©° ìµœì  ëª¨ë¸ì„ ì„ íƒí•œë‹¤.
	filepath: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
	monitor: ëª¨ë‹ˆí„°ë§í•˜ëŠ” ì§€í‘œ
	mode: ëª¨ë¸ ìµœì í™”ì˜ ê¸°ì¤€ - ìµœëŒ€í™”/ìµœì†Œí™”
	verbose: ì •ë³´ í‘œì‹œ ì •ë„(0, 1, 2)
	save_best_only=True: ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì˜ ëª¨ë¸ë§Œ ì €ì¥
	
	# import
	from tensorflow.keras.callbacks import ModelCheckpoint

	mc = ModelCheckpoint('my_checkpoibnt.ckpt',
						 monitor='val_loss', 
						 mode='min', 
						 save_best_only=True)
						 
	* fit : ëª¨ë¸ì„ í•™ìŠµí•œë‹¤.
	validation_data: ê²€ì¦ìš© ë°ì´í„°.
	ì¢…ì†ë³€ìˆ˜ì™€ íƒ€ê²Ÿë³€ìˆ˜ì˜ ìŒìœ¼ë¡œ ì…ë ¥í•´ì•¼ í•œë‹¤.
	epochs: í•™ìŠµ ë°˜ë³µ íšŸìˆ˜
	batch_size: í•™ìŠµ ì‹œ í•œ ë²ˆì— í•™ìŠµí•˜ëŠ” ë°ì´í„° ì‚¬ì´ì¦ˆ
	callbacks: EarlyStopping, ModelCheckpointì™€ ê°™ì€ í•™ìŠµ ê³¼ì •ì—ì„œ í˜¸ì¶œë˜ëŠ” í•¨ìˆ˜
	history = model.fit(X_train, y_train,
						validation_data = (X_test, y_test),
						epochs=epochs,
						batch_size=batch_size,
						callbacks=[es, mc])
						
						
### ** ëª¨ë¸ ê²€ì¦
	* history : ëª¨ë¸ í•™ìŠµ ê³¼ì •ì—ì„œ ëª¨ë¸ í•™ìŠµ ê²°ê³¼ë¥¼ ì €ì¥í•¨ìœ¼ë¡œì¨ í•™ìŠµ ê³¼ì •ì˜ ë³€í™”ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
	historyëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì €ì¥ë˜ì–´ keyê°’ìœ¼ë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤.

	loss: í•™ìŠµ loss.
	val_loss: ê²€ì¦ loss.
	accuracy: í•™ìŠµ accuracy.
	val_accuracy: ê²€ì¦ accuracy.
	
	# ì‹œê°í™” ì˜ˆì‹œ
  import matplotlib.pyplot as plt
	plt.plot(history.history['loss'], label='Train Loss')
	plt.plot(history.history['val_loss'], label='Validation Loss')
	plt.xlabel('Epochs') 	# Xì¶• ë¼ë²¨
	plt.ylabel('Loss')		# Yì¶• ë¼ë²¨
	plt.legend()			# ë²”ë¡€ í‘œì‹œ - labelê°’
	plt.show()
	
	
	
	
