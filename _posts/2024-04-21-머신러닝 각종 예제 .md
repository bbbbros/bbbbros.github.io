# [실습-퀴즈] Python을 활용한 AI 모델링 - 머신러닝 파트
+ 이번시간에는 Python을 활용한 AI 모델링에서 머신러닝에 대해 실습해 보겠습니다.
+ 머신러닝 모델에는 아래와 같이 모델들이 있습니다.
 + 단일 분류예측 모델 : LogisticRegression, KNN, DecisionTree
 + 앙상블(Ensemble) 모델 : RandomForest, XGBoost, LGBM, Stacking, Weighted Blending
+ 솔직히, 머신러닝이 딥러닝보다 코딩하기 쉽습니다. 4줄 템플릿에 맞쳐 코딩하면 되기 때문입니다.
+ 한가지 당부 드리고 싶은 말은 "백문이불여일타" 입니다. 
+ 이론보다 실습이 더 많은 시간과 노력이 투자 되어야 합니다.

## 학습목차
1. 실습준비
2. 머신러닝 모델 프로세스
 - 데이터 가져오기
 - 데이터 전처리
 - Train, Test 데이터셋 분할
 - 데이터 정규화
 - 단일 분류예측 모델 : LogisticRegression, KNN, DecisionTree
 - 앙상블(Ensemble) 모델 : RandomForest, XGBoost, LGBM, Stacking, Weighted Blending


# 
# 1. 실습준비


```python
# 코드실행시 경고 메시지 무시

import warnings
warnings.filterwarnings(action='ignore') 
```

# 
# 2. 머신러닝 모델 프로세스
① 라이브러리 임포트(import)  
② 데이터 가져오기(Loading the data)  
③ 탐색적 데이터 분석(Exploratory Data Analysis)  
④ 데이터 전처리(Data PreProcessing) : 데이터타입 변환, Null 데이터 처리, 누락데이터 처리, 
더미특성 생성, 특성 추출 (feature engineering) 등  
⑤ Train, Test  데이터셋 분할  
⑥ 데이터 정규화(Normalizing the Data)  
⑦ 모델 개발(Creating the Model)  
⑧ 모델 성능 평가

## ① 라이브러리 임포트

##### 필요 라이브러리 임포트


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

## ② 데이터 로드

#### cust_data.csv 파일 컬럼명
+ 고객등급(class), 성별(sex), 나이(age), 사용서비스수(service), 서비스중지여부 (stop), 미납여부(npay)
+ 3개월 평균 요금(avg_bill), A서비스 3개월 평균요금(A_bill), B서비스 3개월 평균요금(B_bill), 해지여부(termination)


```python
# 앞쪽 전처리에서 저장한 cust_data.csv 파일 읽기
df = pd.read_csv('cust_data.csv')
```


```python
# 필요한 컬럼만 추출
# df1 = df[['cust_class', 'sex_type', 'age', 'efct_svc_count', 'dt_stop_yn', 'npay_yn', 'r3m_avg_bill_amt', 'r3m_A_avg_arpu_amt', 'r3m_B_avg_arpu_amt', 'termination_yn']]
# cust = df1.rename(columns={'cust_class': 'class', 
#                            'sex_type': 'sex', 
#                            'efct_svc_count' : 'service', 
#                            'dt_stop_yn' : 'stop', 
#                            'r3m_avg_bill_amt' : 'avg_bill', 
#                            'r3m_A_avg_arpu_amt' : 'A_bill',
#                            'r3m_B_avg_arpu_amt' : 'B_bill',
#                            'termination_yn' : 'termination'}).copy()
# cust.to_csv('cust_data4머신러닝.csv', index=False)

```

## ③ 데이터 분석


```python
# 12컬럼, 7814 라인
df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 7814 entries, 0 to 7813
    Data columns (total 12 columns):
     #   Column       Non-Null Count  Dtype  
    ---  ------       --------------  -----  
     0   class        7814 non-null   object 
     1   sex          7814 non-null   object 
     2   age          7814 non-null   int64  
     3   service      7814 non-null   int64  
     4   stop         7814 non-null   object 
     5   npay         7814 non-null   object 
     6   avg_bill     7814 non-null   float64
     7   A_bill       7814 non-null   float64
     8   B_bill       7814 non-null   float64
     9   termination  7814 non-null   object 
     10  by_age       7814 non-null   int64  
     11  bill_rating  7814 non-null   object 
    dtypes: float64(3), int64(3), object(6)
    memory usage: 732.7+ KB



```python
df.tail()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>class</th>
      <th>sex</th>
      <th>age</th>
      <th>service</th>
      <th>stop</th>
      <th>npay</th>
      <th>avg_bill</th>
      <th>A_bill</th>
      <th>B_bill</th>
      <th>termination</th>
      <th>by_age</th>
      <th>bill_rating</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>7809</th>
      <td>C</td>
      <td>M</td>
      <td>76</td>
      <td>1</td>
      <td>N</td>
      <td>N</td>
      <td>1860.0000</td>
      <td>1716.000000</td>
      <td>0.0000</td>
      <td>N</td>
      <td>75</td>
      <td>low</td>
    </tr>
    <tr>
      <th>7810</th>
      <td>C</td>
      <td>F</td>
      <td>15</td>
      <td>1</td>
      <td>N</td>
      <td>Y</td>
      <td>1296.0999</td>
      <td>194.414985</td>
      <td>643.1001</td>
      <td>N</td>
      <td>15</td>
      <td>low</td>
    </tr>
    <tr>
      <th>7811</th>
      <td>G</td>
      <td>M</td>
      <td>12</td>
      <td>1</td>
      <td>N</td>
      <td>N</td>
      <td>13799.6666</td>
      <td>2069.949990</td>
      <td>10605.9266</td>
      <td>N</td>
      <td>10</td>
      <td>midhigh</td>
    </tr>
    <tr>
      <th>7812</th>
      <td>C</td>
      <td>F</td>
      <td>40</td>
      <td>0</td>
      <td>N</td>
      <td>N</td>
      <td>3140.0000</td>
      <td>942.000000</td>
      <td>1884.0000</td>
      <td>Y</td>
      <td>40</td>
      <td>low</td>
    </tr>
    <tr>
      <th>7813</th>
      <td>C</td>
      <td>F</td>
      <td>59</td>
      <td>1</td>
      <td>N</td>
      <td>N</td>
      <td>2436.9000</td>
      <td>365.535000</td>
      <td>1839.9000</td>
      <td>N</td>
      <td>55</td>
      <td>low</td>
    </tr>
  </tbody>
</table>
</div>




```python
# termination 레이블 불균형 
# df['termination'].value_counts()
df['termination'].value_counts().plot(kind='bar')
```




    <AxesSubplot:>



![output_15_1](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/7751cc50-6e59-4450-86d9-476a6a9527d5)
    


## ④ 데이터 전처리

+ Object 컬럼에 대해 Pandas get_dummies 함수 활용하여 One-Hot-Encoding


```python
df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 7814 entries, 0 to 7813
    Data columns (total 12 columns):
     #   Column       Non-Null Count  Dtype  
    ---  ------       --------------  -----  
     0   class        7814 non-null   object 
     1   sex          7814 non-null   object 
     2   age          7814 non-null   int64  
     3   service      7814 non-null   int64  
     4   stop         7814 non-null   object 
     5   npay         7814 non-null   object 
     6   avg_bill     7814 non-null   float64
     7   A_bill       7814 non-null   float64
     8   B_bill       7814 non-null   float64
     9   termination  7814 non-null   object 
     10  by_age       7814 non-null   int64  
     11  bill_rating  7814 non-null   object 
    dtypes: float64(3), int64(3), object(6)
    memory usage: 732.7+ KB



```python
# Object 컬럼 리스트 정의
cal_cols = ['class', 'sex', 'stop', 'npay', 'termination', 'bill_rating']
```


```python
# pandas get_dummies 함수 사용하여 Object 컬럼에 대해 One-Hot-Encoding 수행
df1 = pd.get_dummies(data=df, columns=cal_cols, drop_first=True)
```


```python
# 19컬럼, 7814 라인
df1.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 7814 entries, 0 to 7813
    Data columns (total 19 columns):
     #   Column               Non-Null Count  Dtype  
    ---  ------               --------------  -----  
     0   age                  7814 non-null   int64  
     1   service              7814 non-null   int64  
     2   avg_bill             7814 non-null   float64
     3   A_bill               7814 non-null   float64
     4   B_bill               7814 non-null   float64
     5   by_age               7814 non-null   int64  
     6   class_D              7814 non-null   uint8  
     7   class_E              7814 non-null   uint8  
     8   class_F              7814 non-null   uint8  
     9   class_G              7814 non-null   uint8  
     10  class_H              7814 non-null   uint8  
     11  sex_M                7814 non-null   uint8  
     12  stop_Y               7814 non-null   uint8  
     13  npay_Y               7814 non-null   uint8  
     14  termination_Y        7814 non-null   uint8  
     15  bill_rating_low      7814 non-null   uint8  
     16  bill_rating_lowmid   7814 non-null   uint8  
     17  bill_rating_mid      7814 non-null   uint8  
     18  bill_rating_midhigh  7814 non-null   uint8  
    dtypes: float64(3), int64(3), uint8(13)
    memory usage: 465.6 KB



```python
df1.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>service</th>
      <th>avg_bill</th>
      <th>A_bill</th>
      <th>B_bill</th>
      <th>by_age</th>
      <th>class_D</th>
      <th>class_E</th>
      <th>class_F</th>
      <th>class_G</th>
      <th>class_H</th>
      <th>sex_M</th>
      <th>stop_Y</th>
      <th>npay_Y</th>
      <th>termination_Y</th>
      <th>bill_rating_low</th>
      <th>bill_rating_lowmid</th>
      <th>bill_rating_mid</th>
      <th>bill_rating_midhigh</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>28</td>
      <td>0</td>
      <td>2640.0000</td>
      <td>792.00000</td>
      <td>1584.0000</td>
      <td>25</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>24</td>
      <td>1</td>
      <td>16840.0000</td>
      <td>2526.00000</td>
      <td>6983.0000</td>
      <td>20</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>32</td>
      <td>1</td>
      <td>15544.7334</td>
      <td>2331.71001</td>
      <td>6750.4666</td>
      <td>30</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>18</td>
      <td>1</td>
      <td>4700.0000</td>
      <td>0.00000</td>
      <td>4502.0000</td>
      <td>15</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>78</td>
      <td>1</td>
      <td>1361.7999</td>
      <td>1173.99990</td>
      <td>0.0000</td>
      <td>75</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



## ⑤ Train, Test  데이터셋 분할

#### 입력(X)과 레이블(y) 나누기

##### <font color=blue> **[문제] df1 DataFrame에서 'termination_Y' 컬럼을 제외한 나머지 정보를 X에 저장하세요.** </font>


```python
# DataFrame drop 함수 활용
# 'termination_Y' 컬럼 삭제
# DataFrame에서 values만 X에 저장

X = df1.drop(columns=['termination_Y'], axis=1).values
X

```




    array([[2.80000000e+01, 0.00000000e+00, 2.64000000e+03, ...,
            0.00000000e+00, 0.00000000e+00, 0.00000000e+00],
           [2.40000000e+01, 1.00000000e+00, 1.68400000e+04, ...,
            0.00000000e+00, 0.00000000e+00, 0.00000000e+00],
           [3.20000000e+01, 1.00000000e+00, 1.55447334e+04, ...,
            0.00000000e+00, 0.00000000e+00, 0.00000000e+00],
           ...,
           [1.20000000e+01, 1.00000000e+00, 1.37996666e+04, ...,
            0.00000000e+00, 0.00000000e+00, 1.00000000e+00],
           [4.00000000e+01, 0.00000000e+00, 3.14000000e+03, ...,
            0.00000000e+00, 0.00000000e+00, 0.00000000e+00],
           [5.90000000e+01, 1.00000000e+00, 2.43690000e+03, ...,
            0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])



##### <font color=blue> **[문제] df1 DataFrame에서 'termination_Y' 컬럼의 값을 y로 저장하세요.** </font>


```python
# DataFrame 'termination_Y' 컬럼 사용
# DataFrame에서 values만 y에 저장
y = df1['termination_Y'].values
y

```




    array([1, 0, 0, ..., 0, 1, 0], dtype=uint8)




```python
X.shape, y.shape
```




    ((7814, 18), (7814,))



#### Train , Test dataset 나누기


```python
from sklearn.model_selection import train_test_split 
```

##### <font color=blue> **[문제] Train dataset, Test dataset 나누세요.** </font>


```python
# Train dataset, Test dataset 나누기 : train_test_split 함수 사용
# 입력 : X, y 
# Train : Test 비율 = 7: 3  --> test_size=0.3
# y Class 비율에 맞게 나주어 주세요 : stratify=y
# 여러번 수행해도 같은 결과 나오게 고정하기 : random_state=42 
# 결과 : X_train, X_test, y_train, y_test
X_train, X_test, y_train, y_test = train_test_split(X
                                                    , y
                                                    , test_size=0.3
                                                    ,stratify=y
                                                    , random_state=42
                                                    # , shuffle = False
                                                   )




```


```python
X_train.shape
```




    (5469, 18)




```python
y_train.shape
```




    (5469,)



## ⑥ 데이터 정규화/스케일링(Normalizing/Scaling)


```python
# 숫자 분포 이루어진 컬럼 확인
df1.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>service</th>
      <th>avg_bill</th>
      <th>A_bill</th>
      <th>B_bill</th>
      <th>by_age</th>
      <th>class_D</th>
      <th>class_E</th>
      <th>class_F</th>
      <th>class_G</th>
      <th>class_H</th>
      <th>sex_M</th>
      <th>stop_Y</th>
      <th>npay_Y</th>
      <th>termination_Y</th>
      <th>bill_rating_low</th>
      <th>bill_rating_lowmid</th>
      <th>bill_rating_mid</th>
      <th>bill_rating_midhigh</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>28</td>
      <td>0</td>
      <td>2640.0000</td>
      <td>792.00000</td>
      <td>1584.0000</td>
      <td>25</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>24</td>
      <td>1</td>
      <td>16840.0000</td>
      <td>2526.00000</td>
      <td>6983.0000</td>
      <td>20</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>32</td>
      <td>1</td>
      <td>15544.7334</td>
      <td>2331.71001</td>
      <td>6750.4666</td>
      <td>30</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>18</td>
      <td>1</td>
      <td>4700.0000</td>
      <td>0.00000</td>
      <td>4502.0000</td>
      <td>15</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>78</td>
      <td>1</td>
      <td>1361.7999</td>
      <td>1173.99990</td>
      <td>0.0000</td>
      <td>75</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




```python
# print(type(X_train)) # 데이터를 무작위로 섞는다.
X_train[0][0]

```




    42.0




```python
from sklearn.preprocessing import MinMaxScaler
```

##### <font color=blue> **[문제] MinMaxScaler 함수를 'scaler'로 정의 하세요.** </font>
## MinMaxScaler는 데이터를 0~1사이 데이터로 바꿔준다.
## fit_transform, transform 사용에 주의!!!!!


```python
# 사이키런의 MinMaxScaler() 함수 활용
# 정의할 결과를 'scaler'로 매핑


scaler = MinMaxScaler()



```


```python
X_train = scaler.fit_transform(X_train)   ### fit_transform은 데이터로 학습을 한다. 그래서 train 데이터로 입력
X_test = scaler.transform(X_test)   ### transform은 학습되어진 모델을 통해 스케일링
```


```python
# X_train[:2], y_train[:2]
X_train[:4], y_train[:4]
# X_train.max()
```




    (array([[0.38      , 0.33333333, 0.4295439 , 0.06384702, 0.41944434,
             0.4       , 0.        , 0.        , 0.        , 0.        ,
             1.        , 1.        , 0.        , 1.        , 0.        ,
             0.        , 0.        , 1.        ],
            [0.58      , 0.11111111, 0.20111297, 0.38498933, 0.        ,
             0.6       , 1.        , 0.        , 0.        , 0.        ,
             0.        , 1.        , 0.        , 0.        , 0.        ,
             1.        , 0.        , 0.        ],
            [0.51      , 0.22222222, 0.5025271 , 0.32347352, 0.67293822,
             0.55      , 0.        , 0.        , 0.        , 1.        ,
             0.        , 1.        , 0.        , 0.        , 0.        ,
             0.        , 0.        , 1.        ],
            [0.62      , 0.11111111, 0.13149699, 0.        , 0.21702104,
             0.65      , 1.        , 0.        , 0.        , 0.        ,
             0.        , 1.        , 0.        , 0.        , 0.        ,
             1.        , 0.        , 0.        ]]),
     array([0, 0, 0, 0], dtype=uint8))



+ 모델 입력갯수, 출력갯수 확인

## ⑦ 모델 개발

#### 모델별 바차트 그려주고 성능 확인을 위한 함수


```python
# 모델별로 Accuracy 점수 저장
# 모델 Accuracy 점수 순서대로 바차트를 그려 모델별로 성능 확인 가능

from sklearn.metrics import accuracy_score

my_predictions = {}

colors = ['r', 'c', 'm', 'y', 'k', 'khaki', 'teal', 'orchid', 'sandybrown',
          'greenyellow', 'dodgerblue', 'deepskyblue', 'rosybrown', 'firebrick',
          'deeppink', 'crimson', 'salmon', 'darkred', 'olivedrab', 'olive', 
          'forestgreen', 'royalblue', 'indigo', 'navy', 'mediumpurple', 'chocolate',
          'gold', 'darkorange', 'seagreen', 'turquoise', 'steelblue', 'slategray', 
          'peru', 'midnightblue', 'slateblue', 'dimgray', 'cadetblue', 'tomato'
         ]

# 모델명, 예측값, 실제값을 주면 위의 plot_predictions 함수 호출하여 Scatter 그래프 그리며
# 모델별 MSE값을 Bar chart로 그려줌
def accuracy_eval(name_, pred, actual):
    global predictions
    global colors

    plt.figure(figsize=(12, 9))

    acc = accuracy_score(actual, pred)
    my_predictions[name_] = acc * 100
    
    y_value = sorted(my_predictions.items(), key=lambda x: x[1], reverse=True)
    
    df = pd.DataFrame(y_value, columns=['model', 'accuracy'])
    print(df)
   
    length = len(df)
    
    plt.figure(figsize=(10, length))
    ax = plt.subplot()
    ax.set_yticks(np.arange(len(df)))
    ax.set_yticklabels(df['model'], fontsize=15)
    bars = ax.barh(np.arange(len(df)), df['accuracy'])
    
    for i, v in enumerate(df['accuracy']):
        idx = np.random.choice(len(colors))
        bars[i].set_color(colors[idx])
        ax.text(v + 2, i, str(round(v, 3)), color='k', fontsize=15, fontweight='bold')
        
    plt.title('accuracy', fontsize=18)
    plt.xlim(0, 100)
    
    plt.show()
```


```python
# 모델별로 Accuracy 점수 저장
# 모델 Accuracy 점수 순서대로 바차트를 그려 모델별로 성능 확인 가능

from sklearn.metrics import accuracy_score

my_result = pd.DataFrame(columns=['model', 'acc', 'precision', 'recall'])

colors = ['r', 'c', 'm', 'y', 'k', 'khaki', 'teal', 'orchid', 'sandybrown',
          'greenyellow', 'dodgerblue', 'deepskyblue', 'rosybrown', 'firebrick',
          'deeppink', 'crimson', 'salmon', 'darkred', 'olivedrab', 'olive', 
          'forestgreen', 'royalblue', 'indigo', 'navy', 'mediumpurple', 'chocolate',
          'gold', 'darkorange', 'seagreen', 'turquoise', 'steelblue', 'slategray', 
          'peru', 'midnightblue', 'slateblue', 'dimgray', 'cadetblue', 'tomato'
         ]

# 모델명, 예측값, 실제값을 주면 위의 plot_predictions 함수 호출하여 Scatter 그래프 그리며
# 모델별 MSE값을 Bar chart로 그려줌
def accuracy_eval2(name_, pred, actual):  # DataFrame을 직접 써서 구현
    global predictions
    global colors
    global my_result
    
    plt.figure(figsize=(12, 9))

    #acc
    acc = accuracy_score(actual, pred) * 100
    
    #precision
    precision = precision_score(actual, pred) * 100
    
    #recall
    recall = recall_score(y_test, lg_pred) * 100
    
    new_row = {'model': name_, 'acc': acc, 'precision': precision, 'recall': recall}
    my_result = my_result.append(new_row, ignore_index=True)
    
    sorted_result = my_result.sort_values(by='acc')
    # y_value = sorted(my_predictions.items(), key=lambda x: x[1], reverse=True)
    print(sorted_result)

    length = len(sorted_result)
    
    plt.figure(figsize=(10, length))
    ax = plt.subplot()
    ax.set_yticks(np.arange(len(sorted_result)))
    ax.set_yticklabels(sorted_result['model'], fontsize=15)
    bars = ax.barh(np.arange(len(sorted_result)), sorted_result['acc'])
    
    for i, v in enumerate(sorted_result['acc']):
        idx = np.random.choice(len(colors))
        bars[i].set_color(colors[idx])
        ax.text(v + 2, i, str(round(v, 3)), color='k', fontsize=15, fontweight='bold')
        
    plt.title('accuracy', fontsize=18)
    plt.xlim(0, 100)
    
    plt.show()
```

###  
### 1) 로지스틱 회귀 (LogisticRegression, 분류)


```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix 
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import classification_report
```

##### <font color=blue> **[문제] LogisticRegression 모델 정의하고 학습시키세요.** </font>


```python
# LogisticRegression 함수 사용 및 정의 : lg 저장
# 정의된 LogisticRegression 학습 fit() : 입력값으로 X_train, y_train 준다.
lg = LogisticRegression()
lg.fit(X_train,y_train)

```




    LogisticRegression()




```python
# 분류기 성능 평가(score)
lg.score(X_test, y_test)
```




    0.929637526652452



- 분류기 성능 평가 지표


```python
lg_pred = lg.predict(X_test)
```


```python
# 오차행렬
# TN  FP
# FN  TP

confusion_matrix(y_test, lg_pred) 
```




    array([[2098,   11],
           [ 154,   82]])




```python
# 정확도 : 굉장히 높다
accuracy_score(y_test, lg_pred)  
```




    0.929637526652452




```python
# 정밀도
precision_score(y_test, lg_pred) 
```




    0.8817204301075269




```python
# 재현율 : 굉장히 낮다.
recall_score(y_test, lg_pred)  
```




    0.3474576271186441




```python
# 정밀도 + 재현율
f1_score(y_test, lg_pred) 
```




    0.4984802431610942




```python
print(classification_report(y_test, lg_pred))
```

                  precision    recall  f1-score   support
    
               0       0.93      0.99      0.96      2109
               1       0.88      0.35      0.50       236
    
        accuracy                           0.93      2345
       macro avg       0.91      0.67      0.73      2345
    weighted avg       0.93      0.93      0.92      2345
    



```python
accuracy_eval('LogisticRegression', lg_pred, y_test)
```

                    model   accuracy
    0  LogisticRegression  92.963753



    <Figure size 864x648 with 0 Axes>


![output_62_2](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/6c91bb77-6ea9-4ec2-b3da-d4589354e9b7)

    



```python
accuracy_eval('KSG-TEST', [0,0,0,0,0,1,1,1,1,1], [0,0,0,0,1,1,1,1,1,1])
```

                    model   accuracy
    0  LogisticRegression  92.963753
    1            KSG-TEST  90.000000



    <Figure size 864x648 with 0 Axes>


![output_63_2](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/d50c47df-a527-4464-b2bc-c2750c8b6a43)


    


###   
### 2) KNN (K-Nearest Neighbor)


```python
from sklearn.neighbors import KNeighborsClassifier
```


```python
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
```




    KNeighborsClassifier()




```python
knn_pred = knn.predict(X_test)
```


```python
accuracy_eval('K-Nearest Neighbor', knn_pred, y_test)
```

                    model   accuracy
    0  K-Nearest Neighbor  94.712154
    1  LogisticRegression  92.963753
    2            KSG-TEST  90.000000



    <Figure size 864x648 with 0 Axes>


![output_68_2](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/7a93a12e-5f49-433c-aef4-e3c2386b9798)


    


###  
### 3) 결정트리(DecisionTree)


```python
from sklearn.tree import DecisionTreeClassifier
```


```python
dt = DecisionTreeClassifier(max_depth=10, random_state=42)
dt.fit(X_train, y_train)
```




    DecisionTreeClassifier(max_depth=10, random_state=42)



##### <font color=blue> **[문제] 학습된 DecisionTreeClassifier 모델로 예측해 보기** </font>


```python
# DecisionTreeClassifier 학습 모델 : dt
# DecisionTreeClassifier 모델의 predict() 활용 : 입력값으로 X_test
# 결과 : dt_pred 저장
dt_pred = dt.predict(X_test)
    
```


```python
# 정확도 계산
acc = accuracy_score(y_test, dt_pred)
print(acc)
```

    0.9731343283582089



```python
accuracy_eval2('DecisionTree', dt_pred, y_test)
```

              model        acc  precision     recall
    0  DecisionTree  97.313433  87.772926  34.745763



    <Figure size 864x648 with 0 Axes>



    
![output_75_2](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/6c60bc33-87d9-4a36-a0a8-50bcc47a3fe7)

    


### 
### **앙상블 기법의 종류**
- 배깅 (Bagging): 여러개의 DecisionTree 활용하고 샘플 중복 생성을 통해 결과 도출. RandomForest
- 부스팅 (Boosting): 약한 학습기를 순차적으로 학습을 하되, 이전 학습에 대하여 잘못 예측된 데이터에 가중치를 부여해 오차를 보완해 나가는 방식. XGBoost, LGBM
- 스태킹 (Stacking): 여러 모델을 기반으로 예측된 결과를 통해 Final 학습기(meta 모델)이 다시 한번 예측

![앙상블](https://teddylee777.github.io/images/2019-12-18/image-20191217144823555.png)

###  
### 4) 랜덤포레스트(RandomForest)
+ Bagging 대표적인 모델로써, 훈련셋트를 무작위로 각기 다른 서브셋으로 데이터셋을 만들고
+ 여러개의 DecisonTree로 학습하고 다수결로 결정하는 모델

**주요 Hyperparameter**
- random_state: 랜덤 시드 고정 값. 고정해두고 튜닝할 것!
- n_jobs: CPU 사용 갯수
- max_depth: 깊어질 수 있는 최대 깊이. 과대적합 방지용
- n_estimators: 앙상블하는 트리의 갯수
- max_features: 최대로 사용할 feature의 갯수. 과대적합 방지용
- min_samples_split: 트리가 분할할 때 최소 샘플의 갯수. default=2. 과대적합 방지용


```python
from sklearn.ensemble import RandomForestClassifier
```


```python
rfc = RandomForestClassifier(n_estimators=3, random_state=42)
rfc.fit(X_train, y_train)
```




    RandomForestClassifier(n_estimators=3, random_state=42)




```python
rfc_pred = rfc.predict(X_test)
print(rfc_pred)
```

    [0 0 0 ... 0 0 0]



```python
accuracy_eval2('RandomForest Ensemble', rfc_pred, y_test)
```

                       model        acc  precision     recall
    0           DecisionTree  97.313433  87.772926  34.745763
    1  RandomForest Ensemble  97.611940  87.815126  34.745763



    <Figure size 864x648 with 0 Axes>



    
![output_83_2](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/26e46217-f17f-4a3e-95d6-c6b697f6ac4d)

    


###  
### 5) XGBoost
+ 여러개의 DecisionTree를 결합하여 Strong Learner 만드는 Boosting 앙상블 기법
+ Kaggle 대회에서 자주 사용하는 모델이다.

**주요 특징**
- scikit-learn 패키지가 아닙니다.
- 성능이 우수함
- GBM보다는 빠르고 성능도 향상되었습니다.
- 여전히 학습시간이 매우 느리다

**주요 Hyperparameter**
- random_state: 랜덤 시드 고정 값. 고정해두고 튜닝할 것!
- n_jobs: CPU 사용 갯수
- learning_rate: 학습율. 너무 큰 학습율은 성능을 떨어뜨리고, 너무 작은 학습율은 학습이 느리다. 적절한 값을 찾아야함. n_estimators와 같이 튜닝. default=0.1
- n_estimators: 부스팅 스테이지 수. (랜덤포레스트 트리의 갯수 설정과 비슷한 개념). default=100
- max_depth: 트리의 깊이. 과대적합 방지용. default=3. 
- subsample: 샘플 사용 비율. 과대적합 방지용. default=1.0
- max_features: 최대로 사용할 feature의 비율. 과대적합 방지용. default=1.0


```python
!pip install xgboost
```

    Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (0.90)
    Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.5.4)
    Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.19.5)
    [33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m



```python
from xgboost import XGBClassifier
```


```python
xgb = XGBClassifier(n_estimators=3, random_state=42)  
xgb.fit(X_train, y_train)
```




    XGBClassifier(n_estimators=3, random_state=42)




```python
xgb_pred = xgb.predict(X_test)
```


```python
accuracy_eval('XGBoost', xgb_pred, y_test)
```

                       model   accuracy
    0  RandomForest Ensemble  97.611940
    1                XGBoost  97.611940
    2           DecisionTree  97.313433
    3      Stacking Ensemble  96.247335
    4     K-Nearest Neighbor  94.712154
    5     LogisticRegression  92.963753
    6               KSG-TEST  90.000000
    7                   LGBM  89.936034



    <Figure size 864x648 with 0 Axes>



    
![output_91_2](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/dc0eb1f1-8b90-46c6-8f52-61ef459e8cb1)

    



```python
accuracy_eval2('XGBoost', xgb_pred, y_test)
```

                       model        acc  precision     recall
    0           DecisionTree  97.313433  87.772926  34.745763
    1  RandomForest Ensemble  97.611940  87.815126  34.745763
    2                XGBoost  97.611940  89.130435  34.745763
    3                XGBoost  97.611940  89.130435  34.745763



    <Figure size 864x648 with 0 Axes>



![output_92_2](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/1e352146-1866-46cf-bbcc-db78df6aaf3a)
    

    


###  
### 6) Light GBM
+ XGBoost와 함께 주목받는 DecisionTree 알고리즘 기반의 Boosting 앙상블 기법
+ XGBoost에 비해 학습시간이 짧은 편이다.

**주요 특징**
- scikit-learn 패키지가 아닙니다.
- 성능이 우수함
- 속도도 매우 빠릅니다.

**주요 Hyperparameter**
- random_state: 랜덤 시드 고정 값. 고정해두고 튜닝할 것!
- n_jobs: CPU 사용 갯수
- learning_rate: 학습율. 너무 큰 학습율은 성능을 떨어뜨리고, 너무 작은 학습율은 학습이 느리다. 적절한 값을 찾아야함. n_estimators와 같이 튜닝. default=0.1
- n_estimators: 부스팅 스테이지 수. (랜덤포레스트 트리의 갯수 설정과 비슷한 개념). default=100
- max_depth: 트리의 깊이. 과대적합 방지용. default=3. 
- colsample_bytree: 샘플 사용 비율 (max_features와 비슷한 개념). 과대적합 방지용. default=1.0


```python
!pip install lightgbm
```

    Requirement already satisfied: lightgbm in /usr/local/lib/python3.6/dist-packages (2.3.0)
    Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from lightgbm) (0.24.2)
    Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.5.4)
    Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.19.5)
    Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->lightgbm) (3.1.0)
    Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->lightgbm) (1.1.0)
    [33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m



```python
from lightgbm import LGBMClassifier
```


```python
lgbm = LGBMClassifier(n_estimators=50, random_state=42)    # n_estimators를 늘리니 정확도가 좋아짐
lgbm.fit(X_train, y_train)
```




    LGBMClassifier(n_estimators=50, random_state=42)




```python
lgbm_pred = lgbm.predict(X_test)
```


```python
accuracy_eval('LGBM', lgbm_pred, y_test)
```

                       model   accuracy
    0                   LGBM  97.739872
    1  RandomForest Ensemble  97.611940
    2                XGBoost  97.611940
    3      Weighted Blending  97.569296
    4           DecisionTree  97.313433
    5      Stacking Ensemble  96.247335
    6     K-Nearest Neighbor  94.712154
    7     LogisticRegression  92.963753
    8               KSG-TEST  90.000000



    <Figure size 864x648 with 0 Axes>



    
![output_100_2](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/daa3a2e5-2b81-42a3-9d44-497cb8b704c5)

    


#### 
### 7) Stacking

개별 모델이 예측한 데이터를 기반으로 **final_estimator** 종합하여 예측을 수행합니다.
- 성능을 극으로 끌어올릴 때 활용하기도 합니다.
- 과대적합을 유발할 수 있습니다. (특히, 데이터셋이 적은 경우)


```python
from sklearn.ensemble import StackingRegressor, StackingClassifier
```


```python
stack_models = [
    ('LogisticRegression', lg), 
    ('KNN', knn), 
    ('DecisionTree', dt),
]
```


```python
# stack_models로 선언된 모델(LogisticRegression,KNN,DecisionTree)의 예측결과를 최종 meta_model(final_estimator)을 RandomForest(rfc) 사용하여 분류 예측 
stacking = StackingClassifier(stack_models, final_estimator=rfc, n_jobs=-1)
```


```python
stacking.fit(X_train, y_train)   # 1분 20초 소요
```




    StackingClassifier(estimators=[('LogisticRegression', LogisticRegression()),
                                   ('KNN', KNeighborsClassifier()),
                                   ('DecisionTree',
                                    DecisionTreeClassifier(max_depth=10,
                                                           random_state=42))],
                       final_estimator=RandomForestClassifier(n_estimators=3,
                                                              random_state=42),
                       n_jobs=-1)




```python
stacking_pred = stacking.predict(X_test)
```


```python
accuracy_eval('Stacking Ensemble', stacking_pred, y_test)
```

                       model   accuracy
    0                   LGBM  97.739872
    1  RandomForest Ensemble  97.611940
    2                XGBoost  97.611940
    3      Weighted Blending  97.569296
    4           DecisionTree  97.313433
    5      Stacking Ensemble  96.247335
    6     K-Nearest Neighbor  94.712154
    7     LogisticRegression  92.963753
    8               KSG-TEST  90.000000



    <Figure size 864x648 with 0 Axes>



    
![output_108_2](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/5aeb3bde-bd94-4b8f-8952-00df6eeb9664)

    


#### 
### 8) Weighted Blending

각 모델의 예측값에 대하여 weight를 곱하여 최종 output 계산
- 모델에 대한 가중치를 조절하여, 최종 output을 산출합니다.
- **가중치의 합은 1.0**이 되도록 합니다.


```python
final_outputs = {
    'DecisionTree': dt_pred, 
    'randomforest': rfc_pred, 
    'xgb': xgb_pred, 
    'lgbm': lgbm_pred,
    'stacking': stacking_pred,
}
```


```python
final_prediction=\
final_outputs['DecisionTree'] * 0.1\
+final_outputs['randomforest'] * 0.2\
+final_outputs['xgb'] * 0.25\
+final_outputs['lgbm'] * 0.15\
+final_outputs['stacking'] * 0.3\
```


```python
# 가중치 계산값이 0.5 초과하면 1, 그렇지 않으면 0
final_prediction = np.where(final_prediction > 0.5, 1, 0)
```


```python
accuracy_eval('Weighted Blending', final_prediction, y_test)
```

                       model   accuracy
    0  RandomForest Ensemble  97.611940
    1                XGBoost  97.611940
    2      Weighted Blending  97.569296
    3           DecisionTree  97.313433
    4      Stacking Ensemble  96.247335
    5     K-Nearest Neighbor  94.712154
    6     LogisticRegression  92.963753
    7               KSG-TEST  90.000000
    8                   LGBM  89.936034



    <Figure size 864x648 with 0 Axes>



    
![output_114_2](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/50415cac-b616-4b3a-98f5-3f48c2eff5d9)

    


## 
## 배운 내용 정리
1. 머신러닝 모델 프로세스 <br>
① 라이브러리 임포트(import)  
② 데이터 가져오기(Loading the data)  
③ 탐색적 데이터 분석(Exploratory Data Analysis)  
④ 데이터 전처리(Data PreProcessing) : 데이터타입 변환, Null 데이터 처리, 누락데이터 처리, 
더미특성 생성, 특성 추출 (feature engineering) 등  
⑤ Train, Test  데이터셋 분할  
⑥ 데이터 정규화(Normalizing the Data)  
⑦ 모델 개발(Creating the Model)  
⑧ 모델 성능 평가
2. 평가 지표 활용 : 모델별 성능 확인을 위한 함수 (가져다 쓰면 된다)
3. 단일 분류예측 모델 : LogisticRegression, KNN, DecisionTree
4. 앙상블 (Ensemble) : RandomForest, XGBoost, LGBM, Stacking, Weighted Blending


```python

```
