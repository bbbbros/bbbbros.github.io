# [ì‹¤ìŠµ-í€´ì¦ˆ] Pythonì„ í™œìš©í•œ AI ëª¨ë¸ë§ - ë¨¸ì‹ ëŸ¬ë‹ íŒŒíŠ¸
+ ì´ë²ˆì‹œê°„ì—ëŠ” Pythonì„ í™œìš©í•œ AI ëª¨ë¸ë§ì—ì„œ ë¨¸ì‹ ëŸ¬ë‹ì— ëŒ€í•´ ì‹¤ìŠµí•´ ë³´ê² ìŠµë‹ˆë‹¤.
+ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì—ëŠ” ì•„ë˜ì™€ ê°™ì´ ëª¨ë¸ë“¤ì´ ìˆìŠµë‹ˆë‹¤.
 + ë‹¨ì¼ ë¶„ë¥˜ì˜ˆì¸¡ ëª¨ë¸ : LogisticRegression, KNN, DecisionTree
 + ì•™ìƒë¸”(Ensemble) ëª¨ë¸ : RandomForest, XGBoost, LGBM, Stacking, Weighted Blending
+ ì†”ì§íˆ, ë¨¸ì‹ ëŸ¬ë‹ì´ ë”¥ëŸ¬ë‹ë³´ë‹¤ ì½”ë”©í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤. 4ì¤„ í…œí”Œë¦¿ì— ë§ì³ ì½”ë”©í•˜ë©´ ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
+ í•œê°€ì§€ ë‹¹ë¶€ ë“œë¦¬ê³  ì‹¶ì€ ë§ì€ "ë°±ë¬¸ì´ë¶ˆì—¬ì¼íƒ€" ì…ë‹ˆë‹¤. 
+ ì´ë¡ ë³´ë‹¤ ì‹¤ìŠµì´ ë” ë§ì€ ì‹œê°„ê³¼ ë…¸ë ¥ì´ íˆ¬ì ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

## í•™ìŠµëª©ì°¨
1. ì‹¤ìŠµì¤€ë¹„
2. ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í”„ë¡œì„¸ìŠ¤
 - ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
 - ë°ì´í„° ì „ì²˜ë¦¬
 - Train, Test ë°ì´í„°ì…‹ ë¶„í• 
 - ë°ì´í„° ì •ê·œí™”
 - ë‹¨ì¼ ë¶„ë¥˜ì˜ˆì¸¡ ëª¨ë¸ : LogisticRegression, KNN, DecisionTree
 - ì•™ìƒë¸”(Ensemble) ëª¨ë¸ : RandomForest, XGBoost, LGBM, Stacking, Weighted Blending


# 
# 1. ì‹¤ìŠµì¤€ë¹„


```python
# ì½”ë“œì‹¤í–‰ì‹œ ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ

import warnings
warnings.filterwarnings(action='ignore') 
```

# 
# 2. ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í”„ë¡œì„¸ìŠ¤
â‘  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸(import)  
â‘¡ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°(Loading the data)  
â‘¢ íƒìƒ‰ì  ë°ì´í„° ë¶„ì„(Exploratory Data Analysis)  
â‘£ ë°ì´í„° ì „ì²˜ë¦¬(Data PreProcessing) : ë°ì´í„°íƒ€ì… ë³€í™˜, Null ë°ì´í„° ì²˜ë¦¬, ëˆ„ë½ë°ì´í„° ì²˜ë¦¬, 
ë”ë¯¸íŠ¹ì„± ìƒì„±, íŠ¹ì„± ì¶”ì¶œ (feature engineering) ë“±  
â‘¤ Train, Test  ë°ì´í„°ì…‹ ë¶„í•   
â‘¥ ë°ì´í„° ì •ê·œí™”(Normalizing the Data)  
â‘¦ ëª¨ë¸ ê°œë°œ(Creating the Model)  
â‘§ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€

## â‘  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸

##### í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

## â‘¡ ë°ì´í„° ë¡œë“œ

#### cust_data.csv íŒŒì¼ ì»¬ëŸ¼ëª…
+ ê³ ê°ë“±ê¸‰(class), ì„±ë³„(sex), ë‚˜ì´(age), ì‚¬ìš©ì„œë¹„ìŠ¤ìˆ˜(service), ì„œë¹„ìŠ¤ì¤‘ì§€ì—¬ë¶€ (stop), ë¯¸ë‚©ì—¬ë¶€(npay)
+ 3ê°œì›” í‰ê·  ìš”ê¸ˆ(avg_bill), Aì„œë¹„ìŠ¤ 3ê°œì›” í‰ê· ìš”ê¸ˆ(A_bill), Bì„œë¹„ìŠ¤ 3ê°œì›” í‰ê· ìš”ê¸ˆ(B_bill), í•´ì§€ì—¬ë¶€(termination)


```python
# ì•ìª½ ì „ì²˜ë¦¬ì—ì„œ ì €ì¥í•œ cust_data.csv íŒŒì¼ ì½ê¸°
df = pd.read_csv('cust_data.csv')
```


```python
# í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
# df1 = df[['cust_class', 'sex_type', 'age', 'efct_svc_count', 'dt_stop_yn', 'npay_yn', 'r3m_avg_bill_amt', 'r3m_A_avg_arpu_amt', 'r3m_B_avg_arpu_amt', 'termination_yn']]
# cust = df1.rename(columns={'cust_class': 'class', 
#                            'sex_type': 'sex', 
#                            'efct_svc_count' : 'service', 
#                            'dt_stop_yn' : 'stop', 
#                            'r3m_avg_bill_amt' : 'avg_bill', 
#                            'r3m_A_avg_arpu_amt' : 'A_bill',
#                            'r3m_B_avg_arpu_amt' : 'B_bill',
#                            'termination_yn' : 'termination'}).copy()
# cust.to_csv('cust_data4ë¨¸ì‹ ëŸ¬ë‹.csv', index=False)

```

## â‘¢ ë°ì´í„° ë¶„ì„


```python
# 12ì»¬ëŸ¼, 7814 ë¼ì¸
df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 7814 entries, 0 to 7813
    Data columns (total 12 columns):
     #   Column       Non-Null Count  Dtype  
    ---  ------       --------------  -----  
     0   class        7814 non-null   object 
     1   sex          7814 non-null   object 
     2   age          7814 non-null   int64  
     3   service      7814 non-null   int64  
     4   stop         7814 non-null   object 
     5   npay         7814 non-null   object 
     6   avg_bill     7814 non-null   float64
     7   A_bill       7814 non-null   float64
     8   B_bill       7814 non-null   float64
     9   termination  7814 non-null   object 
     10  by_age       7814 non-null   int64  
     11  bill_rating  7814 non-null   object 
    dtypes: float64(3), int64(3), object(6)
    memory usage: 732.7+ KB



```python
df.tail()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>class</th>
      <th>sex</th>
      <th>age</th>
      <th>service</th>
      <th>stop</th>
      <th>npay</th>
      <th>avg_bill</th>
      <th>A_bill</th>
      <th>B_bill</th>
      <th>termination</th>
      <th>by_age</th>
      <th>bill_rating</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>7809</th>
      <td>C</td>
      <td>M</td>
      <td>76</td>
      <td>1</td>
      <td>N</td>
      <td>N</td>
      <td>1860.0000</td>
      <td>1716.000000</td>
      <td>0.0000</td>
      <td>N</td>
      <td>75</td>
      <td>low</td>
    </tr>
    <tr>
      <th>7810</th>
      <td>C</td>
      <td>F</td>
      <td>15</td>
      <td>1</td>
      <td>N</td>
      <td>Y</td>
      <td>1296.0999</td>
      <td>194.414985</td>
      <td>643.1001</td>
      <td>N</td>
      <td>15</td>
      <td>low</td>
    </tr>
    <tr>
      <th>7811</th>
      <td>G</td>
      <td>M</td>
      <td>12</td>
      <td>1</td>
      <td>N</td>
      <td>N</td>
      <td>13799.6666</td>
      <td>2069.949990</td>
      <td>10605.9266</td>
      <td>N</td>
      <td>10</td>
      <td>midhigh</td>
    </tr>
    <tr>
      <th>7812</th>
      <td>C</td>
      <td>F</td>
      <td>40</td>
      <td>0</td>
      <td>N</td>
      <td>N</td>
      <td>3140.0000</td>
      <td>942.000000</td>
      <td>1884.0000</td>
      <td>Y</td>
      <td>40</td>
      <td>low</td>
    </tr>
    <tr>
      <th>7813</th>
      <td>C</td>
      <td>F</td>
      <td>59</td>
      <td>1</td>
      <td>N</td>
      <td>N</td>
      <td>2436.9000</td>
      <td>365.535000</td>
      <td>1839.9000</td>
      <td>N</td>
      <td>55</td>
      <td>low</td>
    </tr>
  </tbody>
</table>
</div>




```python
# termination ë ˆì´ë¸” ë¶ˆê· í˜• 
# df['termination'].value_counts()
df['termination'].value_counts().plot(kind='bar')
```




    <AxesSubplot:>



![output_15_1](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/7751cc50-6e59-4450-86d9-476a6a9527d5)
    


## â‘£ ë°ì´í„° ì „ì²˜ë¦¬

+ Object ì»¬ëŸ¼ì— ëŒ€í•´ Pandas get_dummies í•¨ìˆ˜ í™œìš©í•˜ì—¬ One-Hot-Encoding


```python
df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 7814 entries, 0 to 7813
    Data columns (total 12 columns):
     #   Column       Non-Null Count  Dtype  
    ---  ------       --------------  -----  
     0   class        7814 non-null   object 
     1   sex          7814 non-null   object 
     2   age          7814 non-null   int64  
     3   service      7814 non-null   int64  
     4   stop         7814 non-null   object 
     5   npay         7814 non-null   object 
     6   avg_bill     7814 non-null   float64
     7   A_bill       7814 non-null   float64
     8   B_bill       7814 non-null   float64
     9   termination  7814 non-null   object 
     10  by_age       7814 non-null   int64  
     11  bill_rating  7814 non-null   object 
    dtypes: float64(3), int64(3), object(6)
    memory usage: 732.7+ KB



```python
# Object ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ ì •ì˜
cal_cols = ['class', 'sex', 'stop', 'npay', 'termination', 'bill_rating']
```


```python
# pandas get_dummies í•¨ìˆ˜ ì‚¬ìš©í•˜ì—¬ Object ì»¬ëŸ¼ì— ëŒ€í•´ One-Hot-Encoding ìˆ˜í–‰
df1 = pd.get_dummies(data=df, columns=cal_cols, drop_first=True)
```


```python
# 19ì»¬ëŸ¼, 7814 ë¼ì¸
df1.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 7814 entries, 0 to 7813
    Data columns (total 19 columns):
     #   Column               Non-Null Count  Dtype  
    ---  ------               --------------  -----  
     0   age                  7814 non-null   int64  
     1   service              7814 non-null   int64  
     2   avg_bill             7814 non-null   float64
     3   A_bill               7814 non-null   float64
     4   B_bill               7814 non-null   float64
     5   by_age               7814 non-null   int64  
     6   class_D              7814 non-null   uint8  
     7   class_E              7814 non-null   uint8  
     8   class_F              7814 non-null   uint8  
     9   class_G              7814 non-null   uint8  
     10  class_H              7814 non-null   uint8  
     11  sex_M                7814 non-null   uint8  
     12  stop_Y               7814 non-null   uint8  
     13  npay_Y               7814 non-null   uint8  
     14  termination_Y        7814 non-null   uint8  
     15  bill_rating_low      7814 non-null   uint8  
     16  bill_rating_lowmid   7814 non-null   uint8  
     17  bill_rating_mid      7814 non-null   uint8  
     18  bill_rating_midhigh  7814 non-null   uint8  
    dtypes: float64(3), int64(3), uint8(13)
    memory usage: 465.6 KB



```python
df1.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>service</th>
      <th>avg_bill</th>
      <th>A_bill</th>
      <th>B_bill</th>
      <th>by_age</th>
      <th>class_D</th>
      <th>class_E</th>
      <th>class_F</th>
      <th>class_G</th>
      <th>class_H</th>
      <th>sex_M</th>
      <th>stop_Y</th>
      <th>npay_Y</th>
      <th>termination_Y</th>
      <th>bill_rating_low</th>
      <th>bill_rating_lowmid</th>
      <th>bill_rating_mid</th>
      <th>bill_rating_midhigh</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>28</td>
      <td>0</td>
      <td>2640.0000</td>
      <td>792.00000</td>
      <td>1584.0000</td>
      <td>25</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>24</td>
      <td>1</td>
      <td>16840.0000</td>
      <td>2526.00000</td>
      <td>6983.0000</td>
      <td>20</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>32</td>
      <td>1</td>
      <td>15544.7334</td>
      <td>2331.71001</td>
      <td>6750.4666</td>
      <td>30</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>18</td>
      <td>1</td>
      <td>4700.0000</td>
      <td>0.00000</td>
      <td>4502.0000</td>
      <td>15</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>78</td>
      <td>1</td>
      <td>1361.7999</td>
      <td>1173.99990</td>
      <td>0.0000</td>
      <td>75</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



## â‘¤ Train, Test  ë°ì´í„°ì…‹ ë¶„í• 

#### ì…ë ¥(X)ê³¼ ë ˆì´ë¸”(y) ë‚˜ëˆ„ê¸°

##### <font color=blue> **[ë¬¸ì œ] df1 DataFrameì—ì„œ 'termination_Y' ì»¬ëŸ¼ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì •ë³´ë¥¼ Xì— ì €ì¥í•˜ì„¸ìš”.** </font>


```python
# DataFrame drop í•¨ìˆ˜ í™œìš©
# 'termination_Y' ì»¬ëŸ¼ ì‚­ì œ
# DataFrameì—ì„œ valuesë§Œ Xì— ì €ì¥

X = df1.drop(columns=['termination_Y'], axis=1).values
X

```




    array([[2.80000000e+01, 0.00000000e+00, 2.64000000e+03, ...,
            0.00000000e+00, 0.00000000e+00, 0.00000000e+00],
           [2.40000000e+01, 1.00000000e+00, 1.68400000e+04, ...,
            0.00000000e+00, 0.00000000e+00, 0.00000000e+00],
           [3.20000000e+01, 1.00000000e+00, 1.55447334e+04, ...,
            0.00000000e+00, 0.00000000e+00, 0.00000000e+00],
           ...,
           [1.20000000e+01, 1.00000000e+00, 1.37996666e+04, ...,
            0.00000000e+00, 0.00000000e+00, 1.00000000e+00],
           [4.00000000e+01, 0.00000000e+00, 3.14000000e+03, ...,
            0.00000000e+00, 0.00000000e+00, 0.00000000e+00],
           [5.90000000e+01, 1.00000000e+00, 2.43690000e+03, ...,
            0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])



##### <font color=blue> **[ë¬¸ì œ] df1 DataFrameì—ì„œ 'termination_Y' ì»¬ëŸ¼ì˜ ê°’ì„ yë¡œ ì €ì¥í•˜ì„¸ìš”.** </font>


```python
# DataFrame 'termination_Y' ì»¬ëŸ¼ ì‚¬ìš©
# DataFrameì—ì„œ valuesë§Œ yì— ì €ì¥
y = df1['termination_Y'].values
y

```




    array([1, 0, 0, ..., 0, 1, 0], dtype=uint8)




```python
X.shape, y.shape
```




    ((7814, 18), (7814,))



#### Train , Test dataset ë‚˜ëˆ„ê¸°


```python
from sklearn.model_selection import train_test_split 
```

##### <font color=blue> **[ë¬¸ì œ] Train dataset, Test dataset ë‚˜ëˆ„ì„¸ìš”.** </font>


```python
# Train dataset, Test dataset ë‚˜ëˆ„ê¸° : train_test_split í•¨ìˆ˜ ì‚¬ìš©
# ì…ë ¥ : X, y 
# Train : Test ë¹„ìœ¨ = 7: 3  --> test_size=0.3
# y Class ë¹„ìœ¨ì— ë§ê²Œ ë‚˜ì£¼ì–´ ì£¼ì„¸ìš” : stratify=y
# ì—¬ëŸ¬ë²ˆ ìˆ˜í–‰í•´ë„ ê°™ì€ ê²°ê³¼ ë‚˜ì˜¤ê²Œ ê³ ì •í•˜ê¸° : random_state=42 
# ê²°ê³¼ : X_train, X_test, y_train, y_test
X_train, X_test, y_train, y_test = train_test_split(X
                                                    , y
                                                    , test_size=0.3
                                                    ,stratify=y
                                                    , random_state=42
                                                    # , shuffle = False
                                                   )




```


```python
X_train.shape
```




    (5469, 18)




```python
y_train.shape
```




    (5469,)



## â‘¥ ë°ì´í„° ì •ê·œí™”/ìŠ¤ì¼€ì¼ë§(Normalizing/Scaling)


```python
# ìˆ«ì ë¶„í¬ ì´ë£¨ì–´ì§„ ì»¬ëŸ¼ í™•ì¸
df1.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>service</th>
      <th>avg_bill</th>
      <th>A_bill</th>
      <th>B_bill</th>
      <th>by_age</th>
      <th>class_D</th>
      <th>class_E</th>
      <th>class_F</th>
      <th>class_G</th>
      <th>class_H</th>
      <th>sex_M</th>
      <th>stop_Y</th>
      <th>npay_Y</th>
      <th>termination_Y</th>
      <th>bill_rating_low</th>
      <th>bill_rating_lowmid</th>
      <th>bill_rating_mid</th>
      <th>bill_rating_midhigh</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>28</td>
      <td>0</td>
      <td>2640.0000</td>
      <td>792.00000</td>
      <td>1584.0000</td>
      <td>25</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>24</td>
      <td>1</td>
      <td>16840.0000</td>
      <td>2526.00000</td>
      <td>6983.0000</td>
      <td>20</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>32</td>
      <td>1</td>
      <td>15544.7334</td>
      <td>2331.71001</td>
      <td>6750.4666</td>
      <td>30</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>18</td>
      <td>1</td>
      <td>4700.0000</td>
      <td>0.00000</td>
      <td>4502.0000</td>
      <td>15</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>78</td>
      <td>1</td>
      <td>1361.7999</td>
      <td>1173.99990</td>
      <td>0.0000</td>
      <td>75</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




```python
# print(type(X_train)) # ë°ì´í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ëŠ”ë‹¤.
X_train[0][0]

```




    42.0




```python
from sklearn.preprocessing import MinMaxScaler
```

##### <font color=blue> **[ë¬¸ì œ] MinMaxScaler í•¨ìˆ˜ë¥¼ 'scaler'ë¡œ ì •ì˜ í•˜ì„¸ìš”.** </font>
## MinMaxScalerëŠ” ë°ì´í„°ë¥¼ 0~1ì‚¬ì´ ë°ì´í„°ë¡œ ë°”ê¿”ì¤€ë‹¤.
## fit_transform, transform ì‚¬ìš©ì— ì£¼ì˜!!!!!


```python
# ì‚¬ì´í‚¤ëŸ°ì˜ MinMaxScaler() í•¨ìˆ˜ í™œìš©
# ì •ì˜í•  ê²°ê³¼ë¥¼ 'scaler'ë¡œ ë§¤í•‘


scaler = MinMaxScaler()



```


```python
X_train = scaler.fit_transform(X_train)   ### fit_transformì€ ë°ì´í„°ë¡œ í•™ìŠµì„ í•œë‹¤. ê·¸ë˜ì„œ train ë°ì´í„°ë¡œ ì…ë ¥
X_test = scaler.transform(X_test)   ### transformì€ í•™ìŠµë˜ì–´ì§„ ëª¨ë¸ì„ í†µí•´ ìŠ¤ì¼€ì¼ë§
```


```python
# X_train[:2], y_train[:2]
X_train[:4], y_train[:4]
# X_train.max()
```




    (array([[0.38      , 0.33333333, 0.4295439 , 0.06384702, 0.41944434,
             0.4       , 0.        , 0.        , 0.        , 0.        ,
             1.        , 1.        , 0.        , 1.        , 0.        ,
             0.        , 0.        , 1.        ],
            [0.58      , 0.11111111, 0.20111297, 0.38498933, 0.        ,
             0.6       , 1.        , 0.        , 0.        , 0.        ,
             0.        , 1.        , 0.        , 0.        , 0.        ,
             1.        , 0.        , 0.        ],
            [0.51      , 0.22222222, 0.5025271 , 0.32347352, 0.67293822,
             0.55      , 0.        , 0.        , 0.        , 1.        ,
             0.        , 1.        , 0.        , 0.        , 0.        ,
             0.        , 0.        , 1.        ],
            [0.62      , 0.11111111, 0.13149699, 0.        , 0.21702104,
             0.65      , 1.        , 0.        , 0.        , 0.        ,
             0.        , 1.        , 0.        , 0.        , 0.        ,
             1.        , 0.        , 0.        ]]),
     array([0, 0, 0, 0], dtype=uint8))



+ ëª¨ë¸ ì…ë ¥ê°¯ìˆ˜, ì¶œë ¥ê°¯ìˆ˜ í™•ì¸

## â‘¦ ëª¨ë¸ ê°œë°œ

#### ëª¨ë¸ë³„ ë°”ì°¨íŠ¸ ê·¸ë ¤ì£¼ê³  ì„±ëŠ¥ í™•ì¸ì„ ìœ„í•œ í•¨ìˆ˜


```python
# ëª¨ë¸ë³„ë¡œ Accuracy ì ìˆ˜ ì €ì¥
# ëª¨ë¸ Accuracy ì ìˆ˜ ìˆœì„œëŒ€ë¡œ ë°”ì°¨íŠ¸ë¥¼ ê·¸ë ¤ ëª¨ë¸ë³„ë¡œ ì„±ëŠ¥ í™•ì¸ ê°€ëŠ¥

from sklearn.metrics import accuracy_score

my_predictions = {}

colors = ['r', 'c', 'm', 'y', 'k', 'khaki', 'teal', 'orchid', 'sandybrown',
          'greenyellow', 'dodgerblue', 'deepskyblue', 'rosybrown', 'firebrick',
          'deeppink', 'crimson', 'salmon', 'darkred', 'olivedrab', 'olive', 
          'forestgreen', 'royalblue', 'indigo', 'navy', 'mediumpurple', 'chocolate',
          'gold', 'darkorange', 'seagreen', 'turquoise', 'steelblue', 'slategray', 
          'peru', 'midnightblue', 'slateblue', 'dimgray', 'cadetblue', 'tomato'
         ]

# ëª¨ë¸ëª…, ì˜ˆì¸¡ê°’, ì‹¤ì œê°’ì„ ì£¼ë©´ ìœ„ì˜ plot_predictions í•¨ìˆ˜ í˜¸ì¶œí•˜ì—¬ Scatter ê·¸ë˜í”„ ê·¸ë¦¬ë©°
# ëª¨ë¸ë³„ MSEê°’ì„ Bar chartë¡œ ê·¸ë ¤ì¤Œ
def accuracy_eval(name_, pred, actual):
    global predictions
    global colors

    plt.figure(figsize=(12, 9))

    acc = accuracy_score(actual, pred)
    my_predictions[name_] = acc * 100
    
    y_value = sorted(my_predictions.items(), key=lambda x: x[1], reverse=True)
    
    df = pd.DataFrame(y_value, columns=['model', 'accuracy'])
    print(df)
   
    length = len(df)
    
    plt.figure(figsize=(10, length))
    ax = plt.subplot()
    ax.set_yticks(np.arange(len(df)))
    ax.set_yticklabels(df['model'], fontsize=15)
    bars = ax.barh(np.arange(len(df)), df['accuracy'])
    
    for i, v in enumerate(df['accuracy']):
        idx = np.random.choice(len(colors))
        bars[i].set_color(colors[idx])
        ax.text(v + 2, i, str(round(v, 3)), color='k', fontsize=15, fontweight='bold')
        
    plt.title('accuracy', fontsize=18)
    plt.xlim(0, 100)
    
    plt.show()
```


```python
# ëª¨ë¸ë³„ë¡œ Accuracy ì ìˆ˜ ì €ì¥
# ëª¨ë¸ Accuracy ì ìˆ˜ ìˆœì„œëŒ€ë¡œ ë°”ì°¨íŠ¸ë¥¼ ê·¸ë ¤ ëª¨ë¸ë³„ë¡œ ì„±ëŠ¥ í™•ì¸ ê°€ëŠ¥

from sklearn.metrics import accuracy_score

my_result = pd.DataFrame(columns=['model', 'acc', 'precision', 'recall'])

colors = ['r', 'c', 'm', 'y', 'k', 'khaki', 'teal', 'orchid', 'sandybrown',
          'greenyellow', 'dodgerblue', 'deepskyblue', 'rosybrown', 'firebrick',
          'deeppink', 'crimson', 'salmon', 'darkred', 'olivedrab', 'olive', 
          'forestgreen', 'royalblue', 'indigo', 'navy', 'mediumpurple', 'chocolate',
          'gold', 'darkorange', 'seagreen', 'turquoise', 'steelblue', 'slategray', 
          'peru', 'midnightblue', 'slateblue', 'dimgray', 'cadetblue', 'tomato'
         ]

# ëª¨ë¸ëª…, ì˜ˆì¸¡ê°’, ì‹¤ì œê°’ì„ ì£¼ë©´ ìœ„ì˜ plot_predictions í•¨ìˆ˜ í˜¸ì¶œí•˜ì—¬ Scatter ê·¸ë˜í”„ ê·¸ë¦¬ë©°
# ëª¨ë¸ë³„ MSEê°’ì„ Bar chartë¡œ ê·¸ë ¤ì¤Œ
def accuracy_eval2(name_, pred, actual):  # DataFrameì„ ì§ì ‘ ì¨ì„œ êµ¬í˜„
    global predictions
    global colors
    global my_result
    
    plt.figure(figsize=(12, 9))

    #acc
    acc = accuracy_score(actual, pred) * 100
    
    #precision
    precision = precision_score(actual, pred) * 100
    
    #recall
    recall = recall_score(y_test, lg_pred) * 100
    
    new_row = {'model': name_, 'acc': acc, 'precision': precision, 'recall': recall}
    my_result = my_result.append(new_row, ignore_index=True)
    
    sorted_result = my_result.sort_values(by='acc')
    # y_value = sorted(my_predictions.items(), key=lambda x: x[1], reverse=True)
    print(sorted_result)

    length = len(sorted_result)
    
    plt.figure(figsize=(10, length))
    ax = plt.subplot()
    ax.set_yticks(np.arange(len(sorted_result)))
    ax.set_yticklabels(sorted_result['model'], fontsize=15)
    bars = ax.barh(np.arange(len(sorted_result)), sorted_result['acc'])
    
    for i, v in enumerate(sorted_result['acc']):
        idx = np.random.choice(len(colors))
        bars[i].set_color(colors[idx])
        ax.text(v + 2, i, str(round(v, 3)), color='k', fontsize=15, fontweight='bold')
        
    plt.title('accuracy', fontsize=18)
    plt.xlim(0, 100)
    
    plt.show()
```

###  
### 1) ë¡œì§€ìŠ¤í‹± íšŒê·€ (LogisticRegression, ë¶„ë¥˜)


```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix 
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import classification_report
```

##### <font color=blue> **[ë¬¸ì œ] LogisticRegression ëª¨ë¸ ì •ì˜í•˜ê³  í•™ìŠµì‹œí‚¤ì„¸ìš”.** </font>


```python
# LogisticRegression í•¨ìˆ˜ ì‚¬ìš© ë° ì •ì˜ : lg ì €ì¥
# ì •ì˜ëœ LogisticRegression í•™ìŠµ fit() : ì…ë ¥ê°’ìœ¼ë¡œ X_train, y_train ì¤€ë‹¤.
lg = LogisticRegression()
lg.fit(X_train,y_train)

```




    LogisticRegression()




```python
# ë¶„ë¥˜ê¸° ì„±ëŠ¥ í‰ê°€(score)
lg.score(X_test, y_test)
```




    0.929637526652452



- ë¶„ë¥˜ê¸° ì„±ëŠ¥ í‰ê°€ ì§€í‘œ


```python
lg_pred = lg.predict(X_test)
```


```python
# ì˜¤ì°¨í–‰ë ¬
# TN  FP
# FN  TP

confusion_matrix(y_test, lg_pred) 
```




    array([[2098,   11],
           [ 154,   82]])




```python
# ì •í™•ë„ : êµ‰ì¥íˆ ë†’ë‹¤
accuracy_score(y_test, lg_pred)  
```




    0.929637526652452




```python
# ì •ë°€ë„
precision_score(y_test, lg_pred) 
```




    0.8817204301075269




```python
# ì¬í˜„ìœ¨ : êµ‰ì¥íˆ ë‚®ë‹¤.
recall_score(y_test, lg_pred)  
```




    0.3474576271186441




```python
# ì •ë°€ë„ + ì¬í˜„ìœ¨
f1_score(y_test, lg_pred) 
```




    0.4984802431610942




```python
print(classification_report(y_test, lg_pred))
```

                  precision    recall  f1-score   support
    
               0       0.93      0.99      0.96      2109
               1       0.88      0.35      0.50       236
    
        accuracy                           0.93      2345
       macro avg       0.91      0.67      0.73      2345
    weighted avg       0.93      0.93      0.92      2345
    



```python
accuracy_eval('LogisticRegression', lg_pred, y_test)
```

                    model   accuracy
    0  LogisticRegression  92.963753



    <Figure size 864x648 with 0 Axes>


![output_62_2](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/6c91bb77-6ea9-4ec2-b3da-d4589354e9b7)

    



```python
accuracy_eval('KSG-TEST', [0,0,0,0,0,1,1,1,1,1], [0,0,0,0,1,1,1,1,1,1])
```

                    model   accuracy
    0  LogisticRegression  92.963753
    1            KSG-TEST  90.000000



    <Figure size 864x648 with 0 Axes>


![output_63_2](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/d50c47df-a527-4464-b2bc-c2750c8b6a43)


    


###   
### 2) KNN (K-Nearest Neighbor)


```python
from sklearn.neighbors import KNeighborsClassifier
```


```python
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
```




    KNeighborsClassifier()




```python
knn_pred = knn.predict(X_test)
```


```python
accuracy_eval('K-Nearest Neighbor', knn_pred, y_test)
```

                    model   accuracy
    0  K-Nearest Neighbor  94.712154
    1  LogisticRegression  92.963753
    2            KSG-TEST  90.000000



    <Figure size 864x648 with 0 Axes>


![output_68_2](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/7a93a12e-5f49-433c-aef4-e3c2386b9798)


    


###  
### 3) ê²°ì •íŠ¸ë¦¬(DecisionTree)


```python
from sklearn.tree import DecisionTreeClassifier
```


```python
dt = DecisionTreeClassifier(max_depth=10, random_state=42)
dt.fit(X_train, y_train)
```




    DecisionTreeClassifier(max_depth=10, random_state=42)



##### <font color=blue> **[ë¬¸ì œ] í•™ìŠµëœ DecisionTreeClassifier ëª¨ë¸ë¡œ ì˜ˆì¸¡í•´ ë³´ê¸°** </font>


```python
# DecisionTreeClassifier í•™ìŠµ ëª¨ë¸ : dt
# DecisionTreeClassifier ëª¨ë¸ì˜ predict() í™œìš© : ì…ë ¥ê°’ìœ¼ë¡œ X_test
# ê²°ê³¼ : dt_pred ì €ì¥
dt_pred = dt.predict(X_test)
    
```


```python
# ì •í™•ë„ ê³„ì‚°
acc = accuracy_score(y_test, dt_pred)
print(acc)
```

    0.9731343283582089



```python
accuracy_eval2('DecisionTree', dt_pred, y_test)
```

              model        acc  precision     recall
    0  DecisionTree  97.313433  87.772926  34.745763



    <Figure size 864x648 with 0 Axes>



    
![output_75_2](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/6c60bc33-87d9-4a36-a0a8-50bcc47a3fe7)

    


### 
### **ì•™ìƒë¸” ê¸°ë²•ì˜ ì¢…ë¥˜**
- ë°°ê¹… (Bagging): ì—¬ëŸ¬ê°œì˜ DecisionTree í™œìš©í•˜ê³  ìƒ˜í”Œ ì¤‘ë³µ ìƒì„±ì„ í†µí•´ ê²°ê³¼ ë„ì¶œ. RandomForest
- ë¶€ìŠ¤íŒ… (Boosting): ì•½í•œ í•™ìŠµê¸°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµì„ í•˜ë˜, ì´ì „ í•™ìŠµì— ëŒ€í•˜ì—¬ ì˜ëª» ì˜ˆì¸¡ëœ ë°ì´í„°ì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•´ ì˜¤ì°¨ë¥¼ ë³´ì™„í•´ ë‚˜ê°€ëŠ” ë°©ì‹. XGBoost, LGBM
- ìŠ¤íƒœí‚¹ (Stacking): ì—¬ëŸ¬ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡ëœ ê²°ê³¼ë¥¼ í†µí•´ Final í•™ìŠµê¸°(meta ëª¨ë¸)ì´ ë‹¤ì‹œ í•œë²ˆ ì˜ˆì¸¡

![ì•™ìƒë¸”](https://teddylee777.github.io/images/2019-12-18/image-20191217144823555.png)

###  
### 4) ëœë¤í¬ë ˆìŠ¤íŠ¸(RandomForest)
+ Bagging ëŒ€í‘œì ì¸ ëª¨ë¸ë¡œì¨, í›ˆë ¨ì…‹íŠ¸ë¥¼ ë¬´ì‘ìœ„ë¡œ ê°ê¸° ë‹¤ë¥¸ ì„œë¸Œì…‹ìœ¼ë¡œ ë°ì´í„°ì…‹ì„ ë§Œë“¤ê³ 
+ ì—¬ëŸ¬ê°œì˜ DecisonTreeë¡œ í•™ìŠµí•˜ê³  ë‹¤ìˆ˜ê²°ë¡œ ê²°ì •í•˜ëŠ” ëª¨ë¸

**ì£¼ìš” Hyperparameter**
- random_state: ëœë¤ ì‹œë“œ ê³ ì • ê°’. ê³ ì •í•´ë‘ê³  íŠœë‹í•  ê²ƒ!
- n_jobs: CPU ì‚¬ìš© ê°¯ìˆ˜
- max_depth: ê¹Šì–´ì§ˆ ìˆ˜ ìˆëŠ” ìµœëŒ€ ê¹Šì´. ê³¼ëŒ€ì í•© ë°©ì§€ìš©
- n_estimators: ì•™ìƒë¸”í•˜ëŠ” íŠ¸ë¦¬ì˜ ê°¯ìˆ˜
- max_features: ìµœëŒ€ë¡œ ì‚¬ìš©í•  featureì˜ ê°¯ìˆ˜. ê³¼ëŒ€ì í•© ë°©ì§€ìš©
- min_samples_split: íŠ¸ë¦¬ê°€ ë¶„í• í•  ë•Œ ìµœì†Œ ìƒ˜í”Œì˜ ê°¯ìˆ˜. default=2. ê³¼ëŒ€ì í•© ë°©ì§€ìš©


```python
from sklearn.ensemble import RandomForestClassifier
```


```python
rfc = RandomForestClassifier(n_estimators=3, random_state=42)
rfc.fit(X_train, y_train)
```




    RandomForestClassifier(n_estimators=3, random_state=42)




```python
rfc_pred = rfc.predict(X_test)
print(rfc_pred)
```

    [0 0 0 ... 0 0 0]



```python
accuracy_eval2('RandomForest Ensemble', rfc_pred, y_test)
```

                       model        acc  precision     recall
    0           DecisionTree  97.313433  87.772926  34.745763
    1  RandomForest Ensemble  97.611940  87.815126  34.745763



    <Figure size 864x648 with 0 Axes>



    
![output_83_2](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/26e46217-f17f-4a3e-95d6-c6b697f6ac4d)

    


###  
### 5) XGBoost
+ ì—¬ëŸ¬ê°œì˜ DecisionTreeë¥¼ ê²°í•©í•˜ì—¬ Strong Learner ë§Œë“œëŠ” Boosting ì•™ìƒë¸” ê¸°ë²•
+ Kaggle ëŒ€íšŒì—ì„œ ìì£¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì´ë‹¤.

**ì£¼ìš” íŠ¹ì§•**
- scikit-learn íŒ¨í‚¤ì§€ê°€ ì•„ë‹™ë‹ˆë‹¤.
- ì„±ëŠ¥ì´ ìš°ìˆ˜í•¨
- GBMë³´ë‹¤ëŠ” ë¹ ë¥´ê³  ì„±ëŠ¥ë„ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.
- ì—¬ì „íˆ í•™ìŠµì‹œê°„ì´ ë§¤ìš° ëŠë¦¬ë‹¤

**ì£¼ìš” Hyperparameter**
- random_state: ëœë¤ ì‹œë“œ ê³ ì • ê°’. ê³ ì •í•´ë‘ê³  íŠœë‹í•  ê²ƒ!
- n_jobs: CPU ì‚¬ìš© ê°¯ìˆ˜
- learning_rate: í•™ìŠµìœ¨. ë„ˆë¬´ í° í•™ìŠµìœ¨ì€ ì„±ëŠ¥ì„ ë–¨ì–´ëœ¨ë¦¬ê³ , ë„ˆë¬´ ì‘ì€ í•™ìŠµìœ¨ì€ í•™ìŠµì´ ëŠë¦¬ë‹¤. ì ì ˆí•œ ê°’ì„ ì°¾ì•„ì•¼í•¨. n_estimatorsì™€ ê°™ì´ íŠœë‹. default=0.1
- n_estimators: ë¶€ìŠ¤íŒ… ìŠ¤í…Œì´ì§€ ìˆ˜. (ëœë¤í¬ë ˆìŠ¤íŠ¸ íŠ¸ë¦¬ì˜ ê°¯ìˆ˜ ì„¤ì •ê³¼ ë¹„ìŠ·í•œ ê°œë…). default=100
- max_depth: íŠ¸ë¦¬ì˜ ê¹Šì´. ê³¼ëŒ€ì í•© ë°©ì§€ìš©. default=3. 
- subsample: ìƒ˜í”Œ ì‚¬ìš© ë¹„ìœ¨. ê³¼ëŒ€ì í•© ë°©ì§€ìš©. default=1.0
- max_features: ìµœëŒ€ë¡œ ì‚¬ìš©í•  featureì˜ ë¹„ìœ¨. ê³¼ëŒ€ì í•© ë°©ì§€ìš©. default=1.0


```python
!pip install xgboost
```

    Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (0.90)
    Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.5.4)
    Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.19.5)
    [33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m



```python
from xgboost import XGBClassifier
```


```python
xgb = XGBClassifier(n_estimators=3, random_state=42)  
xgb.fit(X_train, y_train)
```




    XGBClassifier(n_estimators=3, random_state=42)




```python
xgb_pred = xgb.predict(X_test)
```


```python
accuracy_eval('XGBoost', xgb_pred, y_test)
```

                       model   accuracy
    0  RandomForest Ensemble  97.611940
    1                XGBoost  97.611940
    2           DecisionTree  97.313433
    3      Stacking Ensemble  96.247335
    4     K-Nearest Neighbor  94.712154
    5     LogisticRegression  92.963753
    6               KSG-TEST  90.000000
    7                   LGBM  89.936034



    <Figure size 864x648 with 0 Axes>



    
![output_91_2](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/dc0eb1f1-8b90-46c6-8f52-61ef459e8cb1)

    



```python
accuracy_eval2('XGBoost', xgb_pred, y_test)
```

                       model        acc  precision     recall
    0           DecisionTree  97.313433  87.772926  34.745763
    1  RandomForest Ensemble  97.611940  87.815126  34.745763
    2                XGBoost  97.611940  89.130435  34.745763
    3                XGBoost  97.611940  89.130435  34.745763



    <Figure size 864x648 with 0 Axes>



![output_92_2](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/1e352146-1866-46cf-bbcc-db78df6aaf3a)
    

    


###  
### 6) Light GBM
+ XGBoostì™€ í•¨ê»˜ ì£¼ëª©ë°›ëŠ” DecisionTree ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ì˜ Boosting ì•™ìƒë¸” ê¸°ë²•
+ XGBoostì— ë¹„í•´ í•™ìŠµì‹œê°„ì´ ì§§ì€ í¸ì´ë‹¤.

**ì£¼ìš” íŠ¹ì§•**
- scikit-learn íŒ¨í‚¤ì§€ê°€ ì•„ë‹™ë‹ˆë‹¤.
- ì„±ëŠ¥ì´ ìš°ìˆ˜í•¨
- ì†ë„ë„ ë§¤ìš° ë¹ ë¦…ë‹ˆë‹¤.

**ì£¼ìš” Hyperparameter**
- random_state: ëœë¤ ì‹œë“œ ê³ ì • ê°’. ê³ ì •í•´ë‘ê³  íŠœë‹í•  ê²ƒ!
- n_jobs: CPU ì‚¬ìš© ê°¯ìˆ˜
- learning_rate: í•™ìŠµìœ¨. ë„ˆë¬´ í° í•™ìŠµìœ¨ì€ ì„±ëŠ¥ì„ ë–¨ì–´ëœ¨ë¦¬ê³ , ë„ˆë¬´ ì‘ì€ í•™ìŠµìœ¨ì€ í•™ìŠµì´ ëŠë¦¬ë‹¤. ì ì ˆí•œ ê°’ì„ ì°¾ì•„ì•¼í•¨. n_estimatorsì™€ ê°™ì´ íŠœë‹. default=0.1
- n_estimators: ë¶€ìŠ¤íŒ… ìŠ¤í…Œì´ì§€ ìˆ˜. (ëœë¤í¬ë ˆìŠ¤íŠ¸ íŠ¸ë¦¬ì˜ ê°¯ìˆ˜ ì„¤ì •ê³¼ ë¹„ìŠ·í•œ ê°œë…). default=100
- max_depth: íŠ¸ë¦¬ì˜ ê¹Šì´. ê³¼ëŒ€ì í•© ë°©ì§€ìš©. default=3. 
- colsample_bytree: ìƒ˜í”Œ ì‚¬ìš© ë¹„ìœ¨ (max_featuresì™€ ë¹„ìŠ·í•œ ê°œë…). ê³¼ëŒ€ì í•© ë°©ì§€ìš©. default=1.0


```python
!pip install lightgbm
```

    Requirement already satisfied: lightgbm in /usr/local/lib/python3.6/dist-packages (2.3.0)
    Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from lightgbm) (0.24.2)
    Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.5.4)
    Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.19.5)
    Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->lightgbm) (3.1.0)
    Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->lightgbm) (1.1.0)
    [33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m



```python
from lightgbm import LGBMClassifier
```


```python
lgbm = LGBMClassifier(n_estimators=50, random_state=42)    # n_estimatorsë¥¼ ëŠ˜ë¦¬ë‹ˆ ì •í™•ë„ê°€ ì¢‹ì•„ì§
lgbm.fit(X_train, y_train)
```




    LGBMClassifier(n_estimators=50, random_state=42)




```python
lgbm_pred = lgbm.predict(X_test)
```


```python
accuracy_eval('LGBM', lgbm_pred, y_test)
```

                       model   accuracy
    0                   LGBM  97.739872
    1  RandomForest Ensemble  97.611940
    2                XGBoost  97.611940
    3      Weighted Blending  97.569296
    4           DecisionTree  97.313433
    5      Stacking Ensemble  96.247335
    6     K-Nearest Neighbor  94.712154
    7     LogisticRegression  92.963753
    8               KSG-TEST  90.000000



    <Figure size 864x648 with 0 Axes>



    
![output_100_2](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/daa3a2e5-2b81-42a3-9d44-497cb8b704c5)

    


#### 
### 7) Stacking

ê°œë³„ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **final_estimator** ì¢…í•©í•˜ì—¬ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- ì„±ëŠ¥ì„ ê·¹ìœ¼ë¡œ ëŒì–´ì˜¬ë¦´ ë•Œ í™œìš©í•˜ê¸°ë„ í•©ë‹ˆë‹¤.
- ê³¼ëŒ€ì í•©ì„ ìœ ë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (íŠ¹íˆ, ë°ì´í„°ì…‹ì´ ì ì€ ê²½ìš°)


```python
from sklearn.ensemble import StackingRegressor, StackingClassifier
```


```python
stack_models = [
    ('LogisticRegression', lg), 
    ('KNN', knn), 
    ('DecisionTree', dt),
]
```


```python
# stack_modelsë¡œ ì„ ì–¸ëœ ëª¨ë¸(LogisticRegression,KNN,DecisionTree)ì˜ ì˜ˆì¸¡ê²°ê³¼ë¥¼ ìµœì¢… meta_model(final_estimator)ì„ RandomForest(rfc) ì‚¬ìš©í•˜ì—¬ ë¶„ë¥˜ ì˜ˆì¸¡ 
stacking = StackingClassifier(stack_models, final_estimator=rfc, n_jobs=-1)
```


```python
stacking.fit(X_train, y_train)   # 1ë¶„ 20ì´ˆ ì†Œìš”
```




    StackingClassifier(estimators=[('LogisticRegression', LogisticRegression()),
                                   ('KNN', KNeighborsClassifier()),
                                   ('DecisionTree',
                                    DecisionTreeClassifier(max_depth=10,
                                                           random_state=42))],
                       final_estimator=RandomForestClassifier(n_estimators=3,
                                                              random_state=42),
                       n_jobs=-1)




```python
stacking_pred = stacking.predict(X_test)
```


```python
accuracy_eval('Stacking Ensemble', stacking_pred, y_test)
```

                       model   accuracy
    0                   LGBM  97.739872
    1  RandomForest Ensemble  97.611940
    2                XGBoost  97.611940
    3      Weighted Blending  97.569296
    4           DecisionTree  97.313433
    5      Stacking Ensemble  96.247335
    6     K-Nearest Neighbor  94.712154
    7     LogisticRegression  92.963753
    8               KSG-TEST  90.000000



    <Figure size 864x648 with 0 Axes>



    
![output_108_2](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/5aeb3bde-bd94-4b8f-8952-00df6eeb9664)

    


#### 
### 8) Weighted Blending

ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì— ëŒ€í•˜ì—¬ weightë¥¼ ê³±í•˜ì—¬ ìµœì¢… output ê³„ì‚°
- ëª¨ë¸ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì ˆí•˜ì—¬, ìµœì¢… outputì„ ì‚°ì¶œí•©ë‹ˆë‹¤.
- **ê°€ì¤‘ì¹˜ì˜ í•©ì€ 1.0**ì´ ë˜ë„ë¡ í•©ë‹ˆë‹¤.


```python
final_outputs = {
    'DecisionTree': dt_pred, 
    'randomforest': rfc_pred, 
    'xgb': xgb_pred, 
    'lgbm': lgbm_pred,
    'stacking': stacking_pred,
}
```


```python
final_prediction=\
final_outputs['DecisionTree'] * 0.1\
+final_outputs['randomforest'] * 0.2\
+final_outputs['xgb'] * 0.25\
+final_outputs['lgbm'] * 0.15\
+final_outputs['stacking'] * 0.3\
```


```python
# ê°€ì¤‘ì¹˜ ê³„ì‚°ê°’ì´ 0.5 ì´ˆê³¼í•˜ë©´ 1, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ 0
final_prediction = np.where(final_prediction > 0.5, 1, 0)
```


```python
accuracy_eval('Weighted Blending', final_prediction, y_test)
```

                       model   accuracy
    0  RandomForest Ensemble  97.611940
    1                XGBoost  97.611940
    2      Weighted Blending  97.569296
    3           DecisionTree  97.313433
    4      Stacking Ensemble  96.247335
    5     K-Nearest Neighbor  94.712154
    6     LogisticRegression  92.963753
    7               KSG-TEST  90.000000
    8                   LGBM  89.936034



    <Figure size 864x648 with 0 Axes>



    
![output_114_2](https://github.com/bbbbros/bbbbros.github.io/assets/161530952/50415cac-b616-4b3a-98f5-3f48c2eff5d9)

    


## 
## ë°°ìš´ ë‚´ìš© ì •ë¦¬
1. ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í”„ë¡œì„¸ìŠ¤ <br>
â‘  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸(import)  
â‘¡ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°(Loading the data)  
â‘¢ íƒìƒ‰ì  ë°ì´í„° ë¶„ì„(Exploratory Data Analysis)  
â‘£ ë°ì´í„° ì „ì²˜ë¦¬(Data PreProcessing) : ë°ì´í„°íƒ€ì… ë³€í™˜, Null ë°ì´í„° ì²˜ë¦¬, ëˆ„ë½ë°ì´í„° ì²˜ë¦¬, 
ë”ë¯¸íŠ¹ì„± ìƒì„±, íŠ¹ì„± ì¶”ì¶œ (feature engineering) ë“±  
â‘¤ Train, Test  ë°ì´í„°ì…‹ ë¶„í•   
â‘¥ ë°ì´í„° ì •ê·œí™”(Normalizing the Data)  
â‘¦ ëª¨ë¸ ê°œë°œ(Creating the Model)  
â‘§ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
2. í‰ê°€ ì§€í‘œ í™œìš© : ëª¨ë¸ë³„ ì„±ëŠ¥ í™•ì¸ì„ ìœ„í•œ í•¨ìˆ˜ (ê°€ì ¸ë‹¤ ì“°ë©´ ëœë‹¤)
3. ë‹¨ì¼ ë¶„ë¥˜ì˜ˆì¸¡ ëª¨ë¸ : LogisticRegression, KNN, DecisionTree
4. ì•™ìƒë¸” (Ensemble) : RandomForest, XGBoost, LGBM, Stacking, Weighted Blending


```python

```
