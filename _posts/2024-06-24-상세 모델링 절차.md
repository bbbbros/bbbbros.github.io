
# *** 전체 과정 ***
# ① 라이브러리 임포트(import)
    import sklearn as sk
	import pandas as pd  #
	import numpy as np
	

# ② 데이터 가져오기(Loading the data)
	import pandas as pd  
	import numpy as np

	csv파일입력	cust = pd.read_csv('./ex.csv', sep =';')  # read_json
	
	* 데이터 병합
	concat이용 	
    pd.concat([df1,df2], ignore_index=False, axis=1)  #ignore_index : 인덱스 재배열여부, axis : 0은 row레벨 1은 column레벨
    pd.concat([df3,df4],join='inner') # 컬럼을 기준으로 inner한다두개 df를 교집합으로 붙이고 ,인덱스는 기존 인덱스 사용 (ignore_index=false) 가 기본값
	merge이용	pd.merge(customer, orders, on='cust_id', how='inner') # 위의 내용과 같지만 공통된 컬럼을 기준(how에 따라)으로 합친다

	* 데이터 저장 : cust.to_csv('./ex.csv', index=False) # 인덱스는 저장하지 않음
	

# ③ 탐색적 데이터 분석(Exploratory Data Analysis)
### ** 시각적 데이터 분석1 (matplotlib)
    %matplotlib inline
    import seaborn as sns
    import matplotlib.pyplot as plt
    
    * matplotlib로 그리기
    # df를 통한 그래프
    df['avg_bill'].plot(figsize=(50,30))
    plt.show()
    
    # 산점도 그리기 : 상관관계 분석
    plt.figure(figsize=(16,6))
    plt.scatter(y=df["avg_bill"], x=df["age"])
    plt.show()
    
    # df를 이용한 산점도 
    df['color'] = df['Result_v1'].map({1:"blue", -1:"red"})
    y_list = df.columns
    for i in range(0, len(y_list)):
        df.plot(kind='scatter', x='Result_v1', y=y_list[i],s=30, c=df['color'])
        plt.title('title', fontsize=20)
        plt.xlabel('Result_v1')
        plt.ylabel(y_list[i])
        plt.show()
    
    # 히스토 그램
    plt.figure()
    plt.hist(df["age"],bins=20)
    plt.show()
    
    # 박스차트
    plt.boxplot(x=x)   # matplotlib 이용
    plt.show()
    
    df.boxplot(by="by_age", column="avg_bill", figsize=(16,8)) # df이용
    plt.show()
    
    # 여러개 그래프 : subplot
    
    # 특정 컬럼에 대한 데이터나 그래프는 pivot이용
    df_pivot=pd.pivot_table(df, index = ['service'])    # 서비스에 대한 평균 요금
    plt.figure(figsize=(15,5))
    plt.plot(df_pivot['avg_bill'],'rD-')
    plt.show()
        
        
    * seaborn 시각적 데이터분석 2 ()
    %matplotlib inline
    import seaborn as sns
     
    # 1.heatmap (상관관계를 표현)
    sns.heatmap(df.corr())
    plt.show()
    
    # 2. boxplot (수치적 자료 표현, 결측치 확인)
    plt.figure(figsize=(16,8))
    sns.boxplot(y=df["avg_bill"], x=df["by_age"],width=0.9)
    plt.show()
    
    # 3. CountChart
    ax = sns.countplot(x=df_total['level1_pnu'], palette = "RdBu")
    df_total['level1_pnu'].value_counts()
    
    # 4. DistChart
    x = df_total['signaltype']
    sns.distplot(x)
    plt.show()
    
    # Pariplot : 데이터 프레임의 수치형 변수를 기준으로 밀도와 분포를 한 눈에 확인
    sns.pairplot(df_total)
    plt.show()

    # BarPlot 아래로 3개 그래프
    i=1
    for c_type in ['개인','개인사업자','법인']:
        plt.subplot(3,1,i)
        graph_data=prod_stat[prod_stat['TYPE']==c_type]
        sns.barplot(data = graph_data,x='PRODUCT_NM',y='CNT')
    i=i+1
    
	plt.show()
		

    * 기타
    #corr 활용하여 상관관계가 적은것 찾기
    df_ex.corr()['target'].sort_values(ascending=False)
    
    #DataFrame 필터링
    df_total = df_total[df_total['perhour'] < 130]
    
    df_tt = df_tt[(df_tt['level1_pnu'] == '서울특별시') | (df_tt['level1_pnu'] == '경기도')] # []의 내부 조건에는 &, | 사이에는 ()를 꼭 써야한다.
    df_tt = df_tt.reset_index(drop=True)
    
    # 한글과 마이너스 깨짐 방지
    import matplotlib.font_manager as fm

    fm.get_fontconfig_fonts()

    font_list = [font.name for font in fm.fontManager.ttflist]
    font_list
    sns.set(font="NanumGothicCoding", 
            rc={"axes.unicode_minus":False}, # 마이너스 부호 깨짐 현상 해결
            style='darkgrid')

    #groupby
    group_ansan = ansan_data.groupby(['컬럼명1','컬럼명2'])['분석할 컬럼명'].count()
    group_ansan.count()
	
	# sorting 정렬
	df = df.sort_values(by=['컬럼1','컬럼2'], ascending=False).reset_index() # 내림차순


# ④ 데이터 전처리(Data PreProcessing)
  : 데이터타입 변환, Null 데이터 처리, 누락데이터 처리, 더미특성 생성, 특성 추출 (feature engineering) 등
    * null값 개수 확인
    df.isna().sum() # np.NaN 
    
    * 중복항목 제거
    df=df.drop_duplicates()
    
    * 데이터 중 특정컬럼값들만 가져오기
    cust = df[['cust_class', 'sex_type', 'age']]
    cust = df.loc[df['cust_class']=='개인',:] 또는 cust = df[df['TYPE']=='개인']
    # date가 인덱스일 경우 날짜로 구분할 수 있음 
    train_df = df.loc[:'2023-11-30']
    test_df = df.loc['2023-12-01':]

    * 불필요한 컬럼 제거, 삭제
    df.drop(columns=["url_chinese_present","html_num_tags('applet')"],inplace=True) 

    * 컬럼명 변경
    cust = cust.rename(columns={'cust_class':'class', 'sex_type':'sex'})

    * 데이터 값 변경 // 유일값 확인 df['Result_v1'].unique()
    cust = cust.replace("_", np.NaN)
    
    df_ex['state'] = df_ex['state'].replace('NY','NewYork')
    df_ex['state'] = df_ex['state'].replace('CA','California')    
    
    df_ex['state'].replace({'CA':'California','NY':'NewYork'}, inplace=True)
    
    * 결측치 처리
    cust=cust.fillna(15) # Null값을 15로 변경 
    cust[['service','A_bill', 'B_bill']] = cust[['service','A_bill', 'B_bill']].fillna(0) # 특정 컬럼만 변경
    # inplace 사용법,  df_ex['BUILDING_NM'].fillna("", inplace=True)  # [가 하나에 유의
    cust=cust.fillna(method='bfill') # Null값을 뒤에 값으로 변경 (앞값 : ffill)
    cust['age'].interpolate() # 보간법 이용

    * 결측치 제거
    cust = cust.dropna(how=’any’) # null이 하나라도 있으면 그 Raw 삭제
    cust = cust.dropna(how=’all’) # 전체행이 null일 경우만 그 Raw 삭제
    df_ex.dropna(subset=['lon'],how='any', inplace=True) # 특정 컬럼값만 확인하여 그 Row를 삭제 
    
    * 범주형 이상치 처리
    cust_data = cust[(cust['class']!='H')]  # H만 제외한 나머지를 저장하는 방법
    cust_data['class'] = cust_data['class'].replace('H','F') # H를 F로 변경
    cust_data['class'].value_counts() # class 값 확인

    * 특정값 바꾸기
    ansan_data.loc[ansan_data['TYPE_DTL_NM']=='ㆍ값없음', 'TYPE'] = '개인사업자' # 컬럼중 특정(ㆍ값없음)값 확인하여 특정 컬럼(TYPE)값 변경
    
    cust_data.loc[(cust_data['TYPE']=='공공기관')|      # 여러개의 값 변경 'TYPE'항목을 확인 ==> 'TYPE'
                   (cust_data['TYPE']=='기타') |
                   (cust_data['TYPE']=='법인사업자')|
                   (cust_data['TYPE']=='정식단체'),'TYPE'] = '법인' 
    
    * 특정값을 기준으로 데이터 분리
    ansan_map_1=ansan_data.loc[ansan_data['TYPE']=='개인',:]
    ansan_map_2=ansan_data.loc[ansan_data['TYPE']=='법인',:]
    
    
    * 수치형 이상치 처리
        #이상치를 제거하는 함수 만들기
    def removeOutliers(x, column):
        # Q1, Q3구하기
        q1 = x[column].quantile(0.25)
        q3 = x[column].quantile(0.75)
        
        # 1.5 * IQR(Q3 - Q1)
        iqr = 1.5 * (q3 - q1)
        
        # 이상치를 제거
        y=x[(x[column] < (q3 + iqr)) & (x[column] > (q1 - iqr))]
        
        return(y)
    cust_data=removeOutliers(cust, 'avg_bill')  # 각 column에 대해서 수행
    cust_data=removeOutliers(cust_data, 'A_bill')
    
        #이상치 변경
    def changeOutliers(data, column):   # 최소값 0, 최대값 q3 + iqr
        x=data.copy()
        # Q1, Q3구하기
        q1 = x[column].quantile(0.25)
        q3 = x[column].quantile(0.75)
        
        # 1.5 * IQR(Q3 - Q1)
        iqr = 1.5 * (q3 - q1)
        
        #이상치 대체값 설정하기
        Min = 0
        if (q1 - iqr) > 0 : Min=(q1 - iqr)
            
        Max = q3 + iqr
        
        # 이상치를 변경
        # X의 값을 직졉 변경해도 되지만 Pyhon Warning이 나오기 떄문에 인덱스를 이용
        x.loc[(x[column] > Max), column]= Max
        x.loc[(x[column] < Min), column]= Min
        
        return(x)
    
    cust_data=changeOutliers(cust, 'avg_bill')

    * 구간 분류
    # cut 임의의 구간
    q1=cust_data['avg_bill'].quantile(0.25)
    q3=cust_data['avg_bill'].quantile(0.75)
    cust_data['bill_rating'] = pd.cut(cust_data['avg_bill'], 
                                    bins = [0, q1, q3, cust['avg_bill'].max()],
                                    labels = ['row', 'middle','high'])
    # 나이
    cust_data['by_age']=cust_data['age']//10*10  ## //는 내림 나눗셈으로  정수형의 몫만 취한다
                                    
                                    
    # 일정구간
    cust_data['bill_rating'] = pd.qcut(cust_data['avg_bill'],3,labels=["row", "meddle", "high"])
    
    * Scaling
    # 데이터 정제
    cust_data_num = cust_data[['avg_bill', 'A_bill', 'B_bill']] # 숫자만 추출
        # 또는     col = ['RID', 'TIME_DEPARTUREDATE', 'TIME_ARRIVEDATE']
        # cust_data_num = cust_data.drop(columns=col) #문자데이터 삭제
    
    # Standardization : 정규 분포를 평균이 0 이고 분산이 1 인 표준 정규 분포로 변환
    Standardization_df = (cust_data_num - cust_data_num.mean())/cust_data_num.std()
    Standardization_df.head()
    
    # Normalizing: 0~1사이 값으로 변환 --> ⑥ 참고
    from sklearn.preprocessing import MinMaxScaler 
    scaler = MinMaxScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    * 원핫인코딩 더미변수 생성 
    df1 = pd.get_dummies(df, columns=['a', 'b', 'c'] ,drop_first=True)  # drop_first:첫번째 항목을 삭제여부
    
    * 라벨인코딩 (Object 컬럼값을 int로 변경)
    from sklearn.preprocessing import LabelEncoder
    
    le_columns=train_data.columns
    le = LabelEncoder()

    for column in le_columns:
        le.fit(train_data[column])
        train_data[column]=le.transform(train_data[column])
        #train_data에 없는 label이 test_data에 있을 수 있으므로 아래 코드가 필요하며, test_data는 fit 없이 transform만 해야함
        for label in np.unique(test_data[column]):
            if label not in le.classes_: # unseen label 데이터인 경우( )
                le.classes_ = np.append(le.classes_, label) # 미처리 시 ValueError발생
        test_data[column]=le.transform(test_data[column])    

    * 기타
    df_temp=pd.DataFrame([]) #데이터프레임 변수 선언
	
    # json데이터 추출하여 pd.DataFrame에 넣기
	for i in np.arange(len(json_data)):
		if json_data[i].json()['documents'] != [] :
			address_data = address_data.append([(json_data[i].json()['documents'][0]['road_address']['address_name'],
												   json_data[i].json()['documents'][0]['road_address']['x'],
												   json_data[i].json()['documents'][0]['road_address']['y'])],
												 ignore_index=True)
	
    df.iloc[:,-3:]  # 데이터 프레임 확인 (뒤에서 3번째부터 끝까지 출력)
    


# ⑤ Train, Test 데이터셋 분할
	from sklearn.model_selection import train_test_split	
    X = df1.drop('termination_Y', axis=1).values   //X = df.iloc[:,0:len(df.columns)-1].values # 0~(끝-1)
    y = df1['termination_Y'].values                //y = df.iloc[:,len(df.columns)-1].values   # 끝
    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.3[, stratify=y], random_state=42)

    # train_data 생성 예시
    train_data = ansan_data_T.sample(frac=0.9, random_state=42)
    test_data = ansan_data_T.drop(train_data.index)
     
    # 이것도 방법임
    from sklearn.model_selection import train_test_split
    train_data, test_data = train_test_split(ansan_data_T, test_size=0.2, random_state=42)

    # test 데이터 분할 (pandas.dataframe 타입으로 값이 나온다)
    X = df.iloc[:,0:len(df.columns)-1].values # 0~(끝-1)
    y = df.iloc[:,len(df.columns)-1].values   # 끝

    x_test_le=test_data_le.iloc[:,1:] # 맨 앞 제외하고 1~끝
    y_test_le=test_data_le.iloc[:,0]  # 맨 앞에만 

    
# ⑦ 머신러닝 - 모델 개발(Creating the Model)
### **** 분류모델
### ** 로지스틱 회귀 (LogisticRegression, 분류모델임에 유의)
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import confusion_matrix 
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    from sklearn.metrics import classification_report   

    lg = LogisticRegression()
    lg.fit(train_x,y_train)

    LogisticRegression()
    
    # 분류기 성능 평가(score)
    lg.score(train_x,train_y), lg.score(test_x, test_y)
    
    # 정확도 
    accuracy_score(test_y, lg_pred) 

    # 정밀도
    precision_score(test_y, lg_pred) 

    # 재현율 : 굉장히 낮다.
    recall_score(test_y, lg_pred) 
    
    # 정밀도 + 재현율
    f1_score(test_y, lg_pred)
    
    # 종합
    print(classification_report(test_y, lg_pred))   
    
    # confusion_matrix를 통한 평가 
    lg_pred = lg.predict(test_x)    
    confusion_matrix(test_y, lg_pred)
    >> array([[2098,   11],
    >> [ 154,   82]])
    
    # feature_importances_ 활용 Feature별 가중치 확인(결정나무 나 앙상블에서 쓰임)
    plt.figure(figsize=(20,12))
    plt.barh(y=df.columns[:-1],
    width = lg.feature_importances_)
    plt.show()

### ** KNN (K-Nearest Neighbor)
    from sklearn.neighbors import KNeighborsClassifier
    
    knn = KNeighborsClassifier(n_neighbors=5)
    knn.fit(X_train, y_train)
    
    KNeighborsClassifier()
    
    knn_pred = knn.predict(X_test)
    
    # 정확도, 정밀도, 재현율 등 LR과 동일

### ** 결정나무 (DecisionTree)
    from sklearn.tree import DecisionTreeClassifier
    
    dt = DecisionTreeClassifier(max_depth=10, random_state=42)
    dt.fit(X_train, y_train)

    DecisionTreeClassifier(max_depth=10, random_state=42)
    dt_pred = dt.predict(X_test)

### ** RandomForest(앙상블)
    from sklearn.ensemble import RandomForestClassifier
    rfc = RandomForestClassifier(n_estimators=3, random_state=42)
    rfc.fit(X_train, y_train)
    rfc_pred = rfc.predict(X_test)
    
### ** Gradient Boost(앙상블)
    from sklearn.ensemble import GradientBoostingClassifier

### ** XG Boost(앙상블)
    from xgboost import XGBClassifier
    xgb = XGBClassifier(n_estimators=3, random_state=42)  
    xgb.fit(X_train, y_train)
    xgb_pred = xgb.predict(X_test)

### ** AdaBoost(앙상블)
    from sklearn.ensemble import AdaBoostClassifier


### ****** 회귀모델
### ** 선형 회귀모델
    from sklearn.linear_model import LinearRegression as lr
    from sklearn.metrics import mean_squared_error, r2_score

    model=lr()
    model.fit(train_x, train_y)

    pred_y = model.predict(test_x)
    print("RMSE on Test set : {0:.5f}".format(mean_squared_error(test_y,pred_y)**0.5))
    print("R-squared Score on Test set : {0:.5f}".format(r2_score(test_y,pred_y)))

### ** 랜덤포레스트
    from sklearn.ensemble import RandomForestRegressor as rfr
    from sklearn.metrics import mean_squared_error, r2_score

    # 다차원 배열을 1차원으로 평평하게 만들어주기!
    train_y = np.ravel(train_y, order='C')

    model=rfr(n_estimators=100,max_depth=5,min_samples_split=30,min_samples_leaf=15)
    # 인자 설명 
    # n_estimators: 앙상블을 구성하는 트리의 개수.(성능)
    # max_depth: 각 트리의 최대 깊이.(과적합 방지)
    # min_samples_split: 노드를 분할하기 위한 최소 샘플 수.(과적합 방지)
    # min_samples_leaf: 리프 노드가 되기 위한 최소 샘플 수.(과적합 방지)
       
    model.fit(train_x, train_y)

    pred_y = model.predict(test_x)
    print("RMSE on Test set : {0:.5f}".format(mean_squared_error(test_y,pred_y)**0.5))
    print("R-squared Score on Test set : {0:.5f}".format(r2_score(test_y,pred_y)))

### ** GBR(Gradient Boost Regression)
    from sklearn.ensemble import GradientBoostingRegressor as grb
    from sklearn.metrics import roc_auc_score, accuracy_score, mean_squared_error, r2_score

    # 다차원 배열을 1차원으로 평평하게 만들어주기!
    train_y = np.ravel(train_y, order='C')

    model=grb(n_estimators=100,learning_rate=0.1,max_depth=5,min_samples_split=30,min_samples_leaf=15)
    # learning_rate: 각 트리의 기여도를 줄이는 학습률. (성능, 기본값: 0.1)
    model.fit(train_x, train_y)

    pred_y = model.predict(test_x)
    print("RMSE on Test set : {0:.5f}".format(mean_squared_error(test_y,pred_y)**0.5))
    print("R-squared Score on Test set : {0:.5f}".format(r2_score(test_y,pred_y)))


 ### ** XGBR (XGBRegressor)
    from xgboost import XGBRegressor as xgb
    from sklearn.metrics import roc_auc_score, accuracy_score, mean_squared_error, r2_score

    # 다차원 배열을 1차원으로 평평하게 만들어주기!
    train_y = np.ravel(train_y, order='C')

    model=xgb(n_estimators=100,gamma=1,eta=0.1,max_depth=5,reg_lambda=5,reg_alpha=5, early_stopping_rounds=10)
    # 인자설명
    # gamma: 트리의 리프 노드를 추가로 분할하기 위해 필요한 최소 손실 감소 (과적합, 기본값: 0).
    # eta (또는 learning_rate): 학습률로, 각 단계의 가중치를 줄이는 역할 (과적합, 기본값: 0.3).
    # reg_lambda (또는 lambda): L2 정규화 항의 가중치 (과적합, 기본값: 1).
    # reg_alpha (또는 alpha): L1 정규화 항의 가중치 (과적합, 기본값: 0).
    # early_stopping_rounds : 성능기대 회수 조정 (성능)
    model.fit(train_x, train_y)

    pred_y = model.predict(test_x)
    print("RMSE on Test set : {0:.5f}".format(mean_squared_error(test_y,pred_y)**0.5))
    print("R-squared Score on Test set : {0:.5f}".format(r2_score(test_y,pred_y)))



### ** 기타
    * Feature의 중요도 확인
    import matplotlib.pyplot as plt
    import seaborn as sns

    xgb_importances_values = model.feature_importances_
    xgb_importances = pd.Series(xgb_importances_values, index = train_x.columns)
    xgb_top10 = xgb_importances.sort_values(ascending=False)[:10]

    plt.rcParams["font.family"] = 'NanumGothicCoding'
    plt.figure(figsize=(8,6))
    plt.title('Top 10 Feature Importances')
    sns.barplot(x=xgb_top10, y=xgb_top10.index,palette = "RdBu")
    plt.show()



# ⑦ 딥러닝 - 모델 개발(Creating the Model)
### **** 분류
### ** DNN
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense, Activation, Dropout
    batch_size = 16
    epochs = 20
    X_train.shape # 입력개수 확인
    # Sequential() 모델 정의 하고 model로 저장
    # input layer는 input_shape=() 옵션을 사용한다.
    # 18개 input layer
    # unit 4개 hidden layer
    # unit 3개 hidden layer 
    # 1개 output layser : 이진분류
    model = Sequential()
    model.add(Dense(4,activation='relu',input_shape=(18,)))
    model.add(Dropout(0.3))
    model.add(Dense(3,activation='relu'))
    model.add(Dropout(0.3))
    model.add(Dense(1,activation='sigmoid')) # 결과값이 1개 sigmoid, 2개 이상 softmax,

    * 조기종료 및 모델저장 설정
    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
    early_stop = EarlyStopping(monitor='val_loss', 
                            mode='min', 
                            verbose=1, 
                            patience=5)
    check_point = ModelCheckpoint('best_model.h5',
                            verbose=1,
                            monitor='val_loss',
                            mode='min',
                            save_best_only=True)

	## (분류모델) 결과값이 1개(sigmoid)인경우 binary_crossentropy, 
	                  2개이상(softmax)인경우  categorical_crossentropy(원핫인코딩O), sparse_categorical_crossentropy(원핫인코딩X)
	


    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc']) 

    * 모델학습 - history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, epochs=epochs)
    history = model.fit(x=X_train, y=y_train, 
          epochs=epochs , batch_size=batch_size,
          validation_data=(X_test, y_test), 
          verbose=1,
          callbacks=[early_stop, check_point])
    
    ** CNN, RNN은 별도 참고

### **** 회귀
### ** DNN
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import Sequential
    from tensorflow.keras.layers import Dense, Activation, Dropout
    from keras.callbacks import ModelCheckpoint, EarlyStopping

    def build_model():
        model = Sequential()
        model.add(Dense(64, activation='relu', input_shape=(len(train_x.keys()),)))
        model.add(Dense(64, activation='relu'))
        model.add(Dense(1))

        model.compile(loss='mse', 
                      optimizer='adam', 
                      metrics=['mae', 'mse'])
        return model

    model = build_model()
    model.summary()
        
    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
    early_stop = EarlyStopping(monitor='val_loss', patience=10)
    check_point = ModelCheckpoint('best_model.h5',
                                  save_weights_only=True, 
                                  monitor='val_loss', 
                                  verbose=1,
                                  save_best_only=True) # 체크포인트 저장
        
    history = model.fit(train_x, train_y, epochs=30,  
                       validation_data = (test_x,test_y),
                        callbacks=[early_stop, check_point]
                        )
        


# ⑧ 모델 성능 평가
### ** 성능 시각화
	* 분류-딥러닝 성능 시각화
    losses = pd.DataFrame(model.history.history)
    losses.head()
    
    losses[['loss','val_loss']].plot()  # losses 데이터로 그래프
    
    losses[['loss','val_loss', 'accuracy','val_accuracy']].plot()
    
    plt.plot(history.history['accuracy'])  # 배열의 인덱스 값이 epochs값임, epochs값이 X값으로 들어감
    plt.plot(history.history['val_accuracy'])
    plt.title('Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Acc')
    plt.legend(['acc', 'val_acc'])
    plt.show()
    
 ### ** 모델 성능평가는 어떻게 할지 좀 모르겠네
	* 라이브러리 로드
	import matplotlib.pyplot as plt
	import seaborn as sns

	from sklearn.linear_model import LinearRegression as lr
	from sklearn.ensemble import RandomForestRegressor as rfr
	from sklearn.ensemble import GradientBoostingRegressor as grb
	from xgboost import XGBRegressor as xgb
	from sklearn.metrics import roc_auc_score, accuracy_score, mean_squared_error, r2_score

	import pickle
	import joblib

	import tensorflow as tf
	from tensorflow import keras
	from tensorflow.keras import layers
	from keras.callbacks import ModelCheckpoint, EarlyStopping
	
	* 기존 저장된 모델 로드
	model_rslt = []
	for i in range(5):
		model_rslt.append(joblib.load("{}_model.pkl".format(i))) # 5개의 pkl(머신러닝)모델
	model_rslt.append(keras.models.load_model("DeeplearningModel.h5")) # 1개의 h5 딥러닝 모델
	model_rslt
	
	* 각 모델 예측값 측정
	e1_list = ['ETA1', 'ETA2', 'ETA3', 'ETA4', 'ETA5', 'ETA6']
	e2_list = ['ETAA1', 'ETAA2', 'ETAA3', 'ETAA4', 'ETAA5', 'ETAA6']

	for e1, e2, model in zip(e1_list, e2_list, model_rslt):
		df_evaluation[e1] = model.predict(df_evaluation_feature)
		df_evaluation.loc[(df_evaluation[e1] < 0), e1] = 0
        # 측정값 계산
		etaa = (1-(abs(df_evaluation['ET']-df_evaluation[e1])/df_evaluation['ET']))*100.0 
		df_evaluation[e2] = etaa
		df_evaluation.loc[(df_evaluation[e2] < 0), e2] = 0

	# mean, min, max, std
	etaa = ['ETAA', 'ETAA1', 'ETAA2', 'ETAA3', 'ETAA4', 'ETAA5', 'ETAA6']
	alg = ['DATA', 'ML-LG', 'ML-RFR', 'ML-GBR', 'XBR', 'XBR2', 'Deep']

	print('+-------------------------------------------------------+')
	print('|   ALG    | Mean(%) |    STD    |  MIN(%)  |  MAX(%)   |')
	print('+----------+---------+-----------+----------+-----------+')
	for i, e in zip(range(len(alg)), etaa):
		eMean = df_evaluation[e].mean()
		eStd = df_evaluation[e].std()
		eMin = df_evaluation[e].min()
		eMax = df_evaluation[e].max()
		print('|  {:6s}  |   {:3.1f}  |   {:05.1f}   |   {:4.1f}   |  {:7.1f}  |'.format(alg[i], eMean, eStd, eMin, eMax))
	print('+----------+---------+-----------+----------+-----------+\n\n')
	
	
	* 성능 그래프로 전시
	plt.figure(figsize=(100, 50))
	idx = np.arange(0, 100)
	plt.plot(idx, df_evaluation.ET[0:100], 'b', marker='s')
	plt.plot(idx, df_evaluation.ETA1[0:100], 'g', marker='s')
	plt.plot()
	plt.xlabel('Index')
	plt.ylabel('ET(sec)')
	plt.legend(['Real', 'ML-LG'])
	plt.show()
	
	
	
	

# ⑨ 모델 최적화
### ** panda 식
    from xgboost import XGBRegressor as xgb
    from sklearn.metrics import roc_auc_score, accuracy_score, mean_squared_error, r2_score
    
    # 이런식으로 인자값을 바꾸면서 테스트
    #model=xgb(n_estimators=100, eta=0.1)
    #model=xgb(n_estimators=100, eta=0.3)
    #model=xgb(n_estimators=100, eta=0.2, 
    #      max_depth=5, subsample= 0.8, colsample_bytree=0.5, reg_alpha=3, gamma=5)
    model=xgb(n_estimators=200, eta=0.1,
          max_depth=5, subsample= 0.8, colsample_bytree=0.5, reg_alpha=3, gamma=5)           
          
    model.fit(train_x, train_y)

    pred_y = model.predict(test_x)
    print("RMSE on Test set : {0:.5f}".format(mean_squared_error(test_y,pred_y)**0.5))
    print("R-squared Score on Test set : {0:.5f}".format(r2_score(test_y,pred_y)))
  
### ** caviar 식 (클라우드 부하와 시간이 오래걸림)
    from xgboost import XGBRegressor as xgb
    from sklearn.metrics import roc_auc_score, accuracy_score, mean_squared_error, r2_score
    from sklearn.model_selection import GridSearchCV
    import time # 수행시간 체크
    # 파라메터 미리 정의
    params = { 'n_estimators' : [50, 100, 200],
               'learning_rate' : [0, 0.01],
               'max_depth' : [0, 3],
                }

    xgb_model = xgb(random_state = 0, n_jobs = 1) # 모델 정의
    grid_cv = GridSearchCV(xgb_model, param_grid = params, cv = 3, n_jobs = 1) # 다중수행 설정
    start_time = time.process_time()
    grid_cv.fit(train_x, train_y)
    end_time = time.process_time()

    print('----  {0:.5f}sec, training complete  ----'.format(end_time-start_time))
    print('최적 하이퍼 파라미터: ', grid_cv.best_params_)
    print('최고 예측 정확도: {:.4f}'.format(grid_cv.best_score_))



⑩
https://velog.io/@sy508011/AICE-ASSOCIATE-%EC%9E%90%EA%B2%A9%EC%A6%9D-%EC%B7%A8%EB%93%9D%EC%9D%84-%EC%9C%84%ED%95%9C-%EC%B4%9D-%EC%A0%95%EB%A6%AC




** 시험 범위
1. 데이터분석 : 5~6문항, 30점
ㅇ 필요한 라이브러리 설치
ㅇ Tabular 데이터 로딩, 저장
ㅇ 데이터의 구성 확인, 상관분석
ㅇ 기초 데이터 다루기

2. 전처리 & 시각화 : 4~5 문항 , 30점
ㅇ 데이터 결측치 처리
ㅇ 라벨 인코딩/원핫 인코딩
ㅇ X,Y 데이터 분리
ㅇ 데이터 정규분포화, 표준화

3. 모델링 : 4~5문항, 40점
ㅇ 머신러닝 모델 학습(sklearn)
ㅇ 딥러닝 모델 학습 (tensorflow)
ㅇ 모델 성능 평가 및 시뮬레이션
ㅇ 모델 성능 개선 및 그래픽 출력



	
    
