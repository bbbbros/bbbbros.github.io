---
published : false
---


# 데이터 불러오기부터

### 가. 라이브러리 Import


```python
import pandas as pd
import numpy as np
```

#### [참고] AIDU에서 필요한 추가 라이브러리 Import : 아래 코드는 AIDU에서만 실행 가능합니다.


```python
# from aicentro.session import Session
# from aicentro.framework.tensorflow import tensorflow as AiduFrm
```

### 나. 데이터 불러오기


```python
df_feature = pd.read_csv("onenavi_train_feature.csv",sep="|")
df_target = pd.read_csv("onenavi_train_target.csv",sep="|")
```

#### [참고] AIDU에서는 파일을 불러올 때 아래 예시를 참고 : 아래 코드는 AIDU에서만 실행 가능합니다.


```python
# # 데이터 셋 로딩
# aicentro_session = Session(verify = False)
# aidu_framework = AiduFrm(session = aicentro_session)
# df_feature = pd.read_csv(aidu_framework.config.data_dir+"onenavi_train_feature.csv",sep="|")
# df_target = pd.read_csv(aidu_framework.config.data_dir+"onenavi_train_target.csv",sep="|")
# df_feature # 데이터 프레임 확인하기
```

### 나. Train/Test Data Split


```python
from sklearn.model_selection import train_test_split

# train_test_split
train_x, test_x, train_y, test_y = train_test_split(df_feature, df_target, test_size=0.20, random_state=42)
```

# 모델 최적화 : Hyperparameter Tuning
* Hyperparmeter는 머신러닝에서 모델을 학습하기 전에 설정해주는 옵션 값이라고 할 수 있습니다.
* 설정을 머신러닝이 스스로 찾을 수 있다면 참 좋을텐데
* 불가능하기 때문에 우리가 직접 찾아서 설정 해주어야 합니다.
* 모델을 최적화하는 전략은 크게 Panda전략과 Caviar전략으로 나누어볼 수 있는데요.
* 간단하게 두 전략의 개념을 짚어보고 실습하는 시간을 가지겠습니다.

# 지금부터 모델 최적화를 시작합니다.

# 1. Panda전략 : Babysitting one model
* 크게 하나의 모델을 정하고 성능을 평가하면서 점진적으로 모델을 '돌보기'하는 전략
* 한 번에 한마리씩 아기를 가지는 팬더는 아기 팬더가 살아남을 수 있도록 지극정성으로 케어를 하는데, 여기서도 동일하게 하나의 모델을 잘 케어해서 최적화하는 개념을 이야기한다.
<img src="attachment:image.png" width="50%"/>

[Panda 전략 Tip : XGBoost 기준]
* 모델 성능을 높이고 싶다면!
1. n-estimators는 높게, eta는 낮게

* 튜닝 예시
1. n-estimators와 learning_rate를 먼저 지정(n-estimators를 100으로 고정하고 최적 learning_rate 찾기)
2. 나머지 hyperparameter 튜닝
3. n-estimators와 learning_rate 조정(둘은 반비례, 꼭 법칙은 아님!)
    * (예시) n-estimators를 2배 늘리면 learning_rate는 1/2로, n-estimators를 10배 늘리면 learning_rate는 1/10


```python
# 1단계 n-estimators와 learning_rate(eta)를 먼저 지정 : eta 0.1 => R-squared Score on Test set : 0.71631
from xgboost import XGBRegressor as xgb
from sklearn.metrics import roc_auc_score, accuracy_score, mean_squared_error, r2_score

model=xgb(n_estimators=100, eta=0.1)
model.fit(train_x, train_y)

pred_y = model.predict(test_x)
print("RMSE on Test set : {0:.5f}".format(mean_squared_error(test_y,pred_y)**0.5))
print("R-squared Score on Test set : {0:.5f}".format(r2_score(test_y,pred_y)))
```

    [16:40:58] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    RMSE on Test set : 279.15119
    R-squared Score on Test set : 0.71129



```python
# 1단계 n-estimators와 learning_rate(eta)를 먼저 지정 : eta 0.2 => R-squared Score on Test set : 0.71680
from xgboost import XGBRegressor as xgb
from sklearn.metrics import roc_auc_score, accuracy_score, mean_squared_error, r2_score

model=xgb(n_estimators=100, eta=0.2)
model.fit(train_x, train_y)

pred_y = model.predict(test_x)
print("RMSE on Test set : {0:.5f}".format(mean_squared_error(test_y,pred_y)**0.5))
print("R-squared Score on Test set : {0:.5f}".format(r2_score(test_y,pred_y)))
```

    [16:41:06] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    RMSE on Test set : 279.15119
    R-squared Score on Test set : 0.71129



```python
# 1단계 n-estimators와 learning_rate(eta)를 먼저 지정 : eta 0.3 => R-squared Score on Test set : 0.70988
from xgboost import XGBRegressor as xgb
from sklearn.metrics import roc_auc_score, accuracy_score, mean_squared_error, r2_score

model=xgb(n_estimators=100, eta=0.3)
model.fit(train_x, train_y)

pred_y = model.predict(test_x)
print("RMSE on Test set : {0:.5f}".format(mean_squared_error(test_y,pred_y)**0.5))
print("R-squared Score on Test set : {0:.5f}".format(r2_score(test_y,pred_y)))
```

    [16:41:15] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    RMSE on Test set : 279.15119
    R-squared Score on Test set : 0.71129



```python
# 2단계 나머지 hyperparameter 튜닝 : 0.71724
from xgboost import XGBRegressor as xgb
from sklearn.metrics import roc_auc_score, accuracy_score, mean_squared_error, r2_score

model=xgb(n_estimators=100, eta=0.2, 
          max_depth=5, subsample= 0.8, colsample_bytree=0.5, reg_alpha=3, gamma=5)
model.fit(train_x, train_y)

pred_y = model.predict(test_x)
print("RMSE on Test set : {0:.5f}".format(mean_squared_error(test_y,pred_y)**0.5))
print("R-squared Score on Test set : {0:.5f}".format(r2_score(test_y,pred_y)))
```

    [16:41:23] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    RMSE on Test set : 276.52164
    R-squared Score on Test set : 0.71671



```python
# 3단계 n-estimators와 learning_rate 조정 : 0.71890
from xgboost import XGBRegressor as xgb
from sklearn.metrics import roc_auc_score, accuracy_score, mean_squared_error, r2_score

model=xgb(n_estimators=200, eta=0.1,
          max_depth=5, subsample= 0.8, colsample_bytree=0.5, reg_alpha=3, gamma=5)
model.fit(train_x, train_y)

pred_y = model.predict(test_x)
print("RMSE on Test set : {0:.5f}".format(mean_squared_error(test_y,pred_y)**0.5))
print("R-squared Score on Test set : {0:.5f}".format(r2_score(test_y,pred_y)))
```

    [16:41:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    RMSE on Test set : 275.69169
    R-squared Score on Test set : 0.71841



```python
# 모델을 저장합니다.
import pickle
import joblib


joblib.dump(model, '4_model.pkl') 
```




    ['4_model.pkl']



# 2. Caviar전략 : Training many models in parallel
* 여러개 모델을 동시에 돌려서 가장 좋은 모델을 선택하는 전략
* 한 철에 1억개의 알을 품는 물고기가 있다고 한다. 물고기가 번식하는 과정은 하나에 많은 집중을 쏟기보다 하나 또는 그 이상이 더 잘 살아남기를 그지 지켜보는데, 여기에서 착안하여 여러 모델을 시도해서 최적의 모델을 찾는 전략을 Caviar전략이라고 한다.
![image.png](attachment:image.png)

#### Caviar 전략에서는 Tip이라고 정리하기보다 Gridsearch라는 유용한 라이브러리를 소개하고자 한다.
* Gridsearch는 여러 모델을 코드 몇 줄로 시도하고 최적 파라미터를 도출하기에 편하다.
* 다만, 시간이 많이 걸린다.


```python
from xgboost import XGBRegressor as xgb
from sklearn.metrics import roc_auc_score, accuracy_score, mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV
import time

params = { 'n_estimators' : [50, 100, 200],
           'learning_rate' : [0, 0.01],
           'max_depth' : [0, 3],
            }

xgb_model = xgb(random_state = 0, n_jobs = 1)
grid_cv = GridSearchCV(xgb_model, param_grid = params, cv = 3, n_jobs = 1)
start_time = time.process_time()
grid_cv.fit(train_x, train_y)
end_time = time.process_time()

print('----  {0:.5f}sec, training complete  ----'.format(end_time-start_time))
print('최적 하이퍼 파라미터: ', grid_cv.best_params_)
print('최고 예측 정확도: {:.4f}'.format(grid_cv.best_score_))
```

    [16:41:49] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:41:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:41:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:41:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:41:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:41:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:41:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:41:58] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:42:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:42:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:42:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:42:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:42:11] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:42:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:42:21] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:42:26] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:42:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:42:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:42:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:42:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:42:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:42:58] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:42:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:43:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:43:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:43:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:43:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:43:09] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:43:12] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:43:15] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:43:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:43:24] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:43:29] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:43:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:43:44] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:43:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:44:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    ----  147.56946sec, training complete  ----
    최적 하이퍼 파라미터:  {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200}
    최고 예측 정확도: 0.6399


# 3. 모델 평가


```python
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.linear_model import LinearRegression as lr
from sklearn.ensemble import RandomForestRegressor as rfr
from sklearn.ensemble import GradientBoostingRegressor as grb
from xgboost import XGBRegressor as xgb
from sklearn.metrics import roc_auc_score, accuracy_score, mean_squared_error, r2_score

import pickle
import joblib

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras.callbacks import ModelCheckpoint, EarlyStopping
```

    Using TensorFlow backend.



```python
df_evaluation = pd.read_csv("onenavi_evaluation_new.csv",sep="|")
df_evaluation_feature = pd.read_csv("onenavi_eval_feature.csv",sep="|")
```


```python
model_rslt = []
for i in range(5):
    model_rslt.append(joblib.load("{}_model.pkl".format(i)))
model_rslt.append(keras.models.load_model("DeeplearningModel.h5"))
model_rslt
```

    [16:44:22] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    [16:44:22] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.





    [LinearRegression(),
     RandomForestRegressor(),
     GradientBoostingRegressor(),
     XGBRegressor(missing=nan),
     XGBRegressor(colsample_bytree=0.5, eta=0.1, gamma=5, max_depth=5, missing=nan,
                  n_estimators=200, reg_alpha=3, subsample=0.8),
     <tensorflow.python.keras.engine.sequential.Sequential at 0x7fecb0263cc0>]




```python
    e1_list = ['ETA1', 'ETA2', 'ETA3', 'ETA4', 'ETA5', 'ETA6']
    e2_list = ['ETAA1', 'ETAA2', 'ETAA3', 'ETAA4', 'ETAA5', 'ETAA6']

    for e1, e2, model in zip(e1_list, e2_list, model_rslt):
        df_evaluation[e1] = model.predict(df_evaluation_feature)
        df_evaluation.loc[(df_evaluation[e1] < 0), e1] = 0
        etaa = (1-(abs(df_evaluation['ET']-df_evaluation[e1])/df_evaluation['ET']))*100.0
        df_evaluation[e2] = etaa
        df_evaluation.loc[(df_evaluation[e2] < 0), e2] = 0

    # mean, min, max, std
    etaa = ['ETAA', 'ETAA1', 'ETAA2', 'ETAA3', 'ETAA4', 'ETAA5', 'ETAA6']
    alg = ['DATA', 'ML-LG', 'ML-RFR', 'ML-GBR', 'XBR', 'XBR2', 'Deep']

    print('+-------------------------------------------------------+')
    print('|   ALG    | Mean(%) |    STD    |  MIN(%)  |  MAX(%)   |')
    print('+----------+---------+-----------+----------+-----------+')
    for i, e in zip(range(len(alg)), etaa):
        eMean = df_evaluation[e].mean()
        eStd = df_evaluation[e].std()
        eMin = df_evaluation[e].min()
        eMax = df_evaluation[e].max()
        print('|  {:6s}  |   {:3.1f}  |   {:05.1f}   |   {:4.1f}   |  {:7.1f}  |'.format(alg[i], eMean, eStd, eMin, eMax))
    print('+----------+---------+-----------+----------+-----------+\n\n')
```

    +-------------------------------------------------------+
    |   ALG    | Mean(%) |    STD    |  MIN(%)  |  MAX(%)   |
    +----------+---------+-----------+----------+-----------+
    |  DATA    |   81.4  |   015.7   |    0.0   |    100.0  |
    |  ML-LG   |   81.9  |   017.4   |    0.0   |    100.0  |
    |  ML-RFR  |   81.6  |   017.5   |    0.0   |    100.0  |
    |  ML-GBR  |   81.9  |   017.6   |    0.0   |    100.0  |
    |  XBR     |   81.9  |   017.6   |    0.0   |    100.0  |
    |  XBR2    |   82.7  |   016.7   |    0.0   |    100.0  |
    |  Deep    |   82.4  |   016.7   |    0.0   |    100.0  |
    +----------+---------+-----------+----------+-----------+
    
    


### 좀 더 상세한 분석을 위해 거리 구간별로 결과를 확인해 보겠습니다.


```python
etaa = ['ETAA', 'ETAA1', 'ETAA2', 'ETAA3', 'ETAA4', 'ETAA5', 'ETAA6']
alg = ['DATA', 'ML-LG', 'ML-RFR', 'ML-GBR', 'XBR', 'XBR2', 'Deep']

for length in range(4):
    if length == 0:
        print(' 1,000 <= A_DISTANCE < 5,000m')
    elif length == 1:
        print(' 5,000 <= A_DISTANCE < 10,000m')
    elif length == 2:
        print(' 10,000 <= A_DISTANCE < 15,000m')
    else:
         print('All A_DISTANCE')
    print('+-------------------------------------------------------+')
    print('|   ALG    | Mean(%) |    STD    |  MIN(%)  |  MAX(%)   |')
    print('+----------+---------+-----------+----------+-----------+')
    for i, e in zip(range(len(alg)), etaa):
        if length == 0:
            eMean = df_evaluation.loc[(df_evaluation['A_DISTANCE']>=1000) & (df_evaluation['A_DISTANCE']<5000), e].mean()
            eStd = df_evaluation.loc[(df_evaluation['A_DISTANCE']>=1000) & (df_evaluation['A_DISTANCE']<5000), e].std()
            eMin = df_evaluation.loc[(df_evaluation['A_DISTANCE']>=1000) & (df_evaluation['A_DISTANCE']<5000), e].min()
            eMax = df_evaluation.loc[(df_evaluation['A_DISTANCE']>=1000) & (df_evaluation['A_DISTANCE']<5000), e].max()
        elif length == 1:
            eMean = df_evaluation.loc[(df_evaluation['A_DISTANCE']>=5000) & (df_evaluation['A_DISTANCE']<10000), e].mean()
            eStd = df_evaluation.loc[(df_evaluation['A_DISTANCE']>=5000) & (df_evaluation['A_DISTANCE']<10000), e].std()
            eMin = df_evaluation.loc[(df_evaluation['A_DISTANCE']>=5000) & (df_evaluation['A_DISTANCE']<10000), e].min()
            eMax = df_evaluation.loc[(df_evaluation['A_DISTANCE']>=5000) & (df_evaluation['A_DISTANCE']<10000), e].max()
        elif length == 2:
            eMean = df_evaluation.loc[(df_evaluation['A_DISTANCE']>=10000) & (df_evaluation['A_DISTANCE']<15000), e].mean()
            eStd = df_evaluation.loc[(df_evaluation['A_DISTANCE']>=10000) & (df_evaluation['A_DISTANCE']<15000), e].std()
            eMin = df_evaluation.loc[(df_evaluation['A_DISTANCE']>=10000) & (df_evaluation['A_DISTANCE']<15000), e].min()
            eMax = df_evaluation.loc[(df_evaluation['A_DISTANCE']>=10000) & (df_evaluation['A_DISTANCE']<15000), e].max()
        else:
            eMean = df_evaluation[e].mean()
            eStd = df_evaluation[e].std()
            eMin = df_evaluation[e].min()
            eMax = df_evaluation[e].max()
        print('|  {:6s}  |   {:3.1f}  |   {:05.1f}   |   {:4.1f}   |  {:7.1f}  |'.format(alg[i], eMean, eStd, eMin, eMax))
    print('+----------+---------+-----------+----------+-----------+\n\n')
```

     1,000 <= A_DISTANCE < 5,000m
    +-------------------------------------------------------+
    |   ALG    | Mean(%) |    STD    |  MIN(%)  |  MAX(%)   |
    +----------+---------+-----------+----------+-----------+
    |  DATA    |   77.7  |   017.5   |    0.0   |    100.0  |
    |  ML-LG   |   78.3  |   020.1   |    0.0   |    100.0  |
    |  ML-RFR  |   78.2  |   019.9   |    0.0   |    100.0  |
    |  ML-GBR  |   78.4  |   020.3   |    0.0   |    100.0  |
    |  XBR     |   78.4  |   020.2   |    0.0   |    100.0  |
    |  XBR2    |   79.6  |   019.0   |    0.0   |    100.0  |
    |  Deep    |   79.0  |   019.2   |    0.0   |    100.0  |
    +----------+---------+-----------+----------+-----------+
    
    
     5,000 <= A_DISTANCE < 10,000m
    +-------------------------------------------------------+
    |   ALG    | Mean(%) |    STD    |  MIN(%)  |  MAX(%)   |
    +----------+---------+-----------+----------+-----------+
    |  DATA    |   82.4  |   014.7   |    0.1   |    100.0  |
    |  ML-LG   |   83.0  |   016.1   |    0.0   |    100.0  |
    |  ML-RFR  |   82.6  |   016.4   |    0.0   |    100.0  |
    |  ML-GBR  |   83.0  |   016.3   |    0.0   |    100.0  |
    |  XBR     |   83.0  |   016.3   |    0.0   |    100.0  |
    |  XBR2    |   83.5  |   015.7   |    0.0   |    100.0  |
    |  Deep    |   83.4  |   015.4   |    0.0   |    100.0  |
    +----------+---------+-----------+----------+-----------+
    
    
     10,000 <= A_DISTANCE < 15,000m
    +-------------------------------------------------------+
    |   ALG    | Mean(%) |    STD    |  MIN(%)  |  MAX(%)   |
    +----------+---------+-----------+----------+-----------+
    |  DATA    |   85.2  |   013.3   |    0.2   |    100.0  |
    |  ML-LG   |   85.3  |   014.3   |    0.0   |    100.0  |
    |  ML-RFR  |   85.0  |   014.6   |    0.0   |    100.0  |
    |  ML-GBR  |   85.5  |   014.4   |    0.0   |    100.0  |
    |  XBR     |   85.5  |   014.4   |    0.0   |    100.0  |
    |  XBR2    |   85.8  |   013.7   |    0.0   |    100.0  |
    |  Deep    |   85.6  |   013.9   |    0.0   |    100.0  |
    +----------+---------+-----------+----------+-----------+
    
    
    All A_DISTANCE
    +-------------------------------------------------------+
    |   ALG    | Mean(%) |    STD    |  MIN(%)  |  MAX(%)   |
    +----------+---------+-----------+----------+-----------+
    |  DATA    |   81.4  |   015.7   |    0.0   |    100.0  |
    |  ML-LG   |   81.9  |   017.4   |    0.0   |    100.0  |
    |  ML-RFR  |   81.6  |   017.5   |    0.0   |    100.0  |
    |  ML-GBR  |   81.9  |   017.6   |    0.0   |    100.0  |
    |  XBR     |   81.9  |   017.6   |    0.0   |    100.0  |
    |  XBR2    |   82.7  |   016.7   |    0.0   |    100.0  |
    |  Deep    |   82.4  |   016.7   |    0.0   |    100.0  |
    +----------+---------+-----------+----------+-----------+
    
    


### 모델의 R-squared Score, RMSE가 좋았다고해서 실제 평가데이터에 적용했을 때,
### 비례해서 모델의 정확도 혹은 결과가 좋아지지 않는다는 사실을 알 수 있습니다. 
### 물론 두 수치가 좋아지면 모델의 정확도 혹은 결과가 좋아지는 경향은 있으나, 
### 정확하게 비례하지는 않습니다. 
### 그래서 다각도로 보면서 모델을 만들어줘야합니다.
#### (과적합을 방지하고, 변수의 영향을 조정하는 등)
